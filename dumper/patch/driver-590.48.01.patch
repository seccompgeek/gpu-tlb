diff --git a/kernel-open/nvidia-uvm/uvm.c b/kernel-open/nvidia-uvm/uvm.c
index f482f1ee..da6639d5 100644
--- a/kernel-open/nvidia-uvm/uvm.c
+++ b/kernel-open/nvidia-uvm/uvm.c
@@ -22,350 +22,338 @@
 *******************************************************************************/
 
 #include "uvm_api.h"
+#include "uvm_common.h"
+#include "uvm_fd_type.h"
 #include "uvm_global.h"
 #include "uvm_gpu_replayable_faults.h"
-#include "uvm_tools_init.h"
+#include "uvm_hmm.h"
+#include "uvm_kvmalloc.h"
+#include "uvm_linux_ioctl.h"
 #include "uvm_lock.h"
+#include "uvm_mem.h"
 #include "uvm_test.h"
+#include "uvm_test_file.h"
+#include "uvm_tools.h"
+#include "uvm_tools_init.h"
+#include "uvm_va_block.h"
+#include "uvm_va_range.h"
 #include "uvm_va_space.h"
 #include "uvm_va_space_mm.h"
-#include "uvm_va_range.h"
-#include "uvm_va_block.h"
-#include "uvm_tools.h"
-#include "uvm_common.h"
-#include "uvm_fd_type.h"
-#include "uvm_linux_ioctl.h"
-#include "uvm_hmm.h"
-#include "uvm_mem.h"
-#include "uvm_kvmalloc.h"
-#include "uvm_test_file.h"
 
-#define NVIDIA_UVM_DEVICE_NAME          "nvidia-uvm"
+#define NVIDIA_UVM_DEVICE_NAME "nvidia-uvm"
 
 static dev_t g_uvm_base_dev;
 static struct cdev g_uvm_cdev;
 static const struct file_operations uvm_fops;
 
-bool uvm_file_is_nvidia_uvm(struct file *filp)
-{
-    return (filp != NULL) && (filp->f_op == &uvm_fops);
+bool uvm_file_is_nvidia_uvm(struct file *filp) {
+  return (filp != NULL) && (filp->f_op == &uvm_fops);
 }
 
-bool uvm_file_is_nvidia_uvm_va_space(struct file *filp)
-{
-    return uvm_file_is_nvidia_uvm(filp) && uvm_fd_type(filp, NULL) == UVM_FD_VA_SPACE;
+bool uvm_file_is_nvidia_uvm_va_space(struct file *filp) {
+  return uvm_file_is_nvidia_uvm(filp) &&
+         uvm_fd_type(filp, NULL) == UVM_FD_VA_SPACE;
 }
 
-static NV_STATUS uvm_api_mm_initialize(UVM_MM_INITIALIZE_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space;
-    uvm_va_space_mm_t *va_space_mm;
-    struct file *uvm_file;
-    struct mm_struct *mm;
-    NV_STATUS status;
-
-    uvm_file = fget(params->uvmFd);
-    if (!uvm_file_is_nvidia_uvm(uvm_file)) {
-        status = NV_ERR_INVALID_ARGUMENT;
-        goto err;
-    }
-
-    if (uvm_fd_type(uvm_file, (void **)&va_space) != UVM_FD_VA_SPACE) {
-        status = NV_ERR_INVALID_ARGUMENT;
-        goto err;
+static NV_STATUS uvm_api_mm_initialize(UVM_MM_INITIALIZE_PARAMS *params,
+                                       struct file *filp) {
+  uvm_va_space_t *va_space;
+  uvm_va_space_mm_t *va_space_mm;
+  struct file *uvm_file;
+  struct mm_struct *mm;
+  NV_STATUS status;
+
+  uvm_file = fget(params->uvmFd);
+  if (!uvm_file_is_nvidia_uvm(uvm_file)) {
+    status = NV_ERR_INVALID_ARGUMENT;
+    goto err;
+  }
+
+  if (uvm_fd_type(uvm_file, (void **)&va_space) != UVM_FD_VA_SPACE) {
+    status = NV_ERR_INVALID_ARGUMENT;
+    goto err;
+  }
+
+  // Tell userspace the MM FD is not required and it may be released
+  // with no loss of functionality.
+  if (!uvm_va_space_mm_enabled(va_space)) {
+    status = NV_WARN_NOTHING_TO_DO;
+    goto err;
+  }
+
+  status = uvm_fd_type_init(filp);
+  if (status != NV_OK)
+    goto err;
+
+  va_space_mm = &va_space->va_space_mm;
+  uvm_spin_lock(&va_space_mm->lock);
+  switch (va_space->va_space_mm.state) {
+  // We only allow the va_space_mm to be initialised once. If
+  // userspace passed the UVM FD to another process it is up to
+  // userspace to ensure it also passes the UVM MM FD that
+  // initialised the va_space_mm or arranges some other way to keep
+  // a reference on the FD.
+  case UVM_VA_SPACE_MM_STATE_ALIVE:
+    status = NV_ERR_INVALID_STATE;
+    goto err_release_unlock;
+    break;
+
+  // Once userspace has released the va_space_mm the GPU is
+  // effectively dead and no new work can be started. We don't
+  // support re-initializing once userspace has closed the FD.
+  case UVM_VA_SPACE_MM_STATE_RELEASED:
+    status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
+    goto err_release_unlock;
+    break;
+
+  // Keep the warnings at bay
+  case UVM_VA_SPACE_MM_STATE_UNINITIALIZED:
+    mm = va_space->va_space_mm.mm;
+    if (!mm || !mmget_not_zero(mm)) {
+      status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
+      goto err_release_unlock;
     }
 
-    // Tell userspace the MM FD is not required and it may be released
-    // with no loss of functionality.
-    if (!uvm_va_space_mm_enabled(va_space)) {
-        status = NV_WARN_NOTHING_TO_DO;
-        goto err;
-    }
+    va_space_mm->state = UVM_VA_SPACE_MM_STATE_ALIVE;
+    break;
 
-    status = uvm_fd_type_init(filp);
-    if (status != NV_OK)
-        goto err;
-
-    va_space_mm = &va_space->va_space_mm;
-    uvm_spin_lock(&va_space_mm->lock);
-    switch (va_space->va_space_mm.state) {
-        // We only allow the va_space_mm to be initialised once. If
-        // userspace passed the UVM FD to another process it is up to
-        // userspace to ensure it also passes the UVM MM FD that
-        // initialised the va_space_mm or arranges some other way to keep
-        // a reference on the FD.
-        case UVM_VA_SPACE_MM_STATE_ALIVE:
-            status = NV_ERR_INVALID_STATE;
-            goto err_release_unlock;
-            break;
-
-        // Once userspace has released the va_space_mm the GPU is
-        // effectively dead and no new work can be started. We don't
-        // support re-initializing once userspace has closed the FD.
-        case UVM_VA_SPACE_MM_STATE_RELEASED:
-            status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
-            goto err_release_unlock;
-            break;
-
-        // Keep the warnings at bay
-        case UVM_VA_SPACE_MM_STATE_UNINITIALIZED:
-            mm = va_space->va_space_mm.mm;
-            if (!mm || !mmget_not_zero(mm)) {
-                status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
-                goto err_release_unlock;
-            }
-
-            va_space_mm->state = UVM_VA_SPACE_MM_STATE_ALIVE;
-            break;
-
-        default:
-            UVM_ASSERT(0);
-            break;
-    }
-    uvm_spin_unlock(&va_space_mm->lock);
-    uvm_fd_type_set(filp, UVM_FD_MM, uvm_file);
+  default:
+    UVM_ASSERT(0);
+    break;
+  }
+  uvm_spin_unlock(&va_space_mm->lock);
+  uvm_fd_type_set(filp, UVM_FD_MM, uvm_file);
 
-    return NV_OK;
+  return NV_OK;
 
 err_release_unlock:
-    uvm_spin_unlock(&va_space_mm->lock);
-    uvm_fd_type_set(filp, UVM_FD_UNINITIALIZED, NULL);
+  uvm_spin_unlock(&va_space_mm->lock);
+  uvm_fd_type_set(filp, UVM_FD_UNINITIALIZED, NULL);
 
 err:
-    if (uvm_file)
-        fput(uvm_file);
+  if (uvm_file)
+    fput(uvm_file);
 
-    return status;
+  return status;
 }
 
 // Called when opening /dev/nvidia-uvm. This code doesn't take any UVM locks, so
 // there's no need to acquire g_uvm_global.pm.lock, but if that changes the PM
 // lock will need to be taken.
-static int uvm_open(struct inode *inode, struct file *filp)
-{
-    struct address_space *mapping;
-    NV_STATUS status = uvm_global_get_status();
-
-    if (status != NV_OK)
-        return -nv_status_to_errno(status);
-
-    mapping = uvm_kvmalloc(sizeof(*mapping));
-    if (!mapping)
-        return -ENOMEM;
-
-    // By default all struct files on the same inode share the same
-    // address_space structure (the inode's) across all processes. This means
-    // unmap_mapping_range would unmap virtual mappings across all processes on
-    // that inode.
-    //
-    // Since the UVM driver uses the mapping offset as the VA of the file's
-    // process, we need to isolate the mappings to each process.
-    address_space_init_once(mapping);
-    mapping->host = inode;
-
-    // Some paths in the kernel, for example force_page_cache_readahead which
-    // can be invoked from user-space via madvise MADV_WILLNEED and fadvise
-    // POSIX_FADV_WILLNEED, check the function pointers within
-    // file->f_mapping->a_ops for validity. However, those paths assume that a_ops
-    // itself is always valid. Handle that by using the inode's a_ops pointer,
-    // which is what f_mapping->a_ops would point to anyway if we weren't re-
-    // assigning f_mapping.
-    mapping->a_ops = inode->i_mapping->a_ops;
-
-    filp->private_data = NULL;
-    filp->f_mapping = mapping;
-
-    return NV_OK;
+static int uvm_open(struct inode *inode, struct file *filp) {
+  struct address_space *mapping;
+  NV_STATUS status = uvm_global_get_status();
+
+  if (status != NV_OK)
+    return -nv_status_to_errno(status);
+
+  mapping = uvm_kvmalloc(sizeof(*mapping));
+  if (!mapping)
+    return -ENOMEM;
+
+  // By default all struct files on the same inode share the same
+  // address_space structure (the inode's) across all processes. This means
+  // unmap_mapping_range would unmap virtual mappings across all processes on
+  // that inode.
+  //
+  // Since the UVM driver uses the mapping offset as the VA of the file's
+  // process, we need to isolate the mappings to each process.
+  address_space_init_once(mapping);
+  mapping->host = inode;
+
+  // Some paths in the kernel, for example force_page_cache_readahead which
+  // can be invoked from user-space via madvise MADV_WILLNEED and fadvise
+  // POSIX_FADV_WILLNEED, check the function pointers within
+  // file->f_mapping->a_ops for validity. However, those paths assume that a_ops
+  // itself is always valid. Handle that by using the inode's a_ops pointer,
+  // which is what f_mapping->a_ops would point to anyway if we weren't re-
+  // assigning f_mapping.
+  mapping->a_ops = inode->i_mapping->a_ops;
+
+  filp->private_data = NULL;
+  filp->f_mapping = mapping;
+
+  return NV_OK;
 }
 
-static int uvm_open_entry(struct inode *inode, struct file *filp)
-{
-   UVM_ENTRY_RET(uvm_open(inode, filp));
+static int uvm_open_entry(struct inode *inode, struct file *filp) {
+  UVM_ENTRY_RET(uvm_open(inode, filp));
 }
 
-static void uvm_release_deferred(void *data)
-{
-    uvm_va_space_t *va_space = data;
+static void uvm_release_deferred(void *data) {
+  uvm_va_space_t *va_space = data;
 
-    // Since this function is only scheduled to run when uvm_release() fails
-    // to trylock-acquire the pm.lock, the following acquisition attempt
-    // is expected to block this thread, and cause it to remain blocked until
-    // uvm_resume() releases the lock. As a result, the deferred release
-    // kthread queue may stall for long periods of time.
-    uvm_down_read(&g_uvm_global.pm.lock);
+  // Since this function is only scheduled to run when uvm_release() fails
+  // to trylock-acquire the pm.lock, the following acquisition attempt
+  // is expected to block this thread, and cause it to remain blocked until
+  // uvm_resume() releases the lock. As a result, the deferred release
+  // kthread queue may stall for long periods of time.
+  uvm_down_read(&g_uvm_global.pm.lock);
 
-    uvm_va_space_destroy(va_space);
+  uvm_va_space_destroy(va_space);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 }
 
-static void uvm_release_va_space(struct file *filp, uvm_va_space_t *va_space)
-{
-    int ret;
+static void uvm_release_va_space(struct file *filp, uvm_va_space_t *va_space) {
+  int ret;
 
-    filp->private_data = NULL;
-    filp->f_mapping = NULL;
+  filp->private_data = NULL;
+  filp->f_mapping = NULL;
 
-    // Because the kernel discards the status code returned from this release
-    // callback, early exit in case of a pm.lock acquisition failure is not
-    // an option. Instead, the teardown work normally performed synchronously
-    // needs to be scheduled to run after uvm_resume() releases the lock.
-    if (uvm_down_read_trylock(&g_uvm_global.pm.lock)) {
-        uvm_va_space_destroy(va_space);
-        uvm_up_read(&g_uvm_global.pm.lock);
-    }
-    else {
-        // Remove references to this inode from the address_space. This isn't
-        // strictly necessary, as any CPU mappings of this file have already
-        // been destroyed, and va_space->mapping won't be used again. Still,
-        // the va_space survives the inode if its destruction is deferred, in
-        // which case the references are rendered stale.
-        address_space_init_once(va_space->mapping);
-
-        nv_kthread_q_item_init(&va_space->deferred_release_q_item, uvm_release_deferred, va_space);
-        ret = nv_kthread_q_schedule_q_item(&g_uvm_global.deferred_release_q, &va_space->deferred_release_q_item);
-        UVM_ASSERT(ret != 0);
-    }
+  // Because the kernel discards the status code returned from this release
+  // callback, early exit in case of a pm.lock acquisition failure is not
+  // an option. Instead, the teardown work normally performed synchronously
+  // needs to be scheduled to run after uvm_resume() releases the lock.
+  if (uvm_down_read_trylock(&g_uvm_global.pm.lock)) {
+    uvm_va_space_destroy(va_space);
+    uvm_up_read(&g_uvm_global.pm.lock);
+  } else {
+    // Remove references to this inode from the address_space. This isn't
+    // strictly necessary, as any CPU mappings of this file have already
+    // been destroyed, and va_space->mapping won't be used again. Still,
+    // the va_space survives the inode if its destruction is deferred, in
+    // which case the references are rendered stale.
+    address_space_init_once(va_space->mapping);
+
+    nv_kthread_q_item_init(&va_space->deferred_release_q_item,
+                           uvm_release_deferred, va_space);
+    ret = nv_kthread_q_schedule_q_item(&g_uvm_global.deferred_release_q,
+                                       &va_space->deferred_release_q_item);
+    UVM_ASSERT(ret != 0);
+  }
 }
 
-static void uvm_release_mm(struct file *filp, struct file *uvm_file)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(uvm_file);
-    uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
-    struct mm_struct *mm = va_space_mm->mm;
+static void uvm_release_mm(struct file *filp, struct file *uvm_file) {
+  uvm_va_space_t *va_space = uvm_va_space_get(uvm_file);
+  uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
+  struct mm_struct *mm = va_space_mm->mm;
 
-    uvm_kvfree(filp->f_mapping);
+  uvm_kvfree(filp->f_mapping);
 
-    if (uvm_va_space_mm_enabled(va_space)) {
-        uvm_va_space_mm_unregister(va_space);
+  if (uvm_va_space_mm_enabled(va_space)) {
+    uvm_va_space_mm_unregister(va_space);
 
-        if (uvm_va_space_mm_enabled(va_space))
-            uvm_mmput(mm);
+    if (uvm_va_space_mm_enabled(va_space))
+      uvm_mmput(mm);
 
-        va_space_mm->mm = NULL;
-        fput(uvm_file);
-    }
+    va_space_mm->mm = NULL;
+    fput(uvm_file);
+  }
 }
 
-static int uvm_release(struct inode *inode, struct file *filp)
-{
-    void *ptr;
-    uvm_fd_type_t fd_type = uvm_fd_type(filp, &ptr);
+static int uvm_release(struct inode *inode, struct file *filp) {
+  void *ptr;
+  uvm_fd_type_t fd_type = uvm_fd_type(filp, &ptr);
 
-    switch (fd_type) {
-        case UVM_FD_UNINITIALIZED:
-            uvm_kvfree(filp->f_mapping);
-            break;
+  switch (fd_type) {
+  case UVM_FD_UNINITIALIZED:
+    uvm_kvfree(filp->f_mapping);
+    break;
 
-        case UVM_FD_VA_SPACE:
-            uvm_release_va_space(filp, (uvm_va_space_t *)ptr);
-            break;
+  case UVM_FD_VA_SPACE:
+    uvm_release_va_space(filp, (uvm_va_space_t *)ptr);
+    break;
 
-        case UVM_FD_MM:
-            uvm_release_mm(filp, (struct file *)ptr);
-            break;
+  case UVM_FD_MM:
+    uvm_release_mm(filp, (struct file *)ptr);
+    break;
 
-        case UVM_FD_TEST:
-            uvm_test_file_release(filp, (uvm_test_file_t *)ptr);
-            break;
+  case UVM_FD_TEST:
+    uvm_test_file_release(filp, (uvm_test_file_t *)ptr);
+    break;
 
-        default:
-            UVM_ASSERT_MSG(0, "Unexpected fd type: %d\n", fd_type);
-    }
+  default:
+    UVM_ASSERT_MSG(0, "Unexpected fd type: %d\n", fd_type);
+  }
 
-    return 0;
+  return 0;
 }
 
-static int uvm_release_entry(struct inode *inode, struct file *filp)
-{
-   UVM_ENTRY_RET(uvm_release(inode, filp));
+static int uvm_release_entry(struct inode *inode, struct file *filp) {
+  UVM_ENTRY_RET(uvm_release(inode, filp));
 }
 
-static void uvm_destroy_vma_managed(struct vm_area_struct *vma, bool make_zombie)
-{
-    uvm_va_range_managed_t *managed_range, *managed_range_next;
-    NvU64 size = 0;
-
-    uvm_assert_rwsem_locked_write(&uvm_va_space_get(vma->vm_file)->lock);
-    uvm_for_each_va_range_managed_in_vma_safe(managed_range, managed_range_next, vma) {
-        // On exit_mmap (process teardown), current->mm is cleared so
-        // uvm_va_range_vma_current would return NULL.
-        UVM_ASSERT(uvm_va_range_vma(managed_range) == vma);
-        UVM_ASSERT(managed_range->va_range.node.start >= vma->vm_start);
-        UVM_ASSERT(managed_range->va_range.node.end   <  vma->vm_end);
-        size += uvm_va_range_size(&managed_range->va_range);
-        if (make_zombie)
-            uvm_va_range_zombify(managed_range);
-        else
-            uvm_va_range_destroy(&managed_range->va_range, NULL);
-    }
+static void uvm_destroy_vma_managed(struct vm_area_struct *vma,
+                                    bool make_zombie) {
+  uvm_va_range_managed_t *managed_range, *managed_range_next;
+  NvU64 size = 0;
+
+  uvm_assert_rwsem_locked_write(&uvm_va_space_get(vma->vm_file)->lock);
+  uvm_for_each_va_range_managed_in_vma_safe(managed_range, managed_range_next,
+                                            vma) {
+    // On exit_mmap (process teardown), current->mm is cleared so
+    // uvm_va_range_vma_current would return NULL.
+    UVM_ASSERT(uvm_va_range_vma(managed_range) == vma);
+    UVM_ASSERT(managed_range->va_range.node.start >= vma->vm_start);
+    UVM_ASSERT(managed_range->va_range.node.end < vma->vm_end);
+    size += uvm_va_range_size(&managed_range->va_range);
+    if (make_zombie)
+      uvm_va_range_zombify(managed_range);
+    else
+      uvm_va_range_destroy(&managed_range->va_range, NULL);
+  }
 
-    if (vma->vm_private_data) {
-        uvm_vma_wrapper_destroy(vma->vm_private_data);
-        vma->vm_private_data = NULL;
-    }
-    UVM_ASSERT(size == vma->vm_end - vma->vm_start);
+  if (vma->vm_private_data) {
+    uvm_vma_wrapper_destroy(vma->vm_private_data);
+    vma->vm_private_data = NULL;
+  }
+  UVM_ASSERT(size == vma->vm_end - vma->vm_start);
 }
 
-static void uvm_destroy_vma_semaphore_pool(struct vm_area_struct *vma)
-{
-    uvm_va_range_semaphore_pool_t *semaphore_pool_range;
-    uvm_va_space_t *va_space;
+static void uvm_destroy_vma_semaphore_pool(struct vm_area_struct *vma) {
+  uvm_va_range_semaphore_pool_t *semaphore_pool_range;
+  uvm_va_space_t *va_space;
 
-    va_space = uvm_va_space_get(vma->vm_file);
-    uvm_assert_rwsem_locked(&va_space->lock);
-    semaphore_pool_range = uvm_va_range_semaphore_pool_find(va_space, vma->vm_start);
-    UVM_ASSERT(semaphore_pool_range &&
-               semaphore_pool_range->va_range.node.start   == vma->vm_start &&
-               semaphore_pool_range->va_range.node.end + 1 == vma->vm_end);
+  va_space = uvm_va_space_get(vma->vm_file);
+  uvm_assert_rwsem_locked(&va_space->lock);
+  semaphore_pool_range =
+      uvm_va_range_semaphore_pool_find(va_space, vma->vm_start);
+  UVM_ASSERT(semaphore_pool_range &&
+             semaphore_pool_range->va_range.node.start == vma->vm_start &&
+             semaphore_pool_range->va_range.node.end + 1 == vma->vm_end);
 
-    uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
+  uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
 }
 
 // The kernel will also SIGBUS faults to vmas with valid ops but no fault
 // handler, but it didn't always do that. Make it explicit so we don't rely on
 // the kernel's implementation.
-static vm_fault_t uvm_vm_fault_sigbus(struct vm_fault *vmf)
-{
-    UVM_DBG_PRINT_RL("Fault to address 0x%lx in disabled vma\n", vmf->address);
-    return VM_FAULT_SIGBUS;
+static vm_fault_t uvm_vm_fault_sigbus(struct vm_fault *vmf) {
+  UVM_DBG_PRINT_RL("Fault to address 0x%lx in disabled vma\n", vmf->address);
+  return VM_FAULT_SIGBUS;
 }
 
-static vm_fault_t uvm_vm_fault_sigbus_entry(struct vm_fault *vmf)
-{
-    UVM_ENTRY_RET(uvm_vm_fault_sigbus(vmf));
+static vm_fault_t uvm_vm_fault_sigbus_entry(struct vm_fault *vmf) {
+  UVM_ENTRY_RET(uvm_vm_fault_sigbus(vmf));
 }
 
-static struct vm_operations_struct uvm_vm_ops_disabled =
-{
+static struct vm_operations_struct uvm_vm_ops_disabled = {
     .fault = uvm_vm_fault_sigbus_entry,
 };
 
-static void uvm_disable_vma(struct vm_area_struct *vma)
-{
-    // In the case of fork, the kernel has already copied the old PTEs over to
-    // the child process, so an access in the child might succeed instead of
-    // causing a fault. To force a fault we'll unmap it directly here.
-    //
-    // Note that since the unmap works on file offset, not virtual address, this
-    // unmaps both the old and new vmas.
-    //
-    // In the case of a move (mremap), the kernel will copy the PTEs over later,
-    // so it doesn't matter if we unmap here. However, the new vma's open will
-    // immediately be followed by a close on the old vma. We call
-    // unmap_mapping_range for the close, which also unmaps the new vma because
-    // they have the same file offset.
-    unmap_mapping_range(vma->vm_file->f_mapping,
-                        vma->vm_pgoff << PAGE_SHIFT,
-                        vma->vm_end - vma->vm_start,
-                        1);
-
-    vma->vm_ops = &uvm_vm_ops_disabled;
-
-    if (vma->vm_private_data) {
-        uvm_vma_wrapper_destroy(vma->vm_private_data);
-        vma->vm_private_data = NULL;
-    }
+static void uvm_disable_vma(struct vm_area_struct *vma) {
+  // In the case of fork, the kernel has already copied the old PTEs over to
+  // the child process, so an access in the child might succeed instead of
+  // causing a fault. To force a fault we'll unmap it directly here.
+  //
+  // Note that since the unmap works on file offset, not virtual address, this
+  // unmaps both the old and new vmas.
+  //
+  // In the case of a move (mremap), the kernel will copy the PTEs over later,
+  // so it doesn't matter if we unmap here. However, the new vma's open will
+  // immediately be followed by a close on the old vma. We call
+  // unmap_mapping_range for the close, which also unmaps the new vma because
+  // they have the same file offset.
+  unmap_mapping_range(vma->vm_file->f_mapping, vma->vm_pgoff << PAGE_SHIFT,
+                      vma->vm_end - vma->vm_start, 1);
+
+  vma->vm_ops = &uvm_vm_ops_disabled;
+
+  if (vma->vm_private_data) {
+    uvm_vma_wrapper_destroy(vma->vm_private_data);
+    vma->vm_private_data = NULL;
+  }
 }
 
 // We can't return an error from uvm_vm_open so on failed splits
@@ -380,17 +368,16 @@ static void uvm_disable_vma(struct vm_area_struct *vma)
 // A failure likely means we're in OOM territory, so this should not
 // be common by any means, and the process might die anyway.
 static void uvm_vm_open_failure(struct vm_area_struct *original,
-                                struct vm_area_struct *new)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(new->vm_file);
-    static const bool make_zombie = false;
+                                struct vm_area_struct *new) {
+  uvm_va_space_t *va_space = uvm_va_space_get(new->vm_file);
+  static const bool make_zombie = false;
 
-    UVM_ASSERT(va_space == uvm_va_space_get(original->vm_file));
-    uvm_assert_rwsem_locked_write(&va_space->lock);
+  UVM_ASSERT(va_space == uvm_va_space_get(original->vm_file));
+  uvm_assert_rwsem_locked_write(&va_space->lock);
 
-    uvm_destroy_vma_managed(original, make_zombie);
-    uvm_disable_vma(original);
-    uvm_disable_vma(new);
+  uvm_destroy_vma_managed(original, make_zombie);
+  uvm_disable_vma(original);
+  uvm_disable_vma(new);
 }
 
 // vm_ops->open cases:
@@ -414,817 +401,838 @@ static void uvm_vm_open_failure(struct vm_area_struct *original,
 //
 // Note that since we set VM_DONTEXPAND on the vma we're guaranteed that the vma
 // will never increase in size, only shrink/split.
-static void uvm_vm_open_managed(struct vm_area_struct *vma)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
-    uvm_va_range_managed_t *managed_range;
-    struct vm_area_struct *original;
-    NV_STATUS status;
-    NvU64 new_end;
-
-    // This is slightly ugly. We need to know the parent vma of this new one,
-    // but we can't use the range tree to look up the original because that
-    // doesn't handle a vma move operation.
-    //
-    // However, all of the old vma's fields have been copied into the new vma,
-    // and open of the new vma is always called before close of the old (in
-    // cases where close will be called immediately afterwards, like move).
-    // vma->vm_private_data will thus still point to the original vma that we
-    // set in mmap or open.
-    //
-    // Things to watch out for here:
-    // - For splits, the old vma hasn't been adjusted yet so its vm_start and
-    //   vm_end region will overlap with this vma's start and end.
-    //
-    // - For splits and moves, the new vma has not yet been inserted into the
-    //   mm's list so vma->vm_prev and vma->vm_next cannot be used, nor will
-    //   the new vma show up in find_vma and friends.
-    original = ((uvm_vma_wrapper_t*)vma->vm_private_data)->vma;
-    vma->vm_private_data = NULL;
-    // On fork or move we want to simply disable the new vma
-    if (vma->vm_mm != original->vm_mm ||
-        (vma->vm_start != original->vm_start && vma->vm_end != original->vm_end)) {
-        uvm_disable_vma(vma);
-        return;
-    }
-
-    // At this point we are guaranteed that the mmap_lock is held in write
-    // mode.
-    uvm_record_lock_mmap_lock_write(current->mm);
-
-    // Split vmas should always fall entirely within the old one, and be on one
-    // side.
-    UVM_ASSERT(vma->vm_start >= original->vm_start && vma->vm_end <= original->vm_end);
-    UVM_ASSERT(vma->vm_start == original->vm_start || vma->vm_end == original->vm_end);
-
-    // The vma is splitting, so create a new range under this vma if necessary.
-    // The kernel handles splits in the middle of the vma by doing two separate
-    // splits so we just have to handle one vma splitting in two here.
-    if (vma->vm_start == original->vm_start)
-        new_end = vma->vm_end - 1; // Left split (new_end is inclusive)
-    else
-        new_end = vma->vm_start - 1; // Right split (new_end is inclusive)
-
-    uvm_va_space_down_write(va_space);
-
-    vma->vm_private_data = uvm_vma_wrapper_alloc(vma);
-    if (!vma->vm_private_data) {
-        uvm_vm_open_failure(original, vma);
-        goto out;
+static void uvm_vm_open_managed(struct vm_area_struct *vma) {
+  uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
+  uvm_va_range_managed_t *managed_range;
+  struct vm_area_struct *original;
+  NV_STATUS status;
+  NvU64 new_end;
+
+  // This is slightly ugly. We need to know the parent vma of this new one,
+  // but we can't use the range tree to look up the original because that
+  // doesn't handle a vma move operation.
+  //
+  // However, all of the old vma's fields have been copied into the new vma,
+  // and open of the new vma is always called before close of the old (in
+  // cases where close will be called immediately afterwards, like move).
+  // vma->vm_private_data will thus still point to the original vma that we
+  // set in mmap or open.
+  //
+  // Things to watch out for here:
+  // - For splits, the old vma hasn't been adjusted yet so its vm_start and
+  //   vm_end region will overlap with this vma's start and end.
+  //
+  // - For splits and moves, the new vma has not yet been inserted into the
+  //   mm's list so vma->vm_prev and vma->vm_next cannot be used, nor will
+  //   the new vma show up in find_vma and friends.
+  original = ((uvm_vma_wrapper_t *)vma->vm_private_data)->vma;
+  vma->vm_private_data = NULL;
+  // On fork or move we want to simply disable the new vma
+  if (vma->vm_mm != original->vm_mm || (vma->vm_start != original->vm_start &&
+                                        vma->vm_end != original->vm_end)) {
+    uvm_disable_vma(vma);
+    return;
+  }
+
+  // At this point we are guaranteed that the mmap_lock is held in write
+  // mode.
+  uvm_record_lock_mmap_lock_write(current->mm);
+
+  // Split vmas should always fall entirely within the old one, and be on one
+  // side.
+  UVM_ASSERT(vma->vm_start >= original->vm_start &&
+             vma->vm_end <= original->vm_end);
+  UVM_ASSERT(vma->vm_start == original->vm_start ||
+             vma->vm_end == original->vm_end);
+
+  // The vma is splitting, so create a new range under this vma if necessary.
+  // The kernel handles splits in the middle of the vma by doing two separate
+  // splits so we just have to handle one vma splitting in two here.
+  if (vma->vm_start == original->vm_start)
+    new_end = vma->vm_end - 1; // Left split (new_end is inclusive)
+  else
+    new_end = vma->vm_start - 1; // Right split (new_end is inclusive)
+
+  uvm_va_space_down_write(va_space);
+
+  vma->vm_private_data = uvm_vma_wrapper_alloc(vma);
+  if (!vma->vm_private_data) {
+    uvm_vm_open_failure(original, vma);
+    goto out;
+  }
+
+  // There can be multiple ranges under the vma already. Check if one spans
+  // the new split boundary. If so, split it.
+  managed_range = uvm_va_range_managed_find(va_space, new_end);
+  UVM_ASSERT(managed_range);
+  UVM_ASSERT(uvm_va_range_vma_current(managed_range) == original);
+  if (managed_range->va_range.node.end != new_end) {
+    status = uvm_va_range_split(managed_range, new_end, NULL);
+    if (status != NV_OK) {
+      UVM_DBG_PRINT("Failed to split VA range, destroying both: %s. "
+                    "original vma [0x%lx, 0x%lx) new vma [0x%lx, 0x%lx)\n",
+                    nvstatusToString(status), original->vm_start,
+                    original->vm_end, vma->vm_start, vma->vm_end);
+      uvm_vm_open_failure(original, vma);
+      goto out;
     }
+  }
 
-    // There can be multiple ranges under the vma already. Check if one spans
-    // the new split boundary. If so, split it.
-    managed_range = uvm_va_range_managed_find(va_space, new_end);
-    UVM_ASSERT(managed_range);
+  // Point managed_ranges to the new vma
+  uvm_for_each_va_range_managed_in_vma(managed_range, vma) {
     UVM_ASSERT(uvm_va_range_vma_current(managed_range) == original);
-    if (managed_range->va_range.node.end != new_end) {
-        status = uvm_va_range_split(managed_range, new_end, NULL);
-        if (status != NV_OK) {
-            UVM_DBG_PRINT("Failed to split VA range, destroying both: %s. "
-                          "original vma [0x%lx, 0x%lx) new vma [0x%lx, 0x%lx)\n",
-                          nvstatusToString(status),
-                          original->vm_start, original->vm_end,
-                          vma->vm_start, vma->vm_end);
-            uvm_vm_open_failure(original, vma);
-            goto out;
-        }
-    }
-
-    // Point managed_ranges to the new vma
-    uvm_for_each_va_range_managed_in_vma(managed_range, vma) {
-        UVM_ASSERT(uvm_va_range_vma_current(managed_range) == original);
-        managed_range->vma_wrapper = vma->vm_private_data;
-    }
+    managed_range->vma_wrapper = vma->vm_private_data;
+  }
 
 out:
-    uvm_va_space_up_write(va_space);
-    uvm_record_unlock_mmap_lock_write(current->mm);
+  uvm_va_space_up_write(va_space);
+  uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
-static void uvm_vm_open_managed_entry(struct vm_area_struct *vma)
-{
-   UVM_ENTRY_VOID(uvm_vm_open_managed(vma));
+static void uvm_vm_open_managed_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_open_managed(vma));
 }
 
-static void uvm_vm_close_managed(struct vm_area_struct *vma)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
-    bool make_zombie = false;
-
-    if (current->mm != NULL)
-        uvm_record_lock_mmap_lock_write(current->mm);
-
-    // current->mm will be NULL on process teardown, in which case we have
-    // special handling.
-    if (current->mm == NULL) {
-        make_zombie = (va_space->initialization_flags & UVM_INIT_FLAGS_MULTI_PROCESS_SHARING_MODE);
-        if (!make_zombie) {
-            // If we're not in multi-process mode, then we want to stop all user
-            // channels before unmapping the managed allocations to avoid
-            // spurious MMU faults in the system log. If we have a va_space_mm
-            // then this must've already happened as part of
-            // uvm_va_space_mm_shutdown. Otherwise we need to handle it here.
-            if (uvm_va_space_mm_enabled(va_space) && current->mm == va_space->va_space_mm.mm) {
-                UVM_ASSERT(atomic_read(&va_space->user_channels_stopped));
-            }
-            else {
-                // Stopping channels involves making RM calls, so we have to do
-                // that with the VA space lock in read mode.
-                uvm_va_space_down_read_rm(va_space);
-                if (!atomic_read(&va_space->user_channels_stopped))
-                    uvm_va_space_stop_all_user_channels(va_space);
-                uvm_va_space_up_read_rm(va_space);
-            }
-        }
+static void uvm_vm_close_managed(struct vm_area_struct *vma) {
+  uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
+  bool make_zombie = false;
+
+  if (current->mm != NULL)
+    uvm_record_lock_mmap_lock_write(current->mm);
+
+  // current->mm will be NULL on process teardown, in which case we have
+  // special handling.
+  if (current->mm == NULL) {
+    make_zombie = (va_space->initialization_flags &
+                   UVM_INIT_FLAGS_MULTI_PROCESS_SHARING_MODE);
+    if (!make_zombie) {
+      // If we're not in multi-process mode, then we want to stop all user
+      // channels before unmapping the managed allocations to avoid
+      // spurious MMU faults in the system log. If we have a va_space_mm
+      // then this must've already happened as part of
+      // uvm_va_space_mm_shutdown. Otherwise we need to handle it here.
+      if (uvm_va_space_mm_enabled(va_space) &&
+          current->mm == va_space->va_space_mm.mm) {
+        UVM_ASSERT(atomic_read(&va_space->user_channels_stopped));
+      } else {
+        // Stopping channels involves making RM calls, so we have to do
+        // that with the VA space lock in read mode.
+        uvm_va_space_down_read_rm(va_space);
+        if (!atomic_read(&va_space->user_channels_stopped))
+          uvm_va_space_stop_all_user_channels(va_space);
+        uvm_va_space_up_read_rm(va_space);
+      }
     }
+  }
 
-    // See uvm_mmap for why we need this in addition to mmap_lock
-    uvm_va_space_down_write(va_space);
+  // See uvm_mmap for why we need this in addition to mmap_lock
+  uvm_va_space_down_write(va_space);
 
-    uvm_destroy_vma_managed(vma, make_zombie);
+  uvm_destroy_vma_managed(vma, make_zombie);
 
-    uvm_va_space_up_write(va_space);
+  uvm_va_space_up_write(va_space);
 
-    if (current->mm != NULL)
-        uvm_record_unlock_mmap_lock_write(current->mm);
+  if (current->mm != NULL)
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
-static void uvm_vm_close_managed_entry(struct vm_area_struct *vma)
-{
-    UVM_ENTRY_VOID(uvm_vm_close_managed(vma));
+static void uvm_vm_close_managed_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_close_managed(vma));
 }
 
-static vm_fault_t uvm_vm_fault(struct vm_fault *vmf)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(vmf->vma->vm_file);
-    return uvm_va_space_cpu_fault_managed(va_space, vmf);
+static vm_fault_t uvm_vm_fault(struct vm_fault *vmf) {
+  uvm_va_space_t *va_space = uvm_va_space_get(vmf->vma->vm_file);
+  return uvm_va_space_cpu_fault_managed(va_space, vmf);
 }
 
-static vm_fault_t uvm_vm_fault_entry(struct vm_fault *vmf)
-{
-    UVM_ENTRY_RET(uvm_vm_fault(vmf));
+static vm_fault_t uvm_vm_fault_entry(struct vm_fault *vmf) {
+  UVM_ENTRY_RET(uvm_vm_fault(vmf));
 }
 
-static struct vm_operations_struct uvm_vm_ops_managed =
-{
-    .open         = uvm_vm_open_managed_entry,
-    .close        = uvm_vm_close_managed_entry,
-    .fault        = uvm_vm_fault_entry,
+static struct vm_operations_struct uvm_vm_ops_managed = {
+    .open = uvm_vm_open_managed_entry,
+    .close = uvm_vm_close_managed_entry,
+    .fault = uvm_vm_fault_entry,
     .page_mkwrite = uvm_vm_fault_entry,
 };
 
-// vm operations on semaphore pool allocations only control CPU mappings. Unmapping GPUs,
-// freeing the allocation, and destroying the range are handled by UVM_FREE.
-static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
-{
-    struct vm_area_struct *origin_vma = (struct vm_area_struct *)vma->vm_private_data;
-    uvm_va_space_t *va_space = uvm_va_space_get(origin_vma->vm_file);
-    uvm_va_range_semaphore_pool_t *semaphore_pool_range;
-    bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
-    NV_STATUS status;
-
-    uvm_record_lock_mmap_lock_write(current->mm);
-
-    uvm_va_space_down_write(va_space);
-
-    semaphore_pool_range = uvm_va_range_semaphore_pool_find(va_space, origin_vma->vm_start);
-    UVM_ASSERT(semaphore_pool_range);
-    UVM_ASSERT_MSG(semaphore_pool_range &&
-                   semaphore_pool_range->va_range.node.start == origin_vma->vm_start &&
-                   semaphore_pool_range->va_range.node.end + 1 == origin_vma->vm_end,
-                   "origin vma [0x%llx, 0x%llx); va_range [0x%llx, 0x%llx) type %d\n",
-                   (NvU64)origin_vma->vm_start,
-                   (NvU64)origin_vma->vm_end,
-                   semaphore_pool_range->va_range.node.start,
-                   semaphore_pool_range->va_range.node.end + 1,
-                   semaphore_pool_range->va_range.type);
-
-    // Semaphore pool vmas do not have vma wrappers, but some functions will
-    // assume vm_private_data is a wrapper.
-    vma->vm_private_data = NULL;
+// vm operations on semaphore pool allocations only control CPU mappings.
+// Unmapping GPUs, freeing the allocation, and destroying the range are handled
+// by UVM_FREE.
+static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma) {
+  struct vm_area_struct *origin_vma =
+      (struct vm_area_struct *)vma->vm_private_data;
+  uvm_va_space_t *va_space = uvm_va_space_get(origin_vma->vm_file);
+  uvm_va_range_semaphore_pool_t *semaphore_pool_range;
+  bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
+  NV_STATUS status;
+
+  uvm_record_lock_mmap_lock_write(current->mm);
+
+  uvm_va_space_down_write(va_space);
+
+  semaphore_pool_range =
+      uvm_va_range_semaphore_pool_find(va_space, origin_vma->vm_start);
+  UVM_ASSERT(semaphore_pool_range);
+  UVM_ASSERT_MSG(
+      semaphore_pool_range &&
+          semaphore_pool_range->va_range.node.start == origin_vma->vm_start &&
+          semaphore_pool_range->va_range.node.end + 1 == origin_vma->vm_end,
+      "origin vma [0x%llx, 0x%llx); va_range [0x%llx, 0x%llx) type %d\n",
+      (NvU64)origin_vma->vm_start, (NvU64)origin_vma->vm_end,
+      semaphore_pool_range->va_range.node.start,
+      semaphore_pool_range->va_range.node.end + 1,
+      semaphore_pool_range->va_range.type);
+
+  // Semaphore pool vmas do not have vma wrappers, but some functions will
+  // assume vm_private_data is a wrapper.
+  vma->vm_private_data = NULL;
 #if defined(VM_WIPEONFORK)
-    nv_vm_flags_set(vma, VM_WIPEONFORK);
+  nv_vm_flags_set(vma, VM_WIPEONFORK);
 #endif
 
-    if (is_fork) {
-        // If we forked, leave the parent vma alone.
-        uvm_disable_vma(vma);
+  if (is_fork) {
+    // If we forked, leave the parent vma alone.
+    uvm_disable_vma(vma);
 
-        // uvm_disable_vma unmaps in the parent as well; clear the uvm_mem CPU
-        // user mapping metadata and then remap.
-        uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
+    // uvm_disable_vma unmaps in the parent as well; clear the uvm_mem CPU
+    // user mapping metadata and then remap.
+    uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
 
-        status = uvm_mem_map_cpu_user(semaphore_pool_range->mem, semaphore_pool_range->va_range.va_space, origin_vma);
-        if (status != NV_OK) {
-            UVM_DBG_PRINT("Failed to remap semaphore pool to CPU for parent after fork; status = %d (%s)",
+    status = uvm_mem_map_cpu_user(semaphore_pool_range->mem,
+                                  semaphore_pool_range->va_range.va_space,
+                                  origin_vma);
+    if (status != NV_OK) {
+      UVM_DBG_PRINT("Failed to remap semaphore pool to CPU for parent after "
+                    "fork; status = %d (%s)",
                     status, nvstatusToString(status));
-            origin_vma->vm_ops = &uvm_vm_ops_disabled;
-        }
-    }
-    else {
-        origin_vma->vm_private_data = NULL;
-        origin_vma->vm_ops = &uvm_vm_ops_disabled;
-        vma->vm_ops = &uvm_vm_ops_disabled;
-        uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
+      origin_vma->vm_ops = &uvm_vm_ops_disabled;
     }
+  } else {
+    origin_vma->vm_private_data = NULL;
+    origin_vma->vm_ops = &uvm_vm_ops_disabled;
+    vma->vm_ops = &uvm_vm_ops_disabled;
+    uvm_mem_unmap_cpu_user(semaphore_pool_range->mem);
+  }
 
-    uvm_va_space_up_write(va_space);
+  uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_lock_write(current->mm);
+  uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
-static void uvm_vm_open_semaphore_pool_entry(struct vm_area_struct *vma)
-{
-   UVM_ENTRY_VOID(uvm_vm_open_semaphore_pool(vma));
+static void uvm_vm_open_semaphore_pool_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_open_semaphore_pool(vma));
 }
 
-// vm operations on semaphore pool allocations only control CPU mappings. Unmapping GPUs,
-// freeing the allocation, and destroying the va_range are handled by UVM_FREE.
-static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
+// vm operations on semaphore pool allocations only control CPU mappings.
+// Unmapping GPUs, freeing the allocation, and destroying the va_range are
+// handled by UVM_FREE.
+static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma) {
+  uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
 
-    if (current->mm != NULL)
-        uvm_record_lock_mmap_lock_write(current->mm);
+  if (current->mm != NULL)
+    uvm_record_lock_mmap_lock_write(current->mm);
 
-    uvm_va_space_down_read(va_space);
+  uvm_va_space_down_read(va_space);
 
-    uvm_destroy_vma_semaphore_pool(vma);
+  uvm_destroy_vma_semaphore_pool(vma);
 
-    uvm_va_space_up_read(va_space);
+  uvm_va_space_up_read(va_space);
 
-    if (current->mm != NULL)
-        uvm_record_unlock_mmap_lock_write(current->mm);
+  if (current->mm != NULL)
+    uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
-static void uvm_vm_close_semaphore_pool_entry(struct vm_area_struct *vma)
-{
-   UVM_ENTRY_VOID(uvm_vm_close_semaphore_pool(vma));
+static void uvm_vm_close_semaphore_pool_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_close_semaphore_pool(vma));
 }
 
-static struct vm_operations_struct uvm_vm_ops_semaphore_pool =
-{
-    .open         = uvm_vm_open_semaphore_pool_entry,
-    .close        = uvm_vm_close_semaphore_pool_entry,
-    .fault        = uvm_vm_fault_sigbus_entry,
+static struct vm_operations_struct uvm_vm_ops_semaphore_pool = {
+    .open = uvm_vm_open_semaphore_pool_entry,
+    .close = uvm_vm_close_semaphore_pool_entry,
+    .fault = uvm_vm_fault_sigbus_entry,
 };
 
-static void uvm_vm_open_device_p2p(struct vm_area_struct *vma)
-{
-    struct vm_area_struct *origin_vma = (struct vm_area_struct *)vma->vm_private_data;
-    uvm_va_space_t *va_space = uvm_va_space_get(origin_vma->vm_file);
-    uvm_va_range_t *va_range;
-    bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
-
-    uvm_record_lock_mmap_lock_write(current->mm);
-
-    uvm_va_space_down_write(va_space);
-
-    va_range = uvm_va_range_find(va_space, origin_vma->vm_start);
-    UVM_ASSERT(va_range);
-    UVM_ASSERT_MSG(va_range->type == UVM_VA_RANGE_TYPE_DEVICE_P2P &&
-                   va_range->node.start == origin_vma->vm_start &&
-                   va_range->node.end + 1 == origin_vma->vm_end,
-                   "origin vma [0x%llx, 0x%llx); va_range [0x%llx, 0x%llx) type %d\n",
-                   (NvU64)origin_vma->vm_start, (NvU64)origin_vma->vm_end, va_range->node.start,
-                   va_range->node.end + 1, va_range->type);
-
-    // Device P2P vmas do not have vma wrappers, but some functions will
-    // assume vm_private_data is a wrapper.
-    vma->vm_private_data = NULL;
+static void uvm_vm_open_device_p2p(struct vm_area_struct *vma) {
+  struct vm_area_struct *origin_vma =
+      (struct vm_area_struct *)vma->vm_private_data;
+  uvm_va_space_t *va_space = uvm_va_space_get(origin_vma->vm_file);
+  uvm_va_range_t *va_range;
+  bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
+
+  uvm_record_lock_mmap_lock_write(current->mm);
+
+  uvm_va_space_down_write(va_space);
+
+  va_range = uvm_va_range_find(va_space, origin_vma->vm_start);
+  UVM_ASSERT(va_range);
+  UVM_ASSERT_MSG(
+      va_range->type == UVM_VA_RANGE_TYPE_DEVICE_P2P &&
+          va_range->node.start == origin_vma->vm_start &&
+          va_range->node.end + 1 == origin_vma->vm_end,
+      "origin vma [0x%llx, 0x%llx); va_range [0x%llx, 0x%llx) type %d\n",
+      (NvU64)origin_vma->vm_start, (NvU64)origin_vma->vm_end,
+      va_range->node.start, va_range->node.end + 1, va_range->type);
+
+  // Device P2P vmas do not have vma wrappers, but some functions will
+  // assume vm_private_data is a wrapper.
+  vma->vm_private_data = NULL;
 #if defined(VM_WIPEONFORK)
-    nv_vm_flags_set(vma, VM_WIPEONFORK);
+  nv_vm_flags_set(vma, VM_WIPEONFORK);
 #endif
 
-    if (is_fork) {
-        // If we forked, leave the parent vma alone.
-        uvm_disable_vma(vma);
-
-        // uvm_disable_vma unmaps in the parent as well so remap the parent
-        uvm_va_range_device_p2p_map_cpu(va_range->va_space, origin_vma, uvm_va_range_to_device_p2p(va_range));
-    }
-    else {
-        // mremap will free the backing pages via unmap so we can't support it.
-        origin_vma->vm_private_data = NULL;
-        origin_vma->vm_ops = &uvm_vm_ops_disabled;
-        vma->vm_ops = &uvm_vm_ops_disabled;
-        unmap_mapping_range(va_space->mapping, va_range->node.start, uvm_va_range_size(va_range), 1);
-    }
+  if (is_fork) {
+    // If we forked, leave the parent vma alone.
+    uvm_disable_vma(vma);
+
+    // uvm_disable_vma unmaps in the parent as well so remap the parent
+    uvm_va_range_device_p2p_map_cpu(va_range->va_space, origin_vma,
+                                    uvm_va_range_to_device_p2p(va_range));
+  } else {
+    // mremap will free the backing pages via unmap so we can't support it.
+    origin_vma->vm_private_data = NULL;
+    origin_vma->vm_ops = &uvm_vm_ops_disabled;
+    vma->vm_ops = &uvm_vm_ops_disabled;
+    unmap_mapping_range(va_space->mapping, va_range->node.start,
+                        uvm_va_range_size(va_range), 1);
+  }
 
-    uvm_va_space_up_write(va_space);
+  uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_lock_write(current->mm);
+  uvm_record_unlock_mmap_lock_write(current->mm);
 }
 
-static void uvm_vm_open_device_p2p_entry(struct vm_area_struct *vma)
-{
-    UVM_ENTRY_VOID(uvm_vm_open_device_p2p(vma));
+static void uvm_vm_open_device_p2p_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_open_device_p2p(vma));
 }
 
 // Device P2P pages are only mapped on the CPU. Pages are allocated externally
 // to UVM but destroying the range must unpin the RM object.
-static void uvm_vm_close_device_p2p(struct vm_area_struct *vma)
-{
-}
+static void uvm_vm_close_device_p2p(struct vm_area_struct *vma) {}
 
-static void uvm_vm_close_device_p2p_entry(struct vm_area_struct *vma)
-{
-    UVM_ENTRY_VOID(uvm_vm_close_device_p2p(vma));
+static void uvm_vm_close_device_p2p_entry(struct vm_area_struct *vma) {
+  UVM_ENTRY_VOID(uvm_vm_close_device_p2p(vma));
 }
 
-static struct vm_operations_struct uvm_vm_ops_device_p2p =
-{
-    .open         = uvm_vm_open_device_p2p_entry,
-    .close        = uvm_vm_close_device_p2p_entry,
-    .fault        = uvm_vm_fault_sigbus_entry,
+static struct vm_operations_struct uvm_vm_ops_device_p2p = {
+    .open = uvm_vm_open_device_p2p_entry,
+    .close = uvm_vm_close_device_p2p_entry,
+    .fault = uvm_vm_fault_sigbus_entry,
 };
 
-static bool va_range_type_expects_mmap(uvm_va_range_type_t type)
-{
-    switch (type) {
-        case UVM_VA_RANGE_TYPE_SEMAPHORE_POOL:
-        case UVM_VA_RANGE_TYPE_DEVICE_P2P:
-            return true;
+static bool va_range_type_expects_mmap(uvm_va_range_type_t type) {
+  switch (type) {
+  case UVM_VA_RANGE_TYPE_SEMAPHORE_POOL:
+  case UVM_VA_RANGE_TYPE_DEVICE_P2P:
+    return true;
 
-        // Although UVM_VA_RANGE_TYPE_MANAGED does support mmap, it doesn't
-        // expect mmap to be called on a pre-existing range. mmap itself creates
-        // the managed va range.
+    // Although UVM_VA_RANGE_TYPE_MANAGED does support mmap, it doesn't
+    // expect mmap to be called on a pre-existing range. mmap itself creates
+    // the managed va range.
 
-        default:
-            return false;
-    }
+  default:
+    return false;
+  }
 }
 
-static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
-{
-    void *fd_type_ptr;
-    uvm_va_space_t *va_space;
-    NV_STATUS status = uvm_global_get_status();
-    int ret = 0;
-    bool vma_wrapper_allocated = false;
-
-    if (status != NV_OK)
-        return -nv_status_to_errno(status);
-
-    switch (uvm_fd_type(filp, &fd_type_ptr)) {
-        case UVM_FD_VA_SPACE:
-            va_space = (uvm_va_space_t *)fd_type_ptr;
-            break;
+static int uvm_mmap(struct file *filp, struct vm_area_struct *vma) {
+  void *fd_type_ptr;
+  uvm_va_space_t *va_space;
+  NV_STATUS status = uvm_global_get_status();
+  int ret = 0;
+  bool vma_wrapper_allocated = false;
+
+  if (status != NV_OK)
+    return -nv_status_to_errno(status);
+
+  switch (uvm_fd_type(filp, &fd_type_ptr)) {
+  case UVM_FD_VA_SPACE:
+    va_space = (uvm_va_space_t *)fd_type_ptr;
+    break;
+
+  case UVM_FD_TEST:
+    return uvm_test_file_mmap((uvm_test_file_t *)fd_type_ptr, vma);
+
+  default:
+    return -EBADFD;
+  }
+
+  // When the VA space is associated with an mm, all vmas under the VA space
+  // must come from that mm.
+  if (uvm_va_space_mm_enabled(va_space)) {
+    UVM_ASSERT(va_space->va_space_mm.mm);
+    if (va_space->va_space_mm.mm != current->mm)
+      return -EOPNOTSUPP;
+  }
+
+  // UVM mappings are required to set offset == VA. This simplifies things
+  // since we don't have to worry about address aliasing (except for fork,
+  // handled separately) and it makes unmap_mapping_range simpler.
+  if (vma->vm_start != (vma->vm_pgoff << PAGE_SHIFT)) {
+    UVM_DBG_PRINT_RL("vm_start 0x%lx != vm_pgoff 0x%lx\n", vma->vm_start,
+                     vma->vm_pgoff << PAGE_SHIFT);
+    return -EINVAL;
+  }
+
+  // Enforce shared read/writable mappings so we get all fault callbacks
+  // without the kernel doing COW behind our backs. The user can still call
+  // mprotect to change protections, but that will only hurt user space.
+  if ((vma->vm_flags & (VM_SHARED | VM_READ | VM_WRITE)) !=
+      (VM_SHARED | VM_READ | VM_WRITE)) {
+    UVM_DBG_PRINT_RL("User requested non-shared or non-writable mapping\n");
+    return -EINVAL;
+  }
+
+  // If the PM lock cannot be acquired, disable the VMA and report success
+  // to the caller. The caller is expected to determine whether the
+  // map operation succeeded via an ioctl() call. This is necessary to
+  // safely handle MAP_FIXED, which needs to complete atomically to prevent
+  // the loss of the virtual address range.
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock)) {
+    uvm_disable_vma(vma);
+    return 0;
+  }
+
+  uvm_record_lock_mmap_lock_write(current->mm);
+
+  // VM_MIXEDMAP      Required to use vm_insert_page
+  //
+  // VM_DONTEXPAND    mremap can grow a vma in place without giving us any
+  //                  callback. We need to prevent this so our ranges stay
+  //                  up-to-date with the vma. This flag doesn't prevent
+  //                  mremap from moving the mapping elsewhere, nor from
+  //                  shrinking it. We can detect both of those cases however
+  //                  with vm_ops->open() and vm_ops->close() callbacks.
+  //
+  // Using VM_DONTCOPY would be nice, but madvise(MADV_DOFORK) can reset that
+  // so we have to handle vm_open on fork anyway. We could disable MADV_DOFORK
+  // with VM_IO, but that causes other mapping issues.
+  // Make the default behavior be VM_DONTCOPY to avoid the performance impact
+  // of removing CPU mappings in the parent on fork()+exec(). Users can call
+  // madvise(MDV_DOFORK) if the child process requires access to the
+  // allocation.
+  nv_vm_flags_set(vma, VM_MIXEDMAP | VM_DONTEXPAND | VM_DONTCOPY);
+
+  vma->vm_ops = &uvm_vm_ops_managed;
+
+  // This identity assignment is needed so uvm_vm_open can find its parent vma
+  vma->vm_private_data = uvm_vma_wrapper_alloc(vma);
+  if (!vma->vm_private_data) {
+    ret = -ENOMEM;
+    goto out;
+  }
+  vma_wrapper_allocated = true;
+
+  // The kernel has taken mmap_lock in write mode, but that doesn't prevent
+  // this va_space from being modified by the GPU fault path or from the ioctl
+  // path where we don't have this mm for sure, so we have to lock the VA
+  // space directly.
+  uvm_va_space_down_write(va_space);
+
+  // uvm_va_range_create_mmap will catch collisions. Below are some example
+  // cases which can cause collisions. There may be others.
+  // 1) An overlapping range was previously created with an ioctl, for example
+  //    for an external mapping.
+  // 2) This file was passed to another process via a UNIX domain socket
+  status = uvm_va_range_create_mmap(va_space, current->mm, vma->vm_private_data,
+                                    NULL);
+
+  if (status == NV_ERR_UVM_ADDRESS_IN_USE) {
+    uvm_va_range_t *existing_range = uvm_va_range_find(va_space, vma->vm_start);
+
+    // Does the existing range exactly match the vma and expects mmap?
+    if (existing_range && existing_range->node.start == vma->vm_start &&
+        existing_range->node.end + 1 == vma->vm_end &&
+        va_range_type_expects_mmap(existing_range->type)) {
+
+      // We speculatively initialized the managed vma before checking for
+      // collisions because we expect successful insertion to be the
+      // common case. Undo that.
+      uvm_vma_wrapper_destroy(vma->vm_private_data);
+      vma_wrapper_allocated = false;
+      vma->vm_private_data = vma;
+
+      switch (existing_range->type) {
+      case UVM_VA_RANGE_TYPE_SEMAPHORE_POOL:
+        vma->vm_ops = &uvm_vm_ops_semaphore_pool;
+        status = uvm_mem_map_cpu_user(
+            uvm_va_range_to_semaphore_pool(existing_range)->mem, va_space, vma);
+        break;
+
+      case UVM_VA_RANGE_TYPE_DEVICE_P2P:
+        vma->vm_ops = &uvm_vm_ops_device_p2p;
+        status = uvm_va_range_device_p2p_map_cpu(
+            va_space, vma, uvm_va_range_to_device_p2p(existing_range));
+        break;
+
+      default:
+        UVM_ASSERT(0);
+        break;
+      }
+    }
+  }
 
-        case UVM_FD_TEST:
-            return uvm_test_file_mmap((uvm_test_file_t *)fd_type_ptr, vma);
+  if (status != NV_OK) {
+    UVM_DBG_PRINT_RL(
+        "Failed to create or map VA range for vma [0x%lx, 0x%lx): %s\n",
+        vma->vm_start, vma->vm_end, nvstatusToString(status));
+    ret = -nv_status_to_errno(status);
+  }
 
-        default:
-            return -EBADFD;
-    }
+  uvm_va_space_up_write(va_space);
 
-    // When the VA space is associated with an mm, all vmas under the VA space
-    // must come from that mm.
-    if (uvm_va_space_mm_enabled(va_space)) {
-        UVM_ASSERT(va_space->va_space_mm.mm);
-        if (va_space->va_space_mm.mm != current->mm)
-            return -EOPNOTSUPP;
-    }
+out:
+  if (ret != 0 && vma_wrapper_allocated)
+    uvm_vma_wrapper_destroy(vma->vm_private_data);
 
-    // UVM mappings are required to set offset == VA. This simplifies things
-    // since we don't have to worry about address aliasing (except for fork,
-    // handled separately) and it makes unmap_mapping_range simpler.
-    if (vma->vm_start != (vma->vm_pgoff << PAGE_SHIFT)) {
-        UVM_DBG_PRINT_RL("vm_start 0x%lx != vm_pgoff 0x%lx\n", vma->vm_start, vma->vm_pgoff << PAGE_SHIFT);
-        return -EINVAL;
-    }
+  uvm_record_unlock_mmap_lock_write(current->mm);
 
-    // Enforce shared read/writable mappings so we get all fault callbacks
-    // without the kernel doing COW behind our backs. The user can still call
-    // mprotect to change protections, but that will only hurt user space.
-    if ((vma->vm_flags & (VM_SHARED|VM_READ|VM_WRITE)) !=
-                         (VM_SHARED|VM_READ|VM_WRITE)) {
-        UVM_DBG_PRINT_RL("User requested non-shared or non-writable mapping\n");
-        return -EINVAL;
-    }
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    // If the PM lock cannot be acquired, disable the VMA and report success
-    // to the caller. The caller is expected to determine whether the
-    // map operation succeeded via an ioctl() call. This is necessary to
-    // safely handle MAP_FIXED, which needs to complete atomically to prevent
-    // the loss of the virtual address range.
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock)) {
-        uvm_disable_vma(vma);
-        return 0;
-    }
+  return ret;
+}
 
-    uvm_record_lock_mmap_lock_write(current->mm);
+bool uvm_vma_is_managed(struct vm_area_struct *vma) {
+  return vma->vm_ops == &uvm_vm_ops_disabled ||
+         vma->vm_ops == &uvm_vm_ops_managed ||
+         vma->vm_ops == &uvm_vm_ops_semaphore_pool;
+}
 
-    // VM_MIXEDMAP      Required to use vm_insert_page
-    //
-    // VM_DONTEXPAND    mremap can grow a vma in place without giving us any
-    //                  callback. We need to prevent this so our ranges stay
-    //                  up-to-date with the vma. This flag doesn't prevent
-    //                  mremap from moving the mapping elsewhere, nor from
-    //                  shrinking it. We can detect both of those cases however
-    //                  with vm_ops->open() and vm_ops->close() callbacks.
-    //
-    // Using VM_DONTCOPY would be nice, but madvise(MADV_DOFORK) can reset that
-    // so we have to handle vm_open on fork anyway. We could disable MADV_DOFORK
-    // with VM_IO, but that causes other mapping issues.
-    // Make the default behavior be VM_DONTCOPY to avoid the performance impact
-    // of removing CPU mappings in the parent on fork()+exec(). Users can call
-    // madvise(MDV_DOFORK) if the child process requires access to the
-    // allocation.
-    nv_vm_flags_set(vma, VM_MIXEDMAP | VM_DONTEXPAND | VM_DONTCOPY);
-
-    vma->vm_ops = &uvm_vm_ops_managed;
-
-    // This identity assignment is needed so uvm_vm_open can find its parent vma
-    vma->vm_private_data = uvm_vma_wrapper_alloc(vma);
-    if (!vma->vm_private_data) {
-        ret = -ENOMEM;
-        goto out;
-    }
-    vma_wrapper_allocated = true;
-
-    // The kernel has taken mmap_lock in write mode, but that doesn't prevent
-    // this va_space from being modified by the GPU fault path or from the ioctl
-    // path where we don't have this mm for sure, so we have to lock the VA
-    // space directly.
-    uvm_va_space_down_write(va_space);
-
-    // uvm_va_range_create_mmap will catch collisions. Below are some example
-    // cases which can cause collisions. There may be others.
-    // 1) An overlapping range was previously created with an ioctl, for example
-    //    for an external mapping.
-    // 2) This file was passed to another process via a UNIX domain socket
-    status = uvm_va_range_create_mmap(va_space, current->mm, vma->vm_private_data, NULL);
-
-    if (status == NV_ERR_UVM_ADDRESS_IN_USE) {
-        uvm_va_range_t *existing_range = uvm_va_range_find(va_space, vma->vm_start);
-
-        // Does the existing range exactly match the vma and expects mmap?
-        if (existing_range &&
-            existing_range->node.start == vma->vm_start &&
-            existing_range->node.end + 1 == vma->vm_end &&
-            va_range_type_expects_mmap(existing_range->type)) {
-
-            // We speculatively initialized the managed vma before checking for
-            // collisions because we expect successful insertion to be the
-            // common case. Undo that.
-            uvm_vma_wrapper_destroy(vma->vm_private_data);
-            vma_wrapper_allocated = false;
-            vma->vm_private_data = vma;
-
-            switch (existing_range->type) {
-                case UVM_VA_RANGE_TYPE_SEMAPHORE_POOL:
-                    vma->vm_ops = &uvm_vm_ops_semaphore_pool;
-                    status = uvm_mem_map_cpu_user(uvm_va_range_to_semaphore_pool(existing_range)->mem, va_space, vma);
-                    break;
-
-                case UVM_VA_RANGE_TYPE_DEVICE_P2P:
-                    vma->vm_ops = &uvm_vm_ops_device_p2p;
-                    status = uvm_va_range_device_p2p_map_cpu(va_space,
-                                                             vma,
-                                                             uvm_va_range_to_device_p2p(existing_range));
-                    break;
-
-                default:
-                    UVM_ASSERT(0);
-                    break;
-            }
-        }
-    }
+static int uvm_mmap_entry(struct file *filp, struct vm_area_struct *vma) {
+  UVM_ENTRY_RET(uvm_mmap(filp, vma));
+}
 
+static NV_STATUS uvm_api_initialize(UVM_INITIALIZE_PARAMS *params,
+                                    struct file *filp) {
+  uvm_va_space_t *va_space;
+  NV_STATUS status;
+  uvm_fd_type_t old_fd_type;
+
+  // Normally we expect private_data == UVM_FD_UNINITIALIZED. However multiple
+  // threads may call this ioctl concurrently so we have to be careful to
+  // avoid initializing multiple va_spaces and/or leaking memory. To do this
+  // we do an atomic compare and swap. Only one thread will observe
+  // UVM_FD_UNINITIALIZED and that thread will allocate and setup the
+  // va_space.
+  //
+  // Other threads will either see UVM_FD_INITIALIZING or UVM_FD_VA_SPACE. In
+  // the case of UVM_FD_VA_SPACE we return success if and only if the
+  // initialization flags match. If another thread is still initializing the
+  // va_space we return NV_ERR_BUSY_RETRY.
+  //
+  // If va_space initialization fails we return the failure code and reset the
+  // FD state back to UVM_FD_UNINITIALIZED to allow another initialization
+  // attempt to be made. This is safe because other threads will have only had
+  // a chance to observe UVM_FD_INITIALIZING and not UVM_FD_VA_SPACE in this
+  // case.
+  old_fd_type = uvm_fd_type_init_cas(filp);
+  switch (old_fd_type) {
+  case UVM_FD_UNINITIALIZED:
+    status = uvm_va_space_create(filp->f_mapping, &va_space, params->flags);
     if (status != NV_OK) {
-        UVM_DBG_PRINT_RL("Failed to create or map VA range for vma [0x%lx, 0x%lx): %s\n",
-                         vma->vm_start, vma->vm_end, nvstatusToString(status));
-        ret = -nv_status_to_errno(status);
+      uvm_fd_type_set(filp, UVM_FD_UNINITIALIZED, NULL);
+      return status;
     }
 
-    uvm_va_space_up_write(va_space);
+    uvm_fd_type_set(filp, UVM_FD_VA_SPACE, va_space);
+    break;
 
-out:
-    if (ret != 0 && vma_wrapper_allocated)
-        uvm_vma_wrapper_destroy(vma->vm_private_data);
-
-    uvm_record_unlock_mmap_lock_write(current->mm);
-
-    uvm_up_read(&g_uvm_global.pm.lock);
+  case UVM_FD_VA_SPACE:
+    va_space = uvm_va_space_get(filp);
+    if (params->flags != va_space->initialization_flags)
+      status = NV_ERR_INVALID_ARGUMENT;
+    else
+      status = NV_OK;
 
-    return ret;
-}
+    break;
 
-bool uvm_vma_is_managed(struct vm_area_struct *vma)
-{
-    return vma->vm_ops == &uvm_vm_ops_disabled ||
-           vma->vm_ops == &uvm_vm_ops_managed ||
-           vma->vm_ops == &uvm_vm_ops_semaphore_pool;
-}
+  case UVM_FD_MM:
+  case UVM_FD_TEST:
+    status = NV_ERR_INVALID_ARGUMENT;
+    break;
 
-static int uvm_mmap_entry(struct file *filp, struct vm_area_struct *vma)
-{
-   UVM_ENTRY_RET(uvm_mmap(filp, vma));
-}
+  case UVM_FD_INITIALIZING:
+    status = NV_ERR_BUSY_RETRY;
+    break;
 
-static NV_STATUS uvm_api_initialize(UVM_INITIALIZE_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space;
-    NV_STATUS status;
-    uvm_fd_type_t old_fd_type;
-
-    // Normally we expect private_data == UVM_FD_UNINITIALIZED. However multiple
-    // threads may call this ioctl concurrently so we have to be careful to
-    // avoid initializing multiple va_spaces and/or leaking memory. To do this
-    // we do an atomic compare and swap. Only one thread will observe
-    // UVM_FD_UNINITIALIZED and that thread will allocate and setup the
-    // va_space.
-    //
-    // Other threads will either see UVM_FD_INITIALIZING or UVM_FD_VA_SPACE. In
-    // the case of UVM_FD_VA_SPACE we return success if and only if the
-    // initialization flags match. If another thread is still initializing the
-    // va_space we return NV_ERR_BUSY_RETRY.
-    //
-    // If va_space initialization fails we return the failure code and reset the
-    // FD state back to UVM_FD_UNINITIALIZED to allow another initialization
-    // attempt to be made. This is safe because other threads will have only had
-    // a chance to observe UVM_FD_INITIALIZING and not UVM_FD_VA_SPACE in this
-    // case.
-    old_fd_type = uvm_fd_type_init_cas(filp);
-    switch (old_fd_type) {
-        case UVM_FD_UNINITIALIZED:
-            status = uvm_va_space_create(filp->f_mapping, &va_space, params->flags);
-            if (status != NV_OK) {
-                uvm_fd_type_set(filp, UVM_FD_UNINITIALIZED, NULL);
-                return status;
-            }
-
-            uvm_fd_type_set(filp, UVM_FD_VA_SPACE, va_space);
-            break;
-
-        case UVM_FD_VA_SPACE:
-            va_space = uvm_va_space_get(filp);
-            if (params->flags != va_space->initialization_flags)
-                status = NV_ERR_INVALID_ARGUMENT;
-            else
-                status = NV_OK;
-
-            break;
-
-        case UVM_FD_MM:
-        case UVM_FD_TEST:
-            status = NV_ERR_INVALID_ARGUMENT;
-            break;
-
-        case UVM_FD_INITIALIZING:
-            status = NV_ERR_BUSY_RETRY;
-            break;
-
-        default:
-            UVM_ASSERT(0);
-            status = NV_ERR_INVALID_STATE; // Quiet compiler warnings
-            break;
-    }
+  default:
+    UVM_ASSERT(0);
+    status = NV_ERR_INVALID_STATE; // Quiet compiler warnings
+    break;
+  }
 
-    return status;
+  return status;
 }
 
-static NV_STATUS uvm_api_pageable_mem_access(UVM_PAGEABLE_MEM_ACCESS_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    params->pageableMemAccess = uvm_va_space_pageable_mem_access_supported(va_space) ? NV_TRUE : NV_FALSE;
-    return NV_OK;
+static NV_STATUS
+uvm_api_pageable_mem_access(UVM_PAGEABLE_MEM_ACCESS_PARAMS *params,
+                            struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  params->pageableMemAccess =
+      uvm_va_space_pageable_mem_access_supported(va_space) ? NV_TRUE : NV_FALSE;
+  return NV_OK;
 }
 
-static long uvm_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-    switch (cmd)
-    {
-        case UVM_DEINITIALIZE:
-            return 0;
-
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_INITIALIZE,                  uvm_api_initialize);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_MM_INITIALIZE,               uvm_api_mm_initialize);
-
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PAGEABLE_MEM_ACCESS,            uvm_api_pageable_mem_access);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PAGEABLE_MEM_ACCESS_ON_GPU,     uvm_api_pageable_mem_access_on_gpu);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_GPU,                   uvm_api_register_gpu);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_GPU,                 uvm_api_unregister_gpu);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CREATE_RANGE_GROUP,             uvm_api_create_range_group);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DESTROY_RANGE_GROUP,            uvm_api_destroy_range_group);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_PEER_ACCESS,             uvm_api_enable_peer_access);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_PEER_ACCESS,            uvm_api_disable_peer_access);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_RANGE_GROUP,                uvm_api_set_range_group);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CREATE_EXTERNAL_RANGE,          uvm_api_create_external_range);
-        UVM_ROUTE_CMD_ALLOC_INIT_CHECK(UVM_MAP_EXTERNAL_ALLOCATION,        uvm_api_map_external_allocation);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MAP_EXTERNAL_SPARSE,            uvm_api_map_external_sparse);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_FREE,                           uvm_api_free);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISCARD,                        uvm_api_discard);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PREVENT_MIGRATION_RANGE_GROUPS, uvm_api_prevent_migration_range_groups);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ALLOW_MIGRATION_RANGE_GROUPS,   uvm_api_allow_migration_range_groups);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_PREFERRED_LOCATION,         uvm_api_set_preferred_location);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNSET_PREFERRED_LOCATION,       uvm_api_unset_preferred_location);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_ACCESSED_BY,                uvm_api_set_accessed_by);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNSET_ACCESSED_BY,              uvm_api_unset_accessed_by);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_GPU_VASPACE,           uvm_api_register_gpu_va_space);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_GPU_VASPACE,         uvm_api_unregister_gpu_va_space);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_CHANNEL,               uvm_api_register_channel);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_CHANNEL,             uvm_api_unregister_channel);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_READ_DUPLICATION,        uvm_api_enable_read_duplication);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_READ_DUPLICATION,       uvm_api_disable_read_duplication);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MIGRATE,                        uvm_api_migrate);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_SYSTEM_WIDE_ATOMICS,     uvm_api_enable_system_wide_atomics);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_SYSTEM_WIDE_ATOMICS,    uvm_api_disable_system_wide_atomics);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_READ_PROCESS_MEMORY,      uvm_api_tools_read_process_memory);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_WRITE_PROCESS_MEMORY,     uvm_api_tools_write_process_memory);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE, uvm_api_tools_get_processor_uuid_table);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MAP_DYNAMIC_PARALLELISM_REGION, uvm_api_map_dynamic_parallelism_region);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNMAP_EXTERNAL,                 uvm_api_unmap_external);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MIGRATE_RANGE_GROUP,            uvm_api_migrate_range_group);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_FLUSH_EVENTS,             uvm_api_tools_flush_events);
-        UVM_ROUTE_CMD_ALLOC_INIT_CHECK(UVM_ALLOC_SEMAPHORE_POOL,           uvm_api_alloc_semaphore_pool);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CLEAN_UP_ZOMBIE_RESOURCES,      uvm_api_clean_up_zombie_resources);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_POPULATE_PAGEABLE,              uvm_api_populate_pageable);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_VALIDATE_VA_RANGE,              uvm_api_validate_va_range);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2,uvm_api_tools_get_processor_uuid_table_v2);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ALLOC_DEVICE_P2P,               uvm_api_alloc_device_p2p);
-        UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CLEAR_ALL_ACCESS_COUNTERS,      uvm_api_clear_all_access_counters);
-    }
+static long uvm_ioctl(struct file *filp, unsigned int cmd, unsigned long arg) {
+  switch (cmd) {
+  case UVM_DEINITIALIZE:
+    return 0;
 
-    // Try the test ioctls if none of the above matched
-    return uvm_test_ioctl(filp, cmd, arg);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_INITIALIZE, uvm_api_initialize);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_MM_INITIALIZE, uvm_api_mm_initialize);
+
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PAGEABLE_MEM_ACCESS,
+                                   uvm_api_pageable_mem_access);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PAGEABLE_MEM_ACCESS_ON_GPU,
+                                   uvm_api_pageable_mem_access_on_gpu);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_GPU, uvm_api_register_gpu);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_GPU, uvm_api_unregister_gpu);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CREATE_RANGE_GROUP,
+                                   uvm_api_create_range_group);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DESTROY_RANGE_GROUP,
+                                   uvm_api_destroy_range_group);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_PEER_ACCESS,
+                                   uvm_api_enable_peer_access);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_PEER_ACCESS,
+                                   uvm_api_disable_peer_access);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_RANGE_GROUP,
+                                   uvm_api_set_range_group);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CREATE_EXTERNAL_RANGE,
+                                   uvm_api_create_external_range);
+    UVM_ROUTE_CMD_ALLOC_INIT_CHECK(UVM_MAP_EXTERNAL_ALLOCATION,
+                                   uvm_api_map_external_allocation);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MAP_EXTERNAL_SPARSE,
+                                   uvm_api_map_external_sparse);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_FREE, uvm_api_free);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISCARD, uvm_api_discard);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_PREVENT_MIGRATION_RANGE_GROUPS,
+                                   uvm_api_prevent_migration_range_groups);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ALLOW_MIGRATION_RANGE_GROUPS,
+                                   uvm_api_allow_migration_range_groups);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_PREFERRED_LOCATION,
+                                   uvm_api_set_preferred_location);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNSET_PREFERRED_LOCATION,
+                                   uvm_api_unset_preferred_location);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_SET_ACCESSED_BY,
+                                   uvm_api_set_accessed_by);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNSET_ACCESSED_BY,
+                                   uvm_api_unset_accessed_by);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_GPU_VASPACE,
+                                   uvm_api_register_gpu_va_space);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_GPU_VASPACE,
+                                   uvm_api_unregister_gpu_va_space);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_REGISTER_CHANNEL,
+                                   uvm_api_register_channel);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNREGISTER_CHANNEL,
+                                   uvm_api_unregister_channel);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_READ_DUPLICATION,
+                                   uvm_api_enable_read_duplication);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_READ_DUPLICATION,
+                                   uvm_api_disable_read_duplication);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MIGRATE, uvm_api_migrate);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ENABLE_SYSTEM_WIDE_ATOMICS,
+                                   uvm_api_enable_system_wide_atomics);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DISABLE_SYSTEM_WIDE_ATOMICS,
+                                   uvm_api_disable_system_wide_atomics);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_READ_PROCESS_MEMORY,
+                                   uvm_api_tools_read_process_memory);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_WRITE_PROCESS_MEMORY,
+                                   uvm_api_tools_write_process_memory);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE,
+                                   uvm_api_tools_get_processor_uuid_table);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MAP_DYNAMIC_PARALLELISM_REGION,
+                                   uvm_api_map_dynamic_parallelism_region);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_UNMAP_EXTERNAL, uvm_api_unmap_external);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_MIGRATE_RANGE_GROUP,
+                                   uvm_api_migrate_range_group);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_FLUSH_EVENTS,
+                                   uvm_api_tools_flush_events);
+    UVM_ROUTE_CMD_ALLOC_INIT_CHECK(UVM_ALLOC_SEMAPHORE_POOL,
+                                   uvm_api_alloc_semaphore_pool);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CLEAN_UP_ZOMBIE_RESOURCES,
+                                   uvm_api_clean_up_zombie_resources);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_POPULATE_PAGEABLE,
+                                   uvm_api_populate_pageable);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_VALIDATE_VA_RANGE,
+                                   uvm_api_validate_va_range);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2,
+                                   uvm_api_tools_get_processor_uuid_table_v2);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_ALLOC_DEVICE_P2P,
+                                   uvm_api_alloc_device_p2p);
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_CLEAR_ALL_ACCESS_COUNTERS,
+                                   uvm_api_clear_all_access_counters);
+
+    // added by kymartin for dumping GPU memory.
+    UVM_ROUTE_CMD_STACK_INIT_CHECK(UVM_DUMP_GPU_MEMORY,
+                                   uvm_api_dump_gpu_memory);
+  }
+
+  // Try the test ioctls if none of the above matched
+  return uvm_test_ioctl(filp, cmd, arg);
 }
 
-static long uvm_unlocked_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-    long ret;
+static long uvm_unlocked_ioctl(struct file *filp, unsigned int cmd,
+                               unsigned long arg) {
+  long ret;
 
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
-        return -EAGAIN;
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
+    return -EAGAIN;
 
-    ret = uvm_ioctl(filp, cmd, arg);
+  ret = uvm_ioctl(filp, cmd, arg);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    uvm_thread_assert_all_unlocked();
+  uvm_thread_assert_all_unlocked();
 
-    return ret;
+  return ret;
 }
 
-static long uvm_unlocked_ioctl_entry(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-   UVM_ENTRY_RET(uvm_unlocked_ioctl(filp, cmd, arg));
+static long uvm_unlocked_ioctl_entry(struct file *filp, unsigned int cmd,
+                                     unsigned long arg) {
+  UVM_ENTRY_RET(uvm_unlocked_ioctl(filp, cmd, arg));
 }
 
-static const struct file_operations uvm_fops =
-{
-    .open            = uvm_open_entry,
-    .release         = uvm_release_entry,
-    .mmap            = uvm_mmap_entry,
-    .unlocked_ioctl  = uvm_unlocked_ioctl_entry,
+static const struct file_operations uvm_fops = {
+    .open = uvm_open_entry,
+    .release = uvm_release_entry,
+    .mmap = uvm_mmap_entry,
+    .unlocked_ioctl = uvm_unlocked_ioctl_entry,
 #if NVCPU_IS_X86_64
-    .compat_ioctl    = uvm_unlocked_ioctl_entry,
+    .compat_ioctl = uvm_unlocked_ioctl_entry,
 #endif
-    .owner           = THIS_MODULE,
+    .owner = THIS_MODULE,
 };
 
-NV_STATUS uvm_test_register_unload_state_buffer(UVM_TEST_REGISTER_UNLOAD_STATE_BUFFER_PARAMS *params, struct file *filp)
-{
-    long ret;
-    struct page *page;
-    NV_STATUS status = NV_OK;
+NV_STATUS uvm_test_register_unload_state_buffer(
+    UVM_TEST_REGISTER_UNLOAD_STATE_BUFFER_PARAMS *params, struct file *filp) {
+  long ret;
+  struct page *page;
+  NV_STATUS status = NV_OK;
 
-    if (!IS_ALIGNED(params->unload_state_buf, sizeof(NvU64)))
-        return NV_ERR_INVALID_ADDRESS;
+  if (!IS_ALIGNED(params->unload_state_buf, sizeof(NvU64)))
+    return NV_ERR_INVALID_ADDRESS;
 
-    // Hold mmap_lock to call get_user_pages(), the UVM locking helper functions
-    // are not used because unload_state_buf may be a managed memory pointer and
-    // therefore a locking assertion from the CPU fault handler could be fired.
-    nv_mmap_read_lock(current->mm);
-    ret = NV_PIN_USER_PAGES(params->unload_state_buf, 1, FOLL_WRITE, &page);
-    nv_mmap_read_unlock(current->mm);
+  // Hold mmap_lock to call get_user_pages(), the UVM locking helper functions
+  // are not used because unload_state_buf may be a managed memory pointer and
+  // therefore a locking assertion from the CPU fault handler could be fired.
+  nv_mmap_read_lock(current->mm);
+  ret = NV_PIN_USER_PAGES(params->unload_state_buf, 1, FOLL_WRITE, &page);
+  nv_mmap_read_unlock(current->mm);
 
-    if (ret < 0)
-        return errno_to_nv_status(ret);
-    UVM_ASSERT(ret == 1);
+  if (ret < 0)
+    return errno_to_nv_status(ret);
+  UVM_ASSERT(ret == 1);
 
-    uvm_mutex_lock(&g_uvm_global.global_lock);
+  uvm_mutex_lock(&g_uvm_global.global_lock);
 
-    if (g_uvm_global.unload_state.ptr) {
-        NV_UNPIN_USER_PAGE(page);
-        status = NV_ERR_IN_USE;
-        goto error;
-    }
+  if (g_uvm_global.unload_state.ptr) {
+    NV_UNPIN_USER_PAGE(page);
+    status = NV_ERR_IN_USE;
+    goto error;
+  }
 
-    g_uvm_global.unload_state.page = page;
-    g_uvm_global.unload_state.ptr = (NvU64 *)((char *)kmap(page) + (params->unload_state_buf & ~PAGE_MASK));
-    *g_uvm_global.unload_state.ptr = 0;
+  g_uvm_global.unload_state.page = page;
+  g_uvm_global.unload_state.ptr =
+      (NvU64 *)((char *)kmap(page) + (params->unload_state_buf & ~PAGE_MASK));
+  *g_uvm_global.unload_state.ptr = 0;
 
 error:
-    uvm_mutex_unlock(&g_uvm_global.global_lock);
+  uvm_mutex_unlock(&g_uvm_global.global_lock);
 
-    return status;
+  return status;
 }
 
-static void uvm_test_unload_state_exit(void)
-{
-    if (g_uvm_global.unload_state.ptr) {
-        kunmap(g_uvm_global.unload_state.page);
-        NV_UNPIN_USER_PAGE(g_uvm_global.unload_state.page);
-    }
+static void uvm_test_unload_state_exit(void) {
+  if (g_uvm_global.unload_state.ptr) {
+    kunmap(g_uvm_global.unload_state.page);
+    NV_UNPIN_USER_PAGE(g_uvm_global.unload_state.page);
+  }
 }
 
-static int uvm_chardev_create(void)
-{
-    dev_t uvm_dev;
-
-    int ret = alloc_chrdev_region(&g_uvm_base_dev,
-                                  0,
-                                  NVIDIA_UVM_NUM_MINOR_DEVICES,
-                                  NVIDIA_UVM_DEVICE_NAME);
-    if (ret != 0) {
-        UVM_ERR_PRINT("alloc_chrdev_region failed: %d\n", ret);
-        return ret;
-    }
-    uvm_dev = MKDEV(MAJOR(g_uvm_base_dev), NVIDIA_UVM_PRIMARY_MINOR_NUMBER);
-
-    uvm_init_character_device(&g_uvm_cdev, &uvm_fops);
-    ret = cdev_add(&g_uvm_cdev, uvm_dev, 1);
-    if (ret != 0) {
-        UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n", MAJOR(uvm_dev), MINOR(uvm_dev), ret);
-        unregister_chrdev_region(g_uvm_base_dev, NVIDIA_UVM_NUM_MINOR_DEVICES);
-        return ret;
-    }
+static int uvm_chardev_create(void) {
+  dev_t uvm_dev;
 
-    return 0;
-}
-
-static void uvm_chardev_exit(void)
-{
-    cdev_del(&g_uvm_cdev);
+  int ret = alloc_chrdev_region(
+      &g_uvm_base_dev, 0, NVIDIA_UVM_NUM_MINOR_DEVICES, NVIDIA_UVM_DEVICE_NAME);
+  if (ret != 0) {
+    UVM_ERR_PRINT("alloc_chrdev_region failed: %d\n", ret);
+    return ret;
+  }
+  uvm_dev = MKDEV(MAJOR(g_uvm_base_dev), NVIDIA_UVM_PRIMARY_MINOR_NUMBER);
+
+  uvm_init_character_device(&g_uvm_cdev, &uvm_fops);
+  ret = cdev_add(&g_uvm_cdev, uvm_dev, 1);
+  if (ret != 0) {
+    UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n", MAJOR(uvm_dev),
+                  MINOR(uvm_dev), ret);
     unregister_chrdev_region(g_uvm_base_dev, NVIDIA_UVM_NUM_MINOR_DEVICES);
-}
-
-static int uvm_init(void)
-{
-    bool initialized_globals = false;
-    bool added_device = false;
-    int ret;
-
-    NV_STATUS status = uvm_global_init();
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("uvm_global_init() failed: %s\n", nvstatusToString(status));
-        ret = -ENODEV;
-        goto error;
-    }
-    initialized_globals = true;
-
-    ret = uvm_chardev_create();
-    if (ret != 0) {
-        UVM_ERR_PRINT("uvm_chardev_create failed: %d\n", ret);
-        goto error;
-    }
-    added_device = true;
+    return ret;
+  }
 
-    ret = uvm_tools_init(g_uvm_base_dev);
-    if (ret != 0) {
-        UVM_ERR_PRINT("uvm_tools_init() failed: %d\n", ret);
-        goto error;
-    }
+  return 0;
+}
 
-    if (uvm_enable_builtin_tests)
-        UVM_INFO_PRINT("Built-in UVM tests are enabled. This is a security risk.\n");
+static void uvm_chardev_exit(void) {
+  cdev_del(&g_uvm_cdev);
+  unregister_chrdev_region(g_uvm_base_dev, NVIDIA_UVM_NUM_MINOR_DEVICES);
+}
 
-    return 0;
+static int uvm_init(void) {
+  bool initialized_globals = false;
+  bool added_device = false;
+  int ret;
+
+  NV_STATUS status = uvm_global_init();
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("uvm_global_init() failed: %s\n", nvstatusToString(status));
+    ret = -ENODEV;
+    goto error;
+  }
+  initialized_globals = true;
+
+  ret = uvm_chardev_create();
+  if (ret != 0) {
+    UVM_ERR_PRINT("uvm_chardev_create failed: %d\n", ret);
+    goto error;
+  }
+  added_device = true;
+
+  ret = uvm_tools_init(g_uvm_base_dev);
+  if (ret != 0) {
+    UVM_ERR_PRINT("uvm_tools_init() failed: %d\n", ret);
+    goto error;
+  }
+
+  if (uvm_enable_builtin_tests)
+    UVM_INFO_PRINT(
+        "Built-in UVM tests are enabled. This is a security risk.\n");
+
+  return 0;
 
 error:
-    if (added_device)
-        uvm_chardev_exit();
+  if (added_device)
+    uvm_chardev_exit();
 
-    if (initialized_globals)
-        uvm_global_exit();
+  if (initialized_globals)
+    uvm_global_exit();
 
-    UVM_ERR_PRINT("uvm init failed: %d\n", ret);
+  UVM_ERR_PRINT("uvm init failed: %d\n", ret);
 
-    return ret;
+  return ret;
 }
 
-static int __init uvm_init_entry(void)
-{
-   UVM_ENTRY_RET(uvm_init());
-}
+static int __init uvm_init_entry(void) { UVM_ENTRY_RET(uvm_init()); }
 
-static void uvm_exit(void)
-{
-    uvm_tools_exit();
-    uvm_chardev_exit();
+static void uvm_exit(void) {
+  uvm_tools_exit();
+  uvm_chardev_exit();
 
-    uvm_global_exit();
+  uvm_global_exit();
 
-    uvm_test_unload_state_exit();
+  uvm_test_unload_state_exit();
 }
 
-static void __exit uvm_exit_entry(void)
-{
-   UVM_ENTRY_VOID(uvm_exit());
-}
+static void __exit uvm_exit_entry(void) { UVM_ENTRY_VOID(uvm_exit()); }
 
 module_init(uvm_init_entry);
 module_exit(uvm_exit_entry);
diff --git a/kernel-open/nvidia-uvm/uvm_api.h b/kernel-open/nvidia-uvm/uvm_api.h
index 78a9fd02..6af4fdd5 100644
--- a/kernel-open/nvidia-uvm/uvm_api.h
+++ b/kernel-open/nvidia-uvm/uvm_api.h
@@ -24,15 +24,15 @@
 #ifndef __UVM_API_H__
 #define __UVM_API_H__
 
-#include "uvm_types.h"
+#include "nv_uvm_user_types.h"
 #include "uvm_common.h"
 #include "uvm_ioctl.h"
+#include "uvm_kvmalloc.h"
 #include "uvm_linux.h"
 #include "uvm_lock.h"
 #include "uvm_thread_context.h"
-#include "uvm_kvmalloc.h"
+#include "uvm_types.h"
 #include "uvm_va_space.h"
-#include "nv_uvm_user_types.h"
 
 // This weird number comes from UVM_PREVENT_MIGRATION_RANGE_GROUPS_PARAMS. That
 // ioctl is called frequently so we don't want to allocate a copy every time.
@@ -42,75 +42,73 @@
 // The UVM_ROUTE_CMD_* macros are only intended for use in the ioctl routines
 
 // If the BUILD_BUG_ON fires, use __UVM_ROUTE_CMD_ALLOC instead.
-#define __UVM_ROUTE_CMD_STACK(cmd, params_type, function_name, do_init_check)       \
-    case cmd:                                                                       \
-    {                                                                               \
-        params_type params;                                                         \
-        BUILD_BUG_ON(sizeof(params) > UVM_MAX_IOCTL_PARAM_STACK_SIZE);              \
-        if (copy_from_user(&params, (void __user*)arg, sizeof(params)))             \
-            return -EFAULT;                                                         \
-                                                                                    \
-        params.rmStatus = uvm_global_get_status();                                  \
-        if (params.rmStatus == NV_OK) {                                             \
-            if (do_init_check) {                                                    \
-                if (!uvm_fd_va_space(filp))                                         \
-                    params.rmStatus = NV_ERR_ILLEGAL_ACTION;                        \
-            }                                                                       \
-            if (likely(params.rmStatus == NV_OK))                                   \
-                params.rmStatus = function_name(&params, filp);                     \
-        }                                                                           \
-                                                                                    \
-        if (copy_to_user((void __user*)arg, &params, sizeof(params)))               \
-            return -EFAULT;                                                         \
-                                                                                    \
-        return 0;                                                                   \
-    }
+#define __UVM_ROUTE_CMD_STACK(cmd, params_type, function_name, do_init_check)  \
+  case cmd: {                                                                  \
+    params_type params;                                                        \
+    BUILD_BUG_ON(sizeof(params) > UVM_MAX_IOCTL_PARAM_STACK_SIZE);             \
+    if (copy_from_user(&params, (void __user *)arg, sizeof(params)))           \
+      return -EFAULT;                                                          \
+                                                                               \
+    params.rmStatus = uvm_global_get_status();                                 \
+    if (params.rmStatus == NV_OK) {                                            \
+      if (do_init_check) {                                                     \
+        if (!uvm_fd_va_space(filp))                                            \
+          params.rmStatus = NV_ERR_ILLEGAL_ACTION;                             \
+      }                                                                        \
+      if (likely(params.rmStatus == NV_OK))                                    \
+        params.rmStatus = function_name(&params, filp);                        \
+    }                                                                          \
+                                                                               \
+    if (copy_to_user((void __user *)arg, &params, sizeof(params)))             \
+      return -EFAULT;                                                          \
+                                                                               \
+    return 0;                                                                  \
+  }
 
 // We need to concatenate cmd##_PARAMS here to avoid the preprocessor's argument
 // prescan. Attempting concatenation in the lower-level macro will fail because
 // it will have been expanded to a literal by then.
-#define UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(cmd, function_name) \
-    __UVM_ROUTE_CMD_STACK(cmd, cmd##_PARAMS, function_name, false)
+#define UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(cmd, function_name)                  \
+  __UVM_ROUTE_CMD_STACK(cmd, cmd##_PARAMS, function_name, false)
 
-#define UVM_ROUTE_CMD_STACK_INIT_CHECK(cmd, function_name) \
-    __UVM_ROUTE_CMD_STACK(cmd, cmd##_PARAMS, function_name, true)
+#define UVM_ROUTE_CMD_STACK_INIT_CHECK(cmd, function_name)                     \
+  __UVM_ROUTE_CMD_STACK(cmd, cmd##_PARAMS, function_name, true)
 
 // If the BUILD_BUG_ON fires, use __UVM_ROUTE_CMD_STACK instead
-#define __UVM_ROUTE_CMD_ALLOC(cmd, params_type, function_name, do_init_check)           \
-    case cmd:                                                                           \
-    {                                                                                   \
-        int ret = 0;                                                                    \
-        params_type *params = uvm_kvmalloc(sizeof(*params));                            \
-        if (!params)                                                                    \
-            return -ENOMEM;                                                             \
-        BUILD_BUG_ON(sizeof(*params) <= UVM_MAX_IOCTL_PARAM_STACK_SIZE);                \
-        if (copy_from_user(params, (void __user*)arg, sizeof(*params))) {               \
-            uvm_kvfree(params);                                                         \
-            return -EFAULT;                                                             \
-        }                                                                               \
-                                                                                        \
-        params->rmStatus = uvm_global_get_status();                                     \
-        if (params->rmStatus == NV_OK) {                                                \
-            if (do_init_check) {                                                        \
-                if (!uvm_fd_va_space(filp))                                             \
-                    params->rmStatus = NV_ERR_ILLEGAL_ACTION;                           \
-            }                                                                           \
-            if (likely(params->rmStatus == NV_OK))                                      \
-                params->rmStatus = function_name(params, filp);                         \
-        }                                                                               \
-                                                                                        \
-        if (copy_to_user((void __user*)arg, params, sizeof(*params)))                   \
-            ret = -EFAULT;                                                              \
-                                                                                        \
-        uvm_kvfree(params);                                                             \
-        return ret;                                                                     \
-    }
+#define __UVM_ROUTE_CMD_ALLOC(cmd, params_type, function_name, do_init_check)  \
+  case cmd: {                                                                  \
+    int ret = 0;                                                               \
+    params_type *params = uvm_kvmalloc(sizeof(*params));                       \
+    if (!params)                                                               \
+      return -ENOMEM;                                                          \
+    BUILD_BUG_ON(sizeof(*params) <= UVM_MAX_IOCTL_PARAM_STACK_SIZE);           \
+    if (copy_from_user(params, (void __user *)arg, sizeof(*params))) {         \
+      uvm_kvfree(params);                                                      \
+      return -EFAULT;                                                          \
+    }                                                                          \
+                                                                               \
+    params->rmStatus = uvm_global_get_status();                                \
+    if (params->rmStatus == NV_OK) {                                           \
+      if (do_init_check) {                                                     \
+        if (!uvm_fd_va_space(filp))                                            \
+          params->rmStatus = NV_ERR_ILLEGAL_ACTION;                            \
+      }                                                                        \
+      if (likely(params->rmStatus == NV_OK))                                   \
+        params->rmStatus = function_name(params, filp);                        \
+    }                                                                          \
+                                                                               \
+    if (copy_to_user((void __user *)arg, params, sizeof(*params)))             \
+      ret = -EFAULT;                                                           \
+                                                                               \
+    uvm_kvfree(params);                                                        \
+    return ret;                                                                \
+  }
 
-#define UVM_ROUTE_CMD_ALLOC_NO_INIT_CHECK(cmd, function_name) \
-    __UVM_ROUTE_CMD_ALLOC(cmd, cmd##_PARAMS, function_name, false)
+#define UVM_ROUTE_CMD_ALLOC_NO_INIT_CHECK(cmd, function_name)                  \
+  __UVM_ROUTE_CMD_ALLOC(cmd, cmd##_PARAMS, function_name, false)
 
-#define UVM_ROUTE_CMD_ALLOC_INIT_CHECK(cmd, function_name) \
-    __UVM_ROUTE_CMD_ALLOC(cmd, cmd##_PARAMS, function_name, true)
+#define UVM_ROUTE_CMD_ALLOC_INIT_CHECK(cmd, function_name)                     \
+  __UVM_ROUTE_CMD_ALLOC(cmd, cmd##_PARAMS, function_name, true)
 
 // Wrap an entry point into the UVM module.
 //
@@ -136,77 +134,68 @@
 //
 // Invocations of foo must be replaced by invocations of foo_entry at the entry
 // points.
-#define UVM_ENTRY_WRAP(line)                                                        \
-    do {                                                                            \
-        bool added;                                                                 \
-                                                                                    \
-        if (in_interrupt()) {                                                       \
-            line;                                                                   \
-        }                                                                           \
-        else if (uvm_thread_context_wrapper_is_used()) {                            \
-            uvm_thread_context_wrapper_t thread_context_wrapper;                    \
-                                                                                    \
-            added = uvm_thread_context_add(&thread_context_wrapper.context);        \
-            line;                                                                   \
-            if (added)                                                              \
-                uvm_thread_context_remove(&thread_context_wrapper.context);         \
-        }                                                                           \
-        else {                                                                      \
-            uvm_thread_context_t thread_context;                                    \
-                                                                                    \
-            added = uvm_thread_context_add(&thread_context);                        \
-            line;                                                                   \
-            if (added)                                                              \
-                uvm_thread_context_remove(&thread_context);                         \
-        }                                                                           \
-    } while (0)                                                                     \
+#define UVM_ENTRY_WRAP(line)                                                   \
+  do {                                                                         \
+    bool added;                                                                \
+                                                                               \
+    if (in_interrupt()) {                                                      \
+      line;                                                                    \
+    } else if (uvm_thread_context_wrapper_is_used()) {                         \
+      uvm_thread_context_wrapper_t thread_context_wrapper;                     \
+                                                                               \
+      added = uvm_thread_context_add(&thread_context_wrapper.context);         \
+      line;                                                                    \
+      if (added)                                                               \
+        uvm_thread_context_remove(&thread_context_wrapper.context);            \
+    } else {                                                                   \
+      uvm_thread_context_t thread_context;                                     \
+                                                                               \
+      added = uvm_thread_context_add(&thread_context);                         \
+      line;                                                                    \
+      if (added)                                                               \
+        uvm_thread_context_remove(&thread_context);                            \
+    }                                                                          \
+  } while (0)
 
 // Wrapper for non-void functions
-#define UVM_ENTRY_RET(func_call)               \
-    do {                                       \
-        typeof(func_call) ret;                 \
-        UVM_ENTRY_WRAP((ret = (func_call)));   \
-        return ret;                            \
-    } while (0)                                \
+#define UVM_ENTRY_RET(func_call)                                               \
+  do {                                                                         \
+    typeof(func_call) ret;                                                     \
+    UVM_ENTRY_WRAP((ret = (func_call)));                                       \
+    return ret;                                                                \
+  } while (0)
 
 // Wrapper for void functions
 #define UVM_ENTRY_VOID UVM_ENTRY_WRAP
 
 // Validate input ranges from the user with specific alignment requirement
-static bool uvm_api_range_invalid_aligned(NvU64 base, NvU64 length, NvU64 alignment)
-{
-    return !IS_ALIGNED(base, alignment)     ||
-           !IS_ALIGNED(length, alignment)   ||
-           base == 0                        ||
-           length == 0                      ||
-           base + length < base; // Overflow
+static bool uvm_api_range_invalid_aligned(NvU64 base, NvU64 length,
+                                          NvU64 alignment) {
+  return !IS_ALIGNED(base, alignment) || !IS_ALIGNED(length, alignment) ||
+         base == 0 || length == 0 || base + length < base; // Overflow
 }
 
 // Most APIs require PAGE_SIZE alignment
-static bool uvm_api_range_invalid(NvU64 base, NvU64 length)
-{
-    return uvm_api_range_invalid_aligned(base, length, PAGE_SIZE);
+static bool uvm_api_range_invalid(NvU64 base, NvU64 length) {
+  return uvm_api_range_invalid_aligned(base, length, PAGE_SIZE);
 }
 
 // Some APIs can only enforce 4K alignment as it's the smallest GPU page size
 // even when the smallest host page is larger.
-static bool uvm_api_range_invalid_4k(NvU64 base, NvU64 length)
-{
-    return uvm_api_range_invalid_aligned(base, length, UVM_PAGE_SIZE_4K);
+static bool uvm_api_range_invalid_4k(NvU64 base, NvU64 length) {
+  return uvm_api_range_invalid_aligned(base, length, UVM_PAGE_SIZE_4K);
 }
 
 // Verify alignment on a 64K boundary.
-static bool uvm_api_range_invalid_64k(NvU64 base, NvU64 length)
-{
-    return uvm_api_range_invalid_aligned(base, length, UVM_PAGE_SIZE_64K);
+static bool uvm_api_range_invalid_64k(NvU64 base, NvU64 length) {
+  return uvm_api_range_invalid_aligned(base, length, UVM_PAGE_SIZE_64K);
 }
 
-typedef enum
-{
-    UVM_API_RANGE_TYPE_MANAGED,
-    UVM_API_RANGE_TYPE_HMM,
-    UVM_API_RANGE_TYPE_ATS,
-    UVM_API_RANGE_TYPE_INVALID
+typedef enum {
+  UVM_API_RANGE_TYPE_MANAGED,
+  UVM_API_RANGE_TYPE_HMM,
+  UVM_API_RANGE_TYPE_ATS,
+  UVM_API_RANGE_TYPE_INVALID
 } uvm_api_range_type_t;
 
 // If the interval [base, base + length) is fully covered by VMAs which all have
@@ -218,51 +207,107 @@ typedef enum
 //
 // LOCKING: va_space->lock must be held in at least read mode. If mm != NULL,
 //          mm->mmap_lock must also be held in at least read mode.
-uvm_api_range_type_t uvm_api_range_type_check(uvm_va_space_t *va_space, struct mm_struct *mm, NvU64 base, NvU64 length);
+uvm_api_range_type_t uvm_api_range_type_check(uvm_va_space_t *va_space,
+                                              struct mm_struct *mm, NvU64 base,
+                                              NvU64 length);
 
-NV_STATUS uvm_api_pageable_mem_access_on_gpu(UVM_PAGEABLE_MEM_ACCESS_ON_GPU_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_register_gpu(UVM_REGISTER_GPU_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unregister_gpu(UVM_UNREGISTER_GPU_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_create_range_group(UVM_CREATE_RANGE_GROUP_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_destroy_range_group(UVM_DESTROY_RANGE_GROUP_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_enable_peer_access(UVM_ENABLE_PEER_ACCESS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_disable_peer_access(UVM_DISABLE_PEER_ACCESS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_set_range_group(UVM_SET_RANGE_GROUP_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_create_external_range(UVM_CREATE_EXTERNAL_RANGE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_map_external_allocation(UVM_MAP_EXTERNAL_ALLOCATION_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_map_external_sparse(UVM_MAP_EXTERNAL_SPARSE_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_pageable_mem_access_on_gpu(
+    UVM_PAGEABLE_MEM_ACCESS_ON_GPU_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_register_gpu(UVM_REGISTER_GPU_PARAMS *params,
+                               struct file *filp);
+NV_STATUS uvm_api_unregister_gpu(UVM_UNREGISTER_GPU_PARAMS *params,
+                                 struct file *filp);
+NV_STATUS uvm_api_create_range_group(UVM_CREATE_RANGE_GROUP_PARAMS *params,
+                                     struct file *filp);
+NV_STATUS uvm_api_destroy_range_group(UVM_DESTROY_RANGE_GROUP_PARAMS *params,
+                                      struct file *filp);
+NV_STATUS uvm_api_enable_peer_access(UVM_ENABLE_PEER_ACCESS_PARAMS *params,
+                                     struct file *filp);
+NV_STATUS uvm_api_disable_peer_access(UVM_DISABLE_PEER_ACCESS_PARAMS *params,
+                                      struct file *filp);
+NV_STATUS uvm_api_set_range_group(UVM_SET_RANGE_GROUP_PARAMS *params,
+                                  struct file *filp);
+NV_STATUS
+uvm_api_create_external_range(UVM_CREATE_EXTERNAL_RANGE_PARAMS *params,
+                              struct file *filp);
+NV_STATUS
+uvm_api_map_external_allocation(UVM_MAP_EXTERNAL_ALLOCATION_PARAMS *params,
+                                struct file *filp);
+NV_STATUS uvm_api_map_external_sparse(UVM_MAP_EXTERNAL_SPARSE_PARAMS *params,
+                                      struct file *filp);
 NV_STATUS uvm_api_free(UVM_FREE_PARAMS *params, struct file *filp);
 NV_STATUS uvm_api_discard(UVM_DISCARD_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_prevent_migration_range_groups(UVM_PREVENT_MIGRATION_RANGE_GROUPS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_allow_migration_range_groups(UVM_ALLOW_MIGRATION_RANGE_GROUPS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_set_accessed_by(const UVM_SET_ACCESSED_BY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unset_accessed_by(const UVM_UNSET_ACCESSED_BY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_register_gpu_va_space(UVM_REGISTER_GPU_VASPACE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unregister_gpu_va_space(UVM_UNREGISTER_GPU_VASPACE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_register_channel(UVM_REGISTER_CHANNEL_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unregister_channel(UVM_UNREGISTER_CHANNEL_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_enable_read_duplication(const UVM_ENABLE_READ_DUPLICATION_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_disable_read_duplication(const UVM_DISABLE_READ_DUPLICATION_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_prevent_migration_range_groups(
+    UVM_PREVENT_MIGRATION_RANGE_GROUPS_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_allow_migration_range_groups(
+    UVM_ALLOW_MIGRATION_RANGE_GROUPS_PARAMS *params, struct file *filp);
+NV_STATUS
+uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS *params,
+                               struct file *filp);
+NV_STATUS uvm_api_unset_preferred_location(
+    const UVM_UNSET_PREFERRED_LOCATION_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_set_accessed_by(const UVM_SET_ACCESSED_BY_PARAMS *params,
+                                  struct file *filp);
+NV_STATUS uvm_api_unset_accessed_by(const UVM_UNSET_ACCESSED_BY_PARAMS *params,
+                                    struct file *filp);
+NV_STATUS uvm_api_register_gpu_va_space(UVM_REGISTER_GPU_VASPACE_PARAMS *params,
+                                        struct file *filp);
+NV_STATUS
+uvm_api_unregister_gpu_va_space(UVM_UNREGISTER_GPU_VASPACE_PARAMS *params,
+                                struct file *filp);
+NV_STATUS uvm_api_register_channel(UVM_REGISTER_CHANNEL_PARAMS *params,
+                                   struct file *filp);
+NV_STATUS uvm_api_unregister_channel(UVM_UNREGISTER_CHANNEL_PARAMS *params,
+                                     struct file *filp);
+NV_STATUS uvm_api_enable_read_duplication(
+    const UVM_ENABLE_READ_DUPLICATION_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_disable_read_duplication(
+    const UVM_DISABLE_READ_DUPLICATION_PARAMS *params, struct file *filp);
 NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_enable_system_wide_atomics(UVM_ENABLE_SYSTEM_WIDE_ATOMICS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_disable_system_wide_atomics(UVM_DISABLE_SYSTEM_WIDE_ATOMICS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_init_event_tracker_v2(UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_set_notification_threshold(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_event_queue_enable_events(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_event_queue_disable_events(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_write_process_memory(UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_map_dynamic_parallelism_region(UVM_MAP_DYNAMIC_PARALLELISM_REGION_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_unmap_external(UVM_UNMAP_EXTERNAL_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_alloc_semaphore_pool(UVM_ALLOC_SEMAPHORE_POOL_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_alloc_device_p2p(UVM_ALLOC_DEVICE_P2P_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_clear_all_access_counters(UVM_CLEAR_ALL_ACCESS_COUNTERS_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_enable_system_wide_atomics(
+    UVM_ENABLE_SYSTEM_WIDE_ATOMICS_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_disable_system_wide_atomics(
+    UVM_DISABLE_SYSTEM_WIDE_ATOMICS_PARAMS *params, struct file *filp);
+NV_STATUS
+uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *params,
+                                 struct file *filp);
+NV_STATUS uvm_api_tools_init_event_tracker_v2(
+    UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_set_notification_threshold(
+    UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_event_queue_enable_events(
+    UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_event_queue_disable_events(
+    UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS *params, struct file *filp);
+NV_STATUS
+uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *params,
+                              struct file *filp);
+NV_STATUS
+uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *params,
+                               struct file *filp);
+NV_STATUS
+uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params,
+                                  struct file *filp);
+NV_STATUS uvm_api_tools_write_process_memory(
+    UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_map_dynamic_parallelism_region(
+    UVM_MAP_DYNAMIC_PARALLELISM_REGION_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_unmap_external(UVM_UNMAP_EXTERNAL_PARAMS *params,
+                                 struct file *filp);
+NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params,
+                                      struct file *filp);
+NV_STATUS uvm_api_alloc_semaphore_pool(UVM_ALLOC_SEMAPHORE_POOL_PARAMS *params,
+                                       struct file *filp);
+NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params,
+                                    struct file *filp);
+NV_STATUS uvm_api_alloc_device_p2p(UVM_ALLOC_DEVICE_P2P_PARAMS *params,
+                                   struct file *filp);
+NV_STATUS
+uvm_api_clear_all_access_counters(UVM_CLEAR_ALL_ACCESS_COUNTERS_PARAMS *params,
+                                  struct file *filp);
+
+// added by kymartin for dumping GPU memory.
+NV_STATUS uvm_api_dump_gpu_memory(UVM_DUMP_GPU_MEMORY_PARAMS *params,
+                                  struct file *filep);
 
 #endif // __UVM_API_H__
diff --git a/kernel-open/nvidia-uvm/uvm_gpu.c b/kernel-open/nvidia-uvm/uvm_gpu.c
index 265a2c46..0f6cefe5 100644
--- a/kernel-open/nvidia-uvm/uvm_gpu.c
+++ b/kernel-open/nvidia-uvm/uvm_gpu.c
@@ -21,30 +21,30 @@
 
 *******************************************************************************/
 
+#include "uvm_gpu.h"
+#include "ctrl2080mc.h"
+#include "nv-kthread-q.h"
 #include "nv_uvm_interface.h"
 #include "uvm_api.h"
+#include "uvm_ats.h"
 #include "uvm_channel.h"
+#include "uvm_common.h"
+#include "uvm_conf_computing.h"
 #include "uvm_global.h"
-#include "uvm_gpu.h"
+#include "uvm_gpu_access_counters.h"
 #include "uvm_gpu_semaphore.h"
 #include "uvm_hal.h"
-#include "uvm_procfs.h"
-#include "uvm_pmm_gpu.h"
-#include "uvm_pmm_sysmem.h"
-#include "uvm_va_space.h"
-#include "uvm_user_channel.h"
+#include "uvm_kvmalloc.h"
+#include "uvm_linux.h"
+#include "uvm_mmu.h"
 #include "uvm_perf_events.h"
 #include "uvm_perf_heuristics.h"
-#include "uvm_common.h"
-#include "ctrl2080mc.h"
-#include "nv-kthread-q.h"
-#include "uvm_gpu_access_counters.h"
-#include "uvm_ats.h"
+#include "uvm_pmm_gpu.h"
+#include "uvm_pmm_sysmem.h"
+#include "uvm_procfs.h"
 #include "uvm_test.h"
-#include "uvm_conf_computing.h"
-#include "uvm_linux.h"
-#include "uvm_mmu.h"
-#include "uvm_kvmalloc.h"
+#include "uvm_user_channel.h"
+#include "uvm_va_space.h"
 
 #define UVM_PROC_GPUS_PEER_DIR_NAME "peers"
 
@@ -54,115 +54,118 @@
 #define UVM_PARAM_PEER_COPY_PHYSICAL "phys"
 static char *uvm_peer_copy = UVM_PARAM_PEER_COPY_PHYSICAL;
 module_param(uvm_peer_copy, charp, S_IRUGO);
-MODULE_PARM_DESC(uvm_peer_copy, "Choose the addressing mode for peer copying, options: "
-                                UVM_PARAM_PEER_COPY_PHYSICAL " [default] or " UVM_PARAM_PEER_COPY_VIRTUAL ". "
-                                "Valid for Ampere+ GPUs.");
-
-static uvm_user_channel_t *get_user_channel(uvm_rb_tree_node_t *node)
-{
-    return container_of(node, uvm_user_channel_t, instance_ptr.node);
-}
-
-static uvm_gpu_link_type_t get_gpu_link_type(UVM_LINK_TYPE link_type)
-{
-    switch (link_type) {
-        case UVM_LINK_TYPE_PCIE:
-            return UVM_GPU_LINK_PCIE;
-        case UVM_LINK_TYPE_PCIE_BAR1:
-            return UVM_GPU_LINK_PCIE_BAR1;
-        case UVM_LINK_TYPE_NVLINK_1:
-            return UVM_GPU_LINK_NVLINK_1;
-        case UVM_LINK_TYPE_NVLINK_2:
-            return UVM_GPU_LINK_NVLINK_2;
-        case UVM_LINK_TYPE_NVLINK_3:
-            return UVM_GPU_LINK_NVLINK_3;
-        case UVM_LINK_TYPE_NVLINK_4:
-            return UVM_GPU_LINK_NVLINK_4;
-        case UVM_LINK_TYPE_NVLINK_5:
-            return UVM_GPU_LINK_NVLINK_5;
-        case UVM_LINK_TYPE_C2C:
-            return UVM_GPU_LINK_C2C;
-        default:
-            return UVM_GPU_LINK_INVALID;
-    }
-}
-
-static void fill_parent_gpu_info(uvm_parent_gpu_t *parent_gpu, const UvmGpuInfo *gpu_info)
-{
-    char uuid_buffer[UVM_UUID_STRING_LENGTH];
-
-    parent_gpu->rm_info = *gpu_info;
-
-    parent_gpu->system_bus.link = get_gpu_link_type(gpu_info->sysmemLink);
-    UVM_ASSERT(parent_gpu->system_bus.link != UVM_GPU_LINK_INVALID);
-
-    parent_gpu->ats_supported = gpu_info->atsSupport;
-
-    parent_gpu->system_bus.link_rate_mbyte_per_s = gpu_info->sysmemLinkRateMBps;
-
-    if (gpu_info->systemMemoryWindowSize > 0) {
-        // memory_window_end is inclusive but uvm_parent_gpu_is_coherent()
-        // checks memory_window_end > memory_window_start as its condition.
-        UVM_ASSERT(gpu_info->systemMemoryWindowSize > 1);
-        parent_gpu->system_bus.memory_window_start = gpu_info->systemMemoryWindowStart;
-        parent_gpu->system_bus.memory_window_end   = gpu_info->systemMemoryWindowStart +
-                                                     gpu_info->systemMemoryWindowSize - 1;
-    }
-
-    parent_gpu->nvswitch_info.is_nvswitch_connected = gpu_info->connectedToSwitch;
-    parent_gpu->peer_address_info.is_nvlink_direct_connected = gpu_info->nvlDirectConnect;
-
-    if (parent_gpu->peer_address_info.is_nvlink_direct_connected) {
-        parent_gpu->peer_address_info.peer_gpa_memory_window_start = gpu_info->nvlDirectConnectMemoryWindowStart;
-    }
-    else if (parent_gpu->nvswitch_info.is_nvswitch_connected) {
-        // nvswitch is routed via physical pages, where the upper 13-bits of the
-        // 47-bit address space holds the routing information for each peer.
-        // Currently, this is limited to a 16GB framebuffer window size.
-        parent_gpu->nvswitch_info.fabric_memory_window_start = gpu_info->nvswitchMemoryWindowStart;
-        parent_gpu->nvswitch_info.egm_fabric_memory_window_start = gpu_info->nvswitchEgmMemoryWindowStart;
-    }
-
-    parent_gpu->ats.non_pasid_ats_enabled = gpu_info->nonPasidAtsSupport;
-
-    // Non-PASID ATS cannot be disabled if present, but it implies PASID ATS
-    // support.
-    // TODO: Bug 5161018: Include integrated GPUs in the assert once they
-    // support PASID ATS.
-    if (UVM_ATS_SUPPORTED() && parent_gpu->ats.non_pasid_ats_enabled && gpu_info->systemMemoryWindowSize)
-        UVM_ASSERT(g_uvm_global.ats.supported);
-
-    uvm_uuid_string(uuid_buffer, &parent_gpu->uuid);
-    snprintf(parent_gpu->name,
-             sizeof(parent_gpu->name),
-             "ID %u: %s: " UVM_PARENT_GPU_UUID_PREFIX "%s",
-             uvm_parent_id_value(parent_gpu->id),
-             parent_gpu->rm_info.name,
-             uuid_buffer);
-}
-
-static NV_STATUS get_gpu_caps(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    UvmGpuCaps gpu_caps;
-
-    memset(&gpu_caps, 0, sizeof(gpu_caps));
+MODULE_PARM_DESC(uvm_peer_copy,
+                 "Choose the addressing mode for peer copying, "
+                 "options: " UVM_PARAM_PEER_COPY_PHYSICAL
+                 " [default] or " UVM_PARAM_PEER_COPY_VIRTUAL ". "
+                 "Valid for Ampere+ GPUs.");
+
+static uvm_user_channel_t *get_user_channel(uvm_rb_tree_node_t *node) {
+  return container_of(node, uvm_user_channel_t, instance_ptr.node);
+}
+
+static uvm_gpu_link_type_t get_gpu_link_type(UVM_LINK_TYPE link_type) {
+  switch (link_type) {
+  case UVM_LINK_TYPE_PCIE:
+    return UVM_GPU_LINK_PCIE;
+  case UVM_LINK_TYPE_PCIE_BAR1:
+    return UVM_GPU_LINK_PCIE_BAR1;
+  case UVM_LINK_TYPE_NVLINK_1:
+    return UVM_GPU_LINK_NVLINK_1;
+  case UVM_LINK_TYPE_NVLINK_2:
+    return UVM_GPU_LINK_NVLINK_2;
+  case UVM_LINK_TYPE_NVLINK_3:
+    return UVM_GPU_LINK_NVLINK_3;
+  case UVM_LINK_TYPE_NVLINK_4:
+    return UVM_GPU_LINK_NVLINK_4;
+  case UVM_LINK_TYPE_NVLINK_5:
+    return UVM_GPU_LINK_NVLINK_5;
+  case UVM_LINK_TYPE_C2C:
+    return UVM_GPU_LINK_C2C;
+  default:
+    return UVM_GPU_LINK_INVALID;
+  }
+}
+
+static void fill_parent_gpu_info(uvm_parent_gpu_t *parent_gpu,
+                                 const UvmGpuInfo *gpu_info) {
+  char uuid_buffer[UVM_UUID_STRING_LENGTH];
+
+  parent_gpu->rm_info = *gpu_info;
+
+  parent_gpu->system_bus.link = get_gpu_link_type(gpu_info->sysmemLink);
+  UVM_ASSERT(parent_gpu->system_bus.link != UVM_GPU_LINK_INVALID);
+
+  parent_gpu->ats_supported = gpu_info->atsSupport;
+
+  parent_gpu->system_bus.link_rate_mbyte_per_s = gpu_info->sysmemLinkRateMBps;
+
+  if (gpu_info->systemMemoryWindowSize > 0) {
+    // memory_window_end is inclusive but uvm_parent_gpu_is_coherent()
+    // checks memory_window_end > memory_window_start as its condition.
+    UVM_ASSERT(gpu_info->systemMemoryWindowSize > 1);
+    parent_gpu->system_bus.memory_window_start =
+        gpu_info->systemMemoryWindowStart;
+    parent_gpu->system_bus.memory_window_end =
+        gpu_info->systemMemoryWindowStart + gpu_info->systemMemoryWindowSize -
+        1;
+  }
+
+  parent_gpu->nvswitch_info.is_nvswitch_connected = gpu_info->connectedToSwitch;
+  parent_gpu->peer_address_info.is_nvlink_direct_connected =
+      gpu_info->nvlDirectConnect;
+
+  if (parent_gpu->peer_address_info.is_nvlink_direct_connected) {
+    parent_gpu->peer_address_info.peer_gpa_memory_window_start =
+        gpu_info->nvlDirectConnectMemoryWindowStart;
+  } else if (parent_gpu->nvswitch_info.is_nvswitch_connected) {
+    // nvswitch is routed via physical pages, where the upper 13-bits of the
+    // 47-bit address space holds the routing information for each peer.
+    // Currently, this is limited to a 16GB framebuffer window size.
+    parent_gpu->nvswitch_info.fabric_memory_window_start =
+        gpu_info->nvswitchMemoryWindowStart;
+    parent_gpu->nvswitch_info.egm_fabric_memory_window_start =
+        gpu_info->nvswitchEgmMemoryWindowStart;
+  }
+
+  parent_gpu->ats.non_pasid_ats_enabled = gpu_info->nonPasidAtsSupport;
+
+  // Non-PASID ATS cannot be disabled if present, but it implies PASID ATS
+  // support.
+  // TODO: Bug 5161018: Include integrated GPUs in the assert once they
+  // support PASID ATS.
+  if (UVM_ATS_SUPPORTED() && parent_gpu->ats.non_pasid_ats_enabled &&
+      gpu_info->systemMemoryWindowSize)
+    UVM_ASSERT(g_uvm_global.ats.supported);
+
+  uvm_uuid_string(uuid_buffer, &parent_gpu->uuid);
+  snprintf(parent_gpu->name, sizeof(parent_gpu->name),
+           "ID %u: %s: " UVM_PARENT_GPU_UUID_PREFIX "%s",
+           uvm_parent_id_value(parent_gpu->id), parent_gpu->rm_info.name,
+           uuid_buffer);
+}
+
+static NV_STATUS get_gpu_caps(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  UvmGpuCaps gpu_caps;
+
+  memset(&gpu_caps, 0, sizeof(gpu_caps));
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceQueryCaps(uvm_gpu_device_handle(gpu), &gpu_caps));
+  if (status != NV_OK)
+    return status;
 
-    status = uvm_rm_locked_call(nvUvmInterfaceQueryCaps(uvm_gpu_device_handle(gpu), &gpu_caps));
-    if (status != NV_OK)
-        return status;
+  if (gpu_caps.numaEnabled) {
+    UVM_ASSERT(uvm_parent_gpu_is_coherent(gpu->parent));
 
-    if (gpu_caps.numaEnabled) {
-        UVM_ASSERT(uvm_parent_gpu_is_coherent(gpu->parent));
+    gpu->mem_info.numa.enabled = true;
+    gpu->mem_info.numa.node_id = gpu_caps.numaNodeId;
+  } else {
+    gpu->mem_info.numa.node_id = NUMA_NO_NODE;
+  }
 
-        gpu->mem_info.numa.enabled = true;
-        gpu->mem_info.numa.node_id = gpu_caps.numaNodeId;
-    }
-    else {
-        gpu->mem_info.numa.node_id = NUMA_NO_NODE;
-    }
-
-    return NV_OK;
+  return NV_OK;
 }
 
 // Return a PASID to use with the internal address space (AS), or -1 if not
@@ -175,8 +178,7 @@ static NV_STATUS get_gpu_caps(uvm_gpu_t *gpu)
 // If non-PASID ATS is supported, then we can skip this because that will handle
 // things for us instead without us having to fake it with PASID ATS. This is
 // only to enable the feature on other ATS systems.
-static int gpu_get_internal_pasid(const uvm_gpu_t *gpu)
-{
+static int gpu_get_internal_pasid(const uvm_gpu_t *gpu) {
 #if UVM_ATS_SVA_SUPPORTED() && defined(NV_IOMMU_IS_DMA_DOMAIN_PRESENT)
 // iommu_is_dma_domain() was added in Linux 5.15 by commit bf3aed4660c6
 // ("iommu: Introduce explicit type for non-strict DMA domains")
@@ -196,2799 +198,2847 @@ static int gpu_get_internal_pasid(const uvm_gpu_t *gpu)
 #define UVM_INTERNAL_PASID 0
 #endif
 
-    // Enable internal ATS only if non-PASID ATS is disabled, but PASID ATS is
-    // enabled, and we are using 64kB base page size. The base page size
-    // limitation is needed to avoid GH180 MMU behaviour which does not refetch
-    // invalid 4K ATS translations on access (see bug 3949400). This also works
-    // in virtualized environments because the entire 64kB guest page has to be
-    // mapped and pinned by the hypervisor for device access.
-    if (!gpu->parent->ats.non_pasid_ats_enabled && g_uvm_global.ats.enabled &&
-        uvm_parent_gpu_supports_ats(gpu->parent) && PAGE_SIZE == UVM_PAGE_SIZE_64K) {
-        struct device *dev = &gpu->parent->pci_dev->dev;
-        struct iommu_domain *domain = iommu_get_domain_for_dev(dev);
-
-        // Check that the iommu domain is controlled by linux DMA API and
-        // return a valid reserved PASID. Using a reserved PASID is OK since
-        // this value is only used for the internal UVM address space that
-        // uses ATS only for GPA->SPA translations that don't use PASID.
-        //
-        // If a general reserved PASID is not available (e.g. non-smmuv3, <6.6)
-        // we'd need to to reserve a PASID from the IOMMU driver here, or risk
-        // PASID collision. Note that since the PASID should not be used during
-        // normal operation, the collision would only manifest in error paths.
-        if (domain && iommu_is_dma_domain(domain))
-            return UVM_INTERNAL_PASID;
-    }
+  // Enable internal ATS only if non-PASID ATS is disabled, but PASID ATS is
+  // enabled, and we are using 64kB base page size. The base page size
+  // limitation is needed to avoid GH180 MMU behaviour which does not refetch
+  // invalid 4K ATS translations on access (see bug 3949400). This also works
+  // in virtualized environments because the entire 64kB guest page has to be
+  // mapped and pinned by the hypervisor for device access.
+  if (!gpu->parent->ats.non_pasid_ats_enabled && g_uvm_global.ats.enabled &&
+      uvm_parent_gpu_supports_ats(gpu->parent) &&
+      PAGE_SIZE == UVM_PAGE_SIZE_64K) {
+    struct device *dev = &gpu->parent->pci_dev->dev;
+    struct iommu_domain *domain = iommu_get_domain_for_dev(dev);
+
+    // Check that the iommu domain is controlled by linux DMA API and
+    // return a valid reserved PASID. Using a reserved PASID is OK since
+    // this value is only used for the internal UVM address space that
+    // uses ATS only for GPA->SPA translations that don't use PASID.
+    //
+    // If a general reserved PASID is not available (e.g. non-smmuv3, <6.6)
+    // we'd need to to reserve a PASID from the IOMMU driver here, or risk
+    // PASID collision. Note that since the PASID should not be used during
+    // normal operation, the collision would only manifest in error paths.
+    if (domain && iommu_is_dma_domain(domain))
+      return UVM_INTERNAL_PASID;
+  }
 #endif
 
-    /* Invalid PASID for internal RM address space */
-    return -1;
+  /* Invalid PASID for internal RM address space */
+  return -1;
 }
 
-static NV_STATUS alloc_and_init_address_space(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    UvmGpuAddressSpaceInfo gpu_address_space_info = {0};
+static NV_STATUS alloc_and_init_address_space(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  UvmGpuAddressSpaceInfo gpu_address_space_info = {0};
 
-    status = uvm_rm_locked_call(nvUvmInterfaceAddressSpaceCreate(uvm_gpu_device_handle(gpu),
-                                                                 gpu->parent->rm_va_base,
-                                                                 gpu->parent->rm_va_size,
-                                                                 gpu_get_internal_pasid(gpu) != -1,
-                                                                 &gpu->rm_address_space,
-                                                                 &gpu_address_space_info));
-    if (status != NV_OK)
-        return status;
+  status = uvm_rm_locked_call(nvUvmInterfaceAddressSpaceCreate(
+      uvm_gpu_device_handle(gpu), gpu->parent->rm_va_base,
+      gpu->parent->rm_va_size, gpu_get_internal_pasid(gpu) != -1,
+      &gpu->rm_address_space, &gpu_address_space_info));
+  if (status != NV_OK)
+    return status;
 
-    UVM_ASSERT(gpu_address_space_info.bigPageSize <= NV_U32_MAX);
+  UVM_ASSERT(gpu_address_space_info.bigPageSize <= NV_U32_MAX);
 
-    gpu->big_page.internal_size = gpu_address_space_info.bigPageSize;
-    gpu->time.time0_register = gpu_address_space_info.time0Offset;
-    gpu->time.time1_register = gpu_address_space_info.time1Offset;
+  gpu->big_page.internal_size = gpu_address_space_info.bigPageSize;
+  gpu->time.time0_register = gpu_address_space_info.time0Offset;
+  gpu->time.time1_register = gpu_address_space_info.time1Offset;
 
-    gpu->max_subcontexts = gpu_address_space_info.maxSubctxCount;
+  gpu->max_subcontexts = gpu_address_space_info.maxSubctxCount;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-int uvm_device_p2p_static_bar(uvm_parent_gpu_t *parent_gpu)
-{
-    return nv_bar_index_to_os_bar_index(parent_gpu->pci_dev, NV_GPU_BAR_INDEX_FB);
+int uvm_device_p2p_static_bar(uvm_parent_gpu_t *parent_gpu) {
+  return nv_bar_index_to_os_bar_index(parent_gpu->pci_dev, NV_GPU_BAR_INDEX_FB);
 }
 
-static NV_STATUS get_gpu_fb_info(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    UvmGpuFbInfo fb_info = {0};
-
-    status = uvm_rm_locked_call(nvUvmInterfaceGetFbInfo(uvm_gpu_device_handle(gpu), &fb_info));
-    if (status != NV_OK)
-        return status;
+static NV_STATUS get_gpu_fb_info(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  UvmGpuFbInfo fb_info = {0};
 
-    if (!fb_info.bZeroFb) {
-        gpu->mem_info.size = ((NvU64)fb_info.heapSize + fb_info.reservedHeapSize) * 1024;
-        gpu->mem_info.max_allocatable_address = fb_info.maxAllocatableAddress;
-        gpu->mem_info.phys_start = (NvU64)fb_info.heapStart * 1024;
-    }
-
-    gpu->mem_info.max_vidmem_page_size = fb_info.maxVidmemPageSize;
-
-    return NV_OK;
-}
-
-static NV_STATUS get_gpu_ecc_info(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    UvmGpuEccInfo ecc_info = {0};
-
-    status = uvm_rm_locked_call(nvUvmInterfaceGetEccInfo(uvm_gpu_device_handle(gpu), &ecc_info));
-    if (status != NV_OK)
-        return status;
-
-    gpu->ecc.enabled = ecc_info.bEccEnabled;
-    if (gpu->ecc.enabled) {
-        gpu->ecc.hw_interrupt_tree_location = (volatile NvU32*)((char*)ecc_info.eccReadLocation + ecc_info.eccOffset);
-        UVM_ASSERT(gpu->ecc.hw_interrupt_tree_location != NULL);
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetFbInfo(uvm_gpu_device_handle(gpu), &fb_info));
+  if (status != NV_OK)
+    return status;
 
-        gpu->ecc.mask = ecc_info.eccMask;
-        UVM_ASSERT(gpu->ecc.mask != 0);
+  if (!fb_info.bZeroFb) {
+    gpu->mem_info.size =
+        ((NvU64)fb_info.heapSize + fb_info.reservedHeapSize) * 1024;
+    gpu->mem_info.max_allocatable_address = fb_info.maxAllocatableAddress;
+    gpu->mem_info.phys_start = (NvU64)fb_info.heapStart * 1024;
+  }
 
-        gpu->ecc.error_notifier = ecc_info.eccErrorNotifier;
-        UVM_ASSERT(gpu->ecc.error_notifier != NULL);
-    }
+  gpu->mem_info.max_vidmem_page_size = fb_info.maxVidmemPageSize;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static NV_STATUS get_gpu_nvlink_info(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    UvmGpuNvlinkInfo nvlink_info = {0};
+static NV_STATUS get_gpu_ecc_info(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  UvmGpuEccInfo ecc_info = {0};
 
-    status = uvm_rm_locked_call(nvUvmInterfaceGetNvlinkInfo(uvm_gpu_device_handle(gpu), &nvlink_info));
-    if (status != NV_OK)
-        return status;
-
-    atomic_set(&gpu->nvlink_status.injected_error, UVM_TEST_NVLINK_ERROR_NONE);
-
-    gpu->nvlink_status.enabled = nvlink_info.bNvlinkRecoveryEnabled;
-    if (gpu->nvlink_status.enabled) {
-        gpu->nvlink_status.hw_interrupt_tree_location =
-             (volatile NvU32*)((char*)nvlink_info.nvlinkReadLocation + nvlink_info.nvlinkOffset);
-        UVM_ASSERT(gpu->nvlink_status.hw_interrupt_tree_location != NULL);
-
-        gpu->nvlink_status.mask = nvlink_info.nvlinkMask;
-        UVM_ASSERT(gpu->nvlink_status.mask != 0);
-
-        gpu->nvlink_status.error_notifier = nvlink_info.nvlinkErrorNotifier;
-        UVM_ASSERT(gpu->nvlink_status.error_notifier != NULL);
-    }
-
-    return NV_OK;
-}
-
-NV_STATUS uvm_gpu_inject_nvlink_error(uvm_gpu_t *gpu, UVM_TEST_NVLINK_ERROR_TYPE error_type)
-{
-    UVM_ASSERT(gpu);
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetEccInfo(uvm_gpu_device_handle(gpu), &ecc_info));
+  if (status != NV_OK)
+    return status;
 
-    if (!uvm_enable_builtin_tests)
-        return NV_ERR_NOT_SUPPORTED;
+  gpu->ecc.enabled = ecc_info.bEccEnabled;
+  if (gpu->ecc.enabled) {
+    gpu->ecc.hw_interrupt_tree_location =
+        (volatile NvU32 *)((char *)ecc_info.eccReadLocation +
+                           ecc_info.eccOffset);
+    UVM_ASSERT(gpu->ecc.hw_interrupt_tree_location != NULL);
 
-    switch (error_type)
-    {
-    case UVM_TEST_NVLINK_ERROR_NONE:
-    case UVM_TEST_NVLINK_ERROR_RESOLVED:
-    case UVM_TEST_NVLINK_ERROR_UNRESOLVED:
-        break;
+    gpu->ecc.mask = ecc_info.eccMask;
+    UVM_ASSERT(gpu->ecc.mask != 0);
 
-    default:
-        return NV_ERR_INVALID_ARGUMENT;
-    }
+    gpu->ecc.error_notifier = ecc_info.eccErrorNotifier;
+    UVM_ASSERT(gpu->ecc.error_notifier != NULL);
+  }
 
-    atomic_set(&gpu->nvlink_status.injected_error, error_type);
-    return NV_OK;
+  return NV_OK;
 }
 
-static bool gpu_supports_uvm(uvm_parent_gpu_t *parent_gpu)
-{
-    // TODO: Bug 1757136: Add Linux SLI support. Until then, explicitly disable
-    //       UVM on SLI.
-    return parent_gpu->rm_info.subdeviceCount == 1;
-}
-
-bool uvm_gpu_can_address(uvm_gpu_t *gpu, NvU64 addr, NvU64 size)
-{
-    // Lower and upper address spaces are typically found in platforms that use
-    // the canonical address form.
-    NvU64 max_va_lower;
-    NvU64 min_va_upper;
-    NvU64 addr_end = addr + size - 1;
-    NvU8 gpu_addr_shift;
-    NvU8 cpu_addr_shift;
-    NvU8 addr_shift;
-
-    // Watch out for calling this too early in init
-    UVM_ASSERT(gpu->address_space_tree.hal);
-    UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
-    UVM_ASSERT(addr <= addr_end);
-    UVM_ASSERT(size > 0);
-
-    gpu_addr_shift = gpu->address_space_tree.hal->num_va_bits();
-    cpu_addr_shift = uvm_cpu_num_va_bits();
-    addr_shift = gpu_addr_shift;
-
-    // Pascal+ GPUs are capable of accessing kernel pointers in various modes
-    // by applying the same upper-bit checks that x86 or ARM CPU processors do.
-    // The x86 and ARM platforms use canonical form addresses. For ARM, even
-    // with Top-Byte Ignore enabled, the following logic validates addresses
-    // from the kernel VA range.
-    //
-    // The following diagram illustrates the valid (V) VA regions that can be
-    // mapped (or addressed) by the GPU/CPU when the CPU uses canonical form.
-    // (C) regions are only accessible by the CPU. Similarly, (G) regions
-    // are only accessible by the GPU. (X) regions are not addressible.
-    // Note that we only consider (V) regions, i.e., address ranges that are
-    // addressable by both, the CPU and GPU.
-    //
-    //               GPU MAX VA < CPU MAX VA           GPU MAX VA >= CPU MAX VA
-    //          0xF..F +----------------+          0xF..F +----------------+
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    // GPU MIN UPPER VA|----------------| CPU MIN UPPER VA|----------------|
-    //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
-    //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
-    // CPU MIN UPPER VA|----------------| GPU MIN UPPER VA|----------------|
-    //                 |XXXXXXXXXXXXXXXX|                 |XXXXXXXXXXXXXXXX|
-    //                 |XXXXXXXXXXXXXXXX|                 |XXXXXXXXXXXXXXXX|
-    // CPU MAX LOWER VA|----------------| GPU MAX LOWER VA|----------------|
-    //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
-    //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
-    // GPU MAX LOWER VA|----------------| CPU MAX LOWER VA|----------------|
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
-    //               0 +----------------+               0 +----------------+
-
-    // On Pascal+ GPUs.
-    if (gpu_addr_shift > 40) {
-        // On x86, when cpu_addr_shift > gpu_addr_shift, it means the CPU uses
-        // 5-level paging and the GPU is pre-Hopper. On Pascal-Ada GPUs (49b
-        // wide VA) we set addr_shift to match a 4-level paging x86 (48b wide).
-        // See more details on uvm_parent_gpu_canonical_address(..);
-        if (cpu_addr_shift > gpu_addr_shift)
-            addr_shift = NVCPU_IS_X86_64 ? 48 : 49;
-        else if (gpu_addr_shift == 57)
-            addr_shift = gpu_addr_shift;
-        else
-            addr_shift = cpu_addr_shift;
-    }
-
-    uvm_get_unaddressable_range(addr_shift, &max_va_lower, &min_va_upper);
-
-    return (addr_end < max_va_lower) || (addr >= min_va_upper);
-}
+static NV_STATUS get_gpu_nvlink_info(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  UvmGpuNvlinkInfo nvlink_info = {0};
 
-// The internal UVM VAS does not use canonical form addresses.
-bool uvm_gpu_can_address_kernel(uvm_gpu_t *gpu, NvU64 addr, NvU64 size)
-{
-    NvU64 addr_end = addr + size - 1;
-    NvU64 max_gpu_va;
-
-    // Watch out for calling this too early in init
-    UVM_ASSERT(gpu->address_space_tree.hal);
-    UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
-    UVM_ASSERT(addr <= addr_end);
-    UVM_ASSERT(size > 0);
-
-    max_gpu_va = 1ULL << gpu->address_space_tree.hal->num_va_bits();
-    return addr_end < max_gpu_va;
-}
-
-NvU64 uvm_parent_gpu_canonical_address(uvm_parent_gpu_t *parent_gpu, NvU64 addr)
-{
-    NvU8 gpu_addr_shift;
-    NvU8 cpu_addr_shift;
-    NvU8 addr_shift;
-    NvU64 input_addr = addr;
-
-    // When the CPU VA width is larger than GPU's, it means that:
-    // On ARM: the CPU is on LVA mode and the GPU is pre-Hopper.
-    // On x86: the CPU uses 5-level paging and the GPU is pre-Hopper.
-    // We sign-extend on the 48b on ARM and on the 47b on x86 to mirror the
-    // behavior of CPUs with smaller (than GPU) VA widths.
-    gpu_addr_shift = parent_gpu->arch_hal->mmu_mode_hal(UVM_PAGE_SIZE_64K)->num_va_bits();
-    cpu_addr_shift = uvm_cpu_num_va_bits();
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetNvlinkInfo(uvm_gpu_device_handle(gpu), &nvlink_info));
+  if (status != NV_OK)
+    return status;
 
+  atomic_set(&gpu->nvlink_status.injected_error, UVM_TEST_NVLINK_ERROR_NONE);
+
+  gpu->nvlink_status.enabled = nvlink_info.bNvlinkRecoveryEnabled;
+  if (gpu->nvlink_status.enabled) {
+    gpu->nvlink_status.hw_interrupt_tree_location =
+        (volatile NvU32 *)((char *)nvlink_info.nvlinkReadLocation +
+                           nvlink_info.nvlinkOffset);
+    UVM_ASSERT(gpu->nvlink_status.hw_interrupt_tree_location != NULL);
+
+    gpu->nvlink_status.mask = nvlink_info.nvlinkMask;
+    UVM_ASSERT(gpu->nvlink_status.mask != 0);
+
+    gpu->nvlink_status.error_notifier = nvlink_info.nvlinkErrorNotifier;
+    UVM_ASSERT(gpu->nvlink_status.error_notifier != NULL);
+  }
+
+  return NV_OK;
+}
+
+NV_STATUS uvm_gpu_inject_nvlink_error(uvm_gpu_t *gpu,
+                                      UVM_TEST_NVLINK_ERROR_TYPE error_type) {
+  UVM_ASSERT(gpu);
+
+  if (!uvm_enable_builtin_tests)
+    return NV_ERR_NOT_SUPPORTED;
+
+  switch (error_type) {
+  case UVM_TEST_NVLINK_ERROR_NONE:
+  case UVM_TEST_NVLINK_ERROR_RESOLVED:
+  case UVM_TEST_NVLINK_ERROR_UNRESOLVED:
+    break;
+
+  default:
+    return NV_ERR_INVALID_ARGUMENT;
+  }
+
+  atomic_set(&gpu->nvlink_status.injected_error, error_type);
+  return NV_OK;
+}
+
+static bool gpu_supports_uvm(uvm_parent_gpu_t *parent_gpu) {
+  // TODO: Bug 1757136: Add Linux SLI support. Until then, explicitly disable
+  //       UVM on SLI.
+  return parent_gpu->rm_info.subdeviceCount == 1;
+}
+
+bool uvm_gpu_can_address(uvm_gpu_t *gpu, NvU64 addr, NvU64 size) {
+  // Lower and upper address spaces are typically found in platforms that use
+  // the canonical address form.
+  NvU64 max_va_lower;
+  NvU64 min_va_upper;
+  NvU64 addr_end = addr + size - 1;
+  NvU8 gpu_addr_shift;
+  NvU8 cpu_addr_shift;
+  NvU8 addr_shift;
+
+  // Watch out for calling this too early in init
+  UVM_ASSERT(gpu->address_space_tree.hal);
+  UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
+  UVM_ASSERT(addr <= addr_end);
+  UVM_ASSERT(size > 0);
+
+  gpu_addr_shift = gpu->address_space_tree.hal->num_va_bits();
+  cpu_addr_shift = uvm_cpu_num_va_bits();
+  addr_shift = gpu_addr_shift;
+
+  // Pascal+ GPUs are capable of accessing kernel pointers in various modes
+  // by applying the same upper-bit checks that x86 or ARM CPU processors do.
+  // The x86 and ARM platforms use canonical form addresses. For ARM, even
+  // with Top-Byte Ignore enabled, the following logic validates addresses
+  // from the kernel VA range.
+  //
+  // The following diagram illustrates the valid (V) VA regions that can be
+  // mapped (or addressed) by the GPU/CPU when the CPU uses canonical form.
+  // (C) regions are only accessible by the CPU. Similarly, (G) regions
+  // are only accessible by the GPU. (X) regions are not addressible.
+  // Note that we only consider (V) regions, i.e., address ranges that are
+  // addressable by both, the CPU and GPU.
+  //
+  //               GPU MAX VA < CPU MAX VA           GPU MAX VA >= CPU MAX VA
+  //          0xF..F +----------------+          0xF..F +----------------+
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  // GPU MIN UPPER VA|----------------| CPU MIN UPPER VA|----------------|
+  //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
+  //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
+  // CPU MIN UPPER VA|----------------| GPU MIN UPPER VA|----------------|
+  //                 |XXXXXXXXXXXXXXXX|                 |XXXXXXXXXXXXXXXX|
+  //                 |XXXXXXXXXXXXXXXX|                 |XXXXXXXXXXXXXXXX|
+  // CPU MAX LOWER VA|----------------| GPU MAX LOWER VA|----------------|
+  //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
+  //                 |CCCCCCCCCCCCCCCC|                 |GGGGGGGGGGGGGGGG|
+  // GPU MAX LOWER VA|----------------| CPU MAX LOWER VA|----------------|
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  //                 |VVVVVVVVVVVVVVVV|                 |VVVVVVVVVVVVVVVV|
+  //               0 +----------------+               0 +----------------+
+
+  // On Pascal+ GPUs.
+  if (gpu_addr_shift > 40) {
+    // On x86, when cpu_addr_shift > gpu_addr_shift, it means the CPU uses
+    // 5-level paging and the GPU is pre-Hopper. On Pascal-Ada GPUs (49b
+    // wide VA) we set addr_shift to match a 4-level paging x86 (48b wide).
+    // See more details on uvm_parent_gpu_canonical_address(..);
     if (cpu_addr_shift > gpu_addr_shift)
-        addr_shift = NVCPU_IS_X86_64 ? 48 : 49;
+      addr_shift = NVCPU_IS_X86_64 ? 48 : 49;
     else if (gpu_addr_shift == 57)
-        addr_shift = gpu_addr_shift;
+      addr_shift = gpu_addr_shift;
     else
-        addr_shift = cpu_addr_shift;
+      addr_shift = cpu_addr_shift;
+  }
 
-    addr = (NvU64)((NvS64)(addr << (64 - addr_shift)) >> (64 - addr_shift));
+  uvm_get_unaddressable_range(addr_shift, &max_va_lower, &min_va_upper);
 
-    // This protection acts on when the address is not covered by the GPU's
-    // OOR_ADDR_CHECK. This can only happen when OOR_ADDR_CHECK is in
-    // permissive (NO_CHECK) mode.
-    if ((addr << (64 - gpu_addr_shift)) != (input_addr << (64 - gpu_addr_shift)))
-        return input_addr;
-
-    return addr;
+  return (addr_end < max_va_lower) || (addr >= min_va_upper);
 }
 
-static void gpu_info_print_ce_caps(uvm_gpu_t *gpu, struct seq_file *s)
-{
-    NvU32 i;
-    UvmGpuCopyEnginesCaps *ces_caps;
-    NV_STATUS status;
-
-    ces_caps = uvm_kvmalloc_zero(sizeof(*ces_caps));
-    if (!ces_caps) {
-        UVM_SEQ_OR_DBG_PRINT(s, "supported_ces: unavailable (no memory)\n");
-        return;
-    }
-
-    status = uvm_rm_locked_call(nvUvmInterfaceQueryCopyEnginesCaps(uvm_gpu_device_handle(gpu), ces_caps));
-    if (status != NV_OK) {
-        UVM_SEQ_OR_DBG_PRINT(s, "supported_ces: unavailable (query failed)\n");
-        goto out;
-    }
-
-    UVM_SEQ_OR_DBG_PRINT(s, "supported_ces:\n");
-    for (i = 0; i < UVM_COPY_ENGINE_COUNT_MAX; ++i) {
-        UvmGpuCopyEngineCaps *ce_caps = ces_caps->copyEngineCaps + i;
-
-        if (!ce_caps->supported)
-            continue;
-
-        UVM_SEQ_OR_DBG_PRINT(s, " ce %u pce mask 0x%08x grce %u shared %u sysmem read %u sysmem write %u sysmem %u "
-                             "nvlink p2p %u p2p %u secure %u\n",
-                             i,
-                             ce_caps->cePceMask,
-                             ce_caps->grce,
-                             ce_caps->shared,
-                             ce_caps->sysmemRead,
-                             ce_caps->sysmemWrite,
-                             ce_caps->sysmem,
-                             ce_caps->nvlinkP2p,
-                             ce_caps->p2p,
-                             ce_caps->secure);
-    }
+// The internal UVM VAS does not use canonical form addresses.
+bool uvm_gpu_can_address_kernel(uvm_gpu_t *gpu, NvU64 addr, NvU64 size) {
+  NvU64 addr_end = addr + size - 1;
+  NvU64 max_gpu_va;
+
+  // Watch out for calling this too early in init
+  UVM_ASSERT(gpu->address_space_tree.hal);
+  UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
+  UVM_ASSERT(addr <= addr_end);
+  UVM_ASSERT(size > 0);
+
+  max_gpu_va = 1ULL << gpu->address_space_tree.hal->num_va_bits();
+  return addr_end < max_gpu_va;
+}
+
+NvU64 uvm_parent_gpu_canonical_address(uvm_parent_gpu_t *parent_gpu,
+                                       NvU64 addr) {
+  NvU8 gpu_addr_shift;
+  NvU8 cpu_addr_shift;
+  NvU8 addr_shift;
+  NvU64 input_addr = addr;
+
+  // When the CPU VA width is larger than GPU's, it means that:
+  // On ARM: the CPU is on LVA mode and the GPU is pre-Hopper.
+  // On x86: the CPU uses 5-level paging and the GPU is pre-Hopper.
+  // We sign-extend on the 48b on ARM and on the 47b on x86 to mirror the
+  // behavior of CPUs with smaller (than GPU) VA widths.
+  gpu_addr_shift =
+      parent_gpu->arch_hal->mmu_mode_hal(UVM_PAGE_SIZE_64K)->num_va_bits();
+  cpu_addr_shift = uvm_cpu_num_va_bits();
+
+  if (cpu_addr_shift > gpu_addr_shift)
+    addr_shift = NVCPU_IS_X86_64 ? 48 : 49;
+  else if (gpu_addr_shift == 57)
+    addr_shift = gpu_addr_shift;
+  else
+    addr_shift = cpu_addr_shift;
+
+  addr = (NvU64)((NvS64)(addr << (64 - addr_shift)) >> (64 - addr_shift));
+
+  // This protection acts on when the address is not covered by the GPU's
+  // OOR_ADDR_CHECK. This can only happen when OOR_ADDR_CHECK is in
+  // permissive (NO_CHECK) mode.
+  if ((addr << (64 - gpu_addr_shift)) != (input_addr << (64 - gpu_addr_shift)))
+    return input_addr;
+
+  return addr;
+}
+
+static void gpu_info_print_ce_caps(uvm_gpu_t *gpu, struct seq_file *s) {
+  NvU32 i;
+  UvmGpuCopyEnginesCaps *ces_caps;
+  NV_STATUS status;
+
+  ces_caps = uvm_kvmalloc_zero(sizeof(*ces_caps));
+  if (!ces_caps) {
+    UVM_SEQ_OR_DBG_PRINT(s, "supported_ces: unavailable (no memory)\n");
+    return;
+  }
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceQueryCopyEnginesCaps(uvm_gpu_device_handle(gpu), ces_caps));
+  if (status != NV_OK) {
+    UVM_SEQ_OR_DBG_PRINT(s, "supported_ces: unavailable (query failed)\n");
+    goto out;
+  }
+
+  UVM_SEQ_OR_DBG_PRINT(s, "supported_ces:\n");
+  for (i = 0; i < UVM_COPY_ENGINE_COUNT_MAX; ++i) {
+    UvmGpuCopyEngineCaps *ce_caps = ces_caps->copyEngineCaps + i;
+
+    if (!ce_caps->supported)
+      continue;
+
+    UVM_SEQ_OR_DBG_PRINT(s,
+                         " ce %u pce mask 0x%08x grce %u shared %u sysmem read "
+                         "%u sysmem write %u sysmem %u "
+                         "nvlink p2p %u p2p %u secure %u\n",
+                         i, ce_caps->cePceMask, ce_caps->grce, ce_caps->shared,
+                         ce_caps->sysmemRead, ce_caps->sysmemWrite,
+                         ce_caps->sysmem, ce_caps->nvlinkP2p, ce_caps->p2p,
+                         ce_caps->secure);
+  }
 
 out:
-    uvm_kvfree(ces_caps);
-}
-
-static const char *uvm_gpu_virt_type_string(UVM_VIRT_MODE virtMode)
-{
-    BUILD_BUG_ON(UVM_VIRT_MODE_COUNT != 4);
-
-    switch (virtMode) {
-        UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_NONE);
-        UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_LEGACY);
-        UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_SRIOV_HEAVY);
-        UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_SRIOV_STANDARD);
-        UVM_ENUM_STRING_DEFAULT();
-    }
-}
-
-static const char *uvm_gpu_link_type_string(uvm_gpu_link_type_t link_type)
-{
-
-    BUILD_BUG_ON(UVM_GPU_LINK_MAX != 9);
-
-    switch (link_type) {
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_INVALID);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_PCIE);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_PCIE_BAR1);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_1);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_2);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_3);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_4);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_5);
-        UVM_ENUM_STRING_CASE(UVM_GPU_LINK_C2C);
-        UVM_ENUM_STRING_DEFAULT();
-    }
-}
-
-static void gpu_info_print_common(uvm_gpu_t *gpu, struct seq_file *s)
-{
-    const UvmGpuInfo *gpu_info = &gpu->parent->rm_info;
-    NvU64 num_pages_in;
-    NvU64 num_pages_out;
-    NvU64 mapped_cpu_pages_size;
-    NvU32 get;
-    NvU32 put;
-    NvU32 i;
-    unsigned int cpu;
-
-    UVM_SEQ_OR_DBG_PRINT(s, "GPU %s\n", uvm_gpu_name(gpu));
-    UVM_SEQ_OR_DBG_PRINT(s, "retained_count                         %llu\n", uvm_gpu_retained_count(gpu));
-    UVM_SEQ_OR_DBG_PRINT(s, "ecc                                    %s\n", gpu->ecc.enabled ? "enabled" : "disabled");
-    UVM_SEQ_OR_DBG_PRINT(s, "nvlink status and recovery             %s\n", gpu->nvlink_status.enabled ? "enabled" : "disabled");
-    if (gpu->parent->closest_cpu_numa_node == -1)
-        UVM_SEQ_OR_DBG_PRINT(s, "closest_cpu_numa_node                  n/a\n");
-    else
-        UVM_SEQ_OR_DBG_PRINT(s, "closest_cpu_numa_node                  %d\n", gpu->parent->closest_cpu_numa_node);
-
-    if (!uvm_procfs_is_debug_enabled())
-        return;
-
-    UVM_SEQ_OR_DBG_PRINT(s, "CPU link type                          %s\n",
-                         uvm_gpu_link_type_string(gpu->parent->system_bus.link));
-    UVM_SEQ_OR_DBG_PRINT(s, "CPU link bandwidth                     %uMBps\n",
-                         gpu->parent->system_bus.link_rate_mbyte_per_s);
-
-    UVM_SEQ_OR_DBG_PRINT(s, "architecture                           0x%X\n", gpu_info->gpuArch);
-    UVM_SEQ_OR_DBG_PRINT(s, "implementation                         0x%X\n", gpu_info->gpuImplementation);
-    UVM_SEQ_OR_DBG_PRINT(s, "gpcs                                   %u\n", gpu_info->gpcCount);
-    UVM_SEQ_OR_DBG_PRINT(s, "max_gpcs                               %u\n", gpu_info->maxGpcCount);
-    UVM_SEQ_OR_DBG_PRINT(s, "tpcs                                   %u\n", gpu_info->tpcCount);
-    UVM_SEQ_OR_DBG_PRINT(s, "max_tpcs_per_gpc                       %u\n", gpu_info->maxTpcPerGpcCount);
-    UVM_SEQ_OR_DBG_PRINT(s, "host_class                             0x%X\n", gpu_info->hostClass);
-    UVM_SEQ_OR_DBG_PRINT(s, "ce_class                               0x%X\n", gpu_info->ceClass);
-    UVM_SEQ_OR_DBG_PRINT(s, "virtualization_mode                    %s\n",
-                         uvm_gpu_virt_type_string(gpu_info->virtMode));
-    UVM_SEQ_OR_DBG_PRINT(s, "big_page_size                          %u\n", gpu->big_page.internal_size);
-    UVM_SEQ_OR_DBG_PRINT(s, "rm_va_base                             0x%llx\n", gpu->parent->rm_va_base);
-    UVM_SEQ_OR_DBG_PRINT(s, "rm_va_size                             0x%llx\n", gpu->parent->rm_va_size);
-    UVM_SEQ_OR_DBG_PRINT(s, "vidmem_start                           %llu (%llu MBs)\n",
-                         gpu->mem_info.phys_start,
-                         gpu->mem_info.phys_start / (1024 * 1024));
-    UVM_SEQ_OR_DBG_PRINT(s, "vidmem_size                            %llu (%llu MBs)\n",
-                         gpu->mem_info.size,
-                         gpu->mem_info.size / (1024 * 1024));
-    UVM_SEQ_OR_DBG_PRINT(s, "vidmem_max_allocatable                 0x%llx (%llu MBs)\n",
-                         gpu->mem_info.max_allocatable_address,
-                         gpu->mem_info.max_allocatable_address / (1024 * 1024));
-
-    if (gpu->mem_info.numa.enabled) {
-        NvU64 window_size = gpu->parent->system_bus.memory_window_end - gpu->parent->system_bus.memory_window_start + 1;
-        UVM_SEQ_OR_DBG_PRINT(s, "numa_node_id                           %u\n", uvm_gpu_numa_node(gpu));
-        UVM_SEQ_OR_DBG_PRINT(s, "memory_window_start                    0x%llx\n",
-                             gpu->parent->system_bus.memory_window_start);
-        UVM_SEQ_OR_DBG_PRINT(s, "memory_window_end                      0x%llx\n",
-                             gpu->parent->system_bus.memory_window_end);
-        UVM_SEQ_OR_DBG_PRINT(s, "system_memory_window_size              0x%llx (%llu MBs)\n",
-                             window_size,
-                             window_size / (1024 * 1024));
-    }
-
-    UVM_SEQ_OR_DBG_PRINT(s, "interrupts                             %llu\n", gpu->parent->isr.interrupt_count);
-
-    if (gpu->parent->isr.replayable_faults.handling) {
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_bh                   %llu\n",
-                             gpu->parent->isr.replayable_faults.stats.bottom_half_count);
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_bh/cpu\n");
-        for_each_cpu(cpu, &gpu->parent->isr.replayable_faults.stats.cpus_used_mask) {
-            UVM_SEQ_OR_DBG_PRINT(s, "    cpu%02u                              %llu\n",
-                                 cpu,
-                                 gpu->parent->isr.replayable_faults.stats.cpu_exec_count[cpu]);
-        }
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_buffer_entries       %u\n",
-                             gpu->parent->fault_buffer.replayable.max_faults);
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_cached_get           %u\n",
-                             gpu->parent->fault_buffer.replayable.cached_get);
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_cached_put           %u\n",
-                             gpu->parent->fault_buffer.replayable.cached_put);
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_get                  %u\n",
-                             gpu->parent->fault_buffer_hal->read_get(gpu->parent));
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_put                  %u\n",
-                             gpu->parent->fault_buffer_hal->read_put(gpu->parent));
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_fault_batch_size     %u\n",
-                             gpu->parent->fault_buffer.max_batch_size);
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_replay_policy        %s\n",
-                             uvm_perf_fault_replay_policy_string(gpu->parent->fault_buffer.replayable.replay_policy));
-        UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_num_faults           %llu\n",
-                             gpu->parent->stats.num_replayable_faults);
-    }
-    if (gpu->parent->isr.non_replayable_faults.handling) {
-        UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_bh               %llu\n",
-                             gpu->parent->isr.non_replayable_faults.stats.bottom_half_count);
-        UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_bh/cpu\n");
-        for_each_cpu(cpu, &gpu->parent->isr.non_replayable_faults.stats.cpus_used_mask) {
-            UVM_SEQ_OR_DBG_PRINT(s, "    cpu%02u                              %llu\n",
-                                 cpu,
-                                 gpu->parent->isr.non_replayable_faults.stats.cpu_exec_count[cpu]);
-        }
-        UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_buffer_entries   %u\n",
-                             gpu->parent->fault_buffer.non_replayable.max_faults);
-        UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_num_faults       %llu\n",
-                             gpu->parent->stats.num_non_replayable_faults);
-    }
-
-    for (i = 0; i < gpu_info->accessCntrBufferCount; i++) {
-        if (gpu->parent->access_counters_supported && gpu->parent->isr.access_counters[i].handling_ref_count > 0) {
-            UVM_SEQ_OR_DBG_PRINT(s, "access_counters_notif_buffer_index     %u\n", i);
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_bh                   %llu\n",
-                                 gpu->parent->isr.access_counters[i].stats.bottom_half_count);
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_bh/cpu\n");
-            for_each_cpu(cpu, &gpu->parent->isr.access_counters[i].stats.cpus_used_mask) {
-                UVM_SEQ_OR_DBG_PRINT(s, "    cpu%02u                              %llu\n",
-                                     cpu,
-                                     gpu->parent->isr.access_counters[i].stats.cpu_exec_count[cpu]);
-            }
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_buffer_entries       %u\n",
-                                 gpu->parent->access_counters.buffer[i].max_notifications);
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_cached_get           %u\n",
-                                 gpu->parent->access_counters.buffer[i].cached_get);
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_cached_put           %u\n",
-                                 gpu->parent->access_counters.buffer[i].cached_put);
-
-            get = UVM_GPU_READ_ONCE(*gpu->parent->access_counters.buffer[i].rm_info.pAccessCntrBufferGet);
-            put = UVM_GPU_READ_ONCE(*gpu->parent->access_counters.buffer[i].rm_info.pAccessCntrBufferPut);
-
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_get                  %u\n", get);
-            UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_put                  %u\n", put);
-        }
-    }
-
-    num_pages_out = atomic64_read(&gpu->parent->stats.num_pages_out);
-    num_pages_in = atomic64_read(&gpu->parent->stats.num_pages_in);
-    mapped_cpu_pages_size = atomic64_read(&gpu->parent->mapped_cpu_pages_size);
-
-    UVM_SEQ_OR_DBG_PRINT(s, "migrated_pages_in                      %llu (%llu MB)\n",
-                         num_pages_in,
-                         (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-    UVM_SEQ_OR_DBG_PRINT(s, "migrated_pages_out                     %llu (%llu MB)\n",
-                         num_pages_out,
-                         (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-    UVM_SEQ_OR_DBG_PRINT(s, "mapped_cpu_pages_dma                   %llu (%llu MB)\n",
-                         mapped_cpu_pages_size / PAGE_SIZE,
-                         mapped_cpu_pages_size / (1024u * 1024u));
-
-    gpu_info_print_ce_caps(gpu, s);
-
-    if (g_uvm_global.conf_computing_enabled) {
-        UVM_SEQ_OR_DBG_PRINT(s, "dma_buffer_pool_num_buffers             %lu\n",
-                             gpu->conf_computing.dma_buffer_pool.num_dma_buffers);
-    }
-}
-
-static void
-gpu_fault_stats_print_common(uvm_parent_gpu_t *parent_gpu, struct seq_file *s)
-{
-    NvU64 num_pages_in;
-    NvU64 num_pages_out;
-
-    UVM_ASSERT(uvm_procfs_is_debug_enabled());
-
-    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults      %llu\n", parent_gpu->stats.num_replayable_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "duplicates             %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_duplicate_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "faults_by_access_type:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  prefetch             %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_prefetch_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  read                 %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_read_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  write                %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_write_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  atomic               %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_atomic_faults);
-    num_pages_out = atomic64_read(&parent_gpu->fault_buffer.replayable.stats.num_pages_out);
-    num_pages_in = atomic64_read(&parent_gpu->fault_buffer.replayable.stats.num_pages_in);
-    UVM_SEQ_OR_DBG_PRINT(s, "migrations:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n", num_pages_in,
-                         (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-    UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_out        %llu (%llu MB)\n", num_pages_out,
-                         (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-    UVM_SEQ_OR_DBG_PRINT(s, "replays:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  start                %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_replays);
-    UVM_SEQ_OR_DBG_PRINT(s, "  start_ack_all        %llu\n",
-                         parent_gpu->fault_buffer.replayable.stats.num_replays_ack_all);
-    UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults  %llu\n", parent_gpu->stats.num_non_replayable_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "faults_by_access_type:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  read                 %llu\n",
-                         parent_gpu->fault_buffer.non_replayable.stats.num_read_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  write                %llu\n",
-                         parent_gpu->fault_buffer.non_replayable.stats.num_write_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  atomic               %llu\n",
-                         parent_gpu->fault_buffer.non_replayable.stats.num_atomic_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "faults_by_addressing:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  virtual              %llu\n",
-                         parent_gpu->stats.num_non_replayable_faults -
-                         parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults);
-    UVM_SEQ_OR_DBG_PRINT(s, "  physical             %llu\n",
-                         parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults);
-    num_pages_out = atomic64_read(&parent_gpu->fault_buffer.non_replayable.stats.num_pages_out);
-    num_pages_in = atomic64_read(&parent_gpu->fault_buffer.non_replayable.stats.num_pages_in);
-    UVM_SEQ_OR_DBG_PRINT(s, "migrations:\n");
-    UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n", num_pages_in,
-                         (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-    UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_out        %llu (%llu MB)\n", num_pages_out,
-                         (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-}
-
-static void gpu_access_counters_print_common(uvm_parent_gpu_t *parent_gpu, struct seq_file *s)
-{
-    NvU64 num_pages_in;
-    NvU64 num_pages_out;
-    NvU32 i;
-
-    UVM_ASSERT(uvm_procfs_is_debug_enabled());
-
-    // procfs_files are created before gpu_init_isr, we need to check if the
-    // access_counters.buffer is allocated.
-    if (parent_gpu->access_counters.buffer) {
-        for (i = 0; i < parent_gpu->rm_info.accessCntrBufferCount; i++) {
-            uvm_access_counter_buffer_t *access_counters = &parent_gpu->access_counters.buffer[i];
-
-            num_pages_out = atomic64_read(&access_counters->stats.num_pages_out);
-            num_pages_in = atomic64_read(&access_counters->stats.num_pages_in);
-            UVM_SEQ_OR_DBG_PRINT(s, "migrations - buffer index %u:\n", i);
-            UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n", num_pages_in,
-                                 (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-            UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_out        %llu (%llu MB)\n", num_pages_out,
-                                 (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
-        }
-    }
+  uvm_kvfree(ces_caps);
+}
+
+static const char *uvm_gpu_virt_type_string(UVM_VIRT_MODE virtMode) {
+  BUILD_BUG_ON(UVM_VIRT_MODE_COUNT != 4);
+
+  switch (virtMode) {
+    UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_NONE);
+    UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_LEGACY);
+    UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_SRIOV_HEAVY);
+    UVM_ENUM_STRING_CASE(UVM_VIRT_MODE_SRIOV_STANDARD);
+    UVM_ENUM_STRING_DEFAULT();
+  }
+}
+
+static const char *uvm_gpu_link_type_string(uvm_gpu_link_type_t link_type) {
+
+  BUILD_BUG_ON(UVM_GPU_LINK_MAX != 9);
+
+  switch (link_type) {
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_INVALID);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_PCIE);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_PCIE_BAR1);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_1);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_2);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_3);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_4);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_NVLINK_5);
+    UVM_ENUM_STRING_CASE(UVM_GPU_LINK_C2C);
+    UVM_ENUM_STRING_DEFAULT();
+  }
+}
+
+static void gpu_info_print_common(uvm_gpu_t *gpu, struct seq_file *s) {
+  const UvmGpuInfo *gpu_info = &gpu->parent->rm_info;
+  NvU64 num_pages_in;
+  NvU64 num_pages_out;
+  NvU64 mapped_cpu_pages_size;
+  NvU32 get;
+  NvU32 put;
+  NvU32 i;
+  unsigned int cpu;
+
+  UVM_SEQ_OR_DBG_PRINT(s, "GPU %s\n", uvm_gpu_name(gpu));
+  UVM_SEQ_OR_DBG_PRINT(s, "retained_count                         %llu\n",
+                       uvm_gpu_retained_count(gpu));
+  UVM_SEQ_OR_DBG_PRINT(s, "ecc                                    %s\n",
+                       gpu->ecc.enabled ? "enabled" : "disabled");
+  UVM_SEQ_OR_DBG_PRINT(s, "nvlink status and recovery             %s\n",
+                       gpu->nvlink_status.enabled ? "enabled" : "disabled");
+  if (gpu->parent->closest_cpu_numa_node == -1)
+    UVM_SEQ_OR_DBG_PRINT(s, "closest_cpu_numa_node                  n/a\n");
+  else
+    UVM_SEQ_OR_DBG_PRINT(s, "closest_cpu_numa_node                  %d\n",
+                         gpu->parent->closest_cpu_numa_node);
+
+  if (!uvm_procfs_is_debug_enabled())
+    return;
+
+  UVM_SEQ_OR_DBG_PRINT(s, "CPU link type                          %s\n",
+                       uvm_gpu_link_type_string(gpu->parent->system_bus.link));
+  UVM_SEQ_OR_DBG_PRINT(s, "CPU link bandwidth                     %uMBps\n",
+                       gpu->parent->system_bus.link_rate_mbyte_per_s);
+
+  UVM_SEQ_OR_DBG_PRINT(s, "architecture                           0x%X\n",
+                       gpu_info->gpuArch);
+  UVM_SEQ_OR_DBG_PRINT(s, "implementation                         0x%X\n",
+                       gpu_info->gpuImplementation);
+  UVM_SEQ_OR_DBG_PRINT(s, "gpcs                                   %u\n",
+                       gpu_info->gpcCount);
+  UVM_SEQ_OR_DBG_PRINT(s, "max_gpcs                               %u\n",
+                       gpu_info->maxGpcCount);
+  UVM_SEQ_OR_DBG_PRINT(s, "tpcs                                   %u\n",
+                       gpu_info->tpcCount);
+  UVM_SEQ_OR_DBG_PRINT(s, "max_tpcs_per_gpc                       %u\n",
+                       gpu_info->maxTpcPerGpcCount);
+  UVM_SEQ_OR_DBG_PRINT(s, "host_class                             0x%X\n",
+                       gpu_info->hostClass);
+  UVM_SEQ_OR_DBG_PRINT(s, "ce_class                               0x%X\n",
+                       gpu_info->ceClass);
+  UVM_SEQ_OR_DBG_PRINT(s, "virtualization_mode                    %s\n",
+                       uvm_gpu_virt_type_string(gpu_info->virtMode));
+  UVM_SEQ_OR_DBG_PRINT(s, "big_page_size                          %u\n",
+                       gpu->big_page.internal_size);
+  UVM_SEQ_OR_DBG_PRINT(s, "rm_va_base                             0x%llx\n",
+                       gpu->parent->rm_va_base);
+  UVM_SEQ_OR_DBG_PRINT(s, "rm_va_size                             0x%llx\n",
+                       gpu->parent->rm_va_size);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "vidmem_start                           %llu (%llu MBs)\n",
+      gpu->mem_info.phys_start, gpu->mem_info.phys_start / (1024 * 1024));
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "vidmem_size                            %llu (%llu MBs)\n",
+      gpu->mem_info.size, gpu->mem_info.size / (1024 * 1024));
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "vidmem_max_allocatable                 0x%llx (%llu MBs)\n",
+      gpu->mem_info.max_allocatable_address,
+      gpu->mem_info.max_allocatable_address / (1024 * 1024));
+
+  if (gpu->mem_info.numa.enabled) {
+    NvU64 window_size = gpu->parent->system_bus.memory_window_end -
+                        gpu->parent->system_bus.memory_window_start + 1;
+    UVM_SEQ_OR_DBG_PRINT(s, "numa_node_id                           %u\n",
+                         uvm_gpu_numa_node(gpu));
+    UVM_SEQ_OR_DBG_PRINT(s, "memory_window_start                    0x%llx\n",
+                         gpu->parent->system_bus.memory_window_start);
+    UVM_SEQ_OR_DBG_PRINT(s, "memory_window_end                      0x%llx\n",
+                         gpu->parent->system_bus.memory_window_end);
+    UVM_SEQ_OR_DBG_PRINT(
+        s, "system_memory_window_size              0x%llx (%llu MBs)\n",
+        window_size, window_size / (1024 * 1024));
+  }
+
+  UVM_SEQ_OR_DBG_PRINT(s, "interrupts                             %llu\n",
+                       gpu->parent->isr.interrupt_count);
+
+  if (gpu->parent->isr.replayable_faults.handling) {
+    UVM_SEQ_OR_DBG_PRINT(
+        s, "replayable_faults_bh                   %llu\n",
+        gpu->parent->isr.replayable_faults.stats.bottom_half_count);
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_bh/cpu\n");
+    for_each_cpu(cpu,
+                 &gpu->parent->isr.replayable_faults.stats.cpus_used_mask) {
+      UVM_SEQ_OR_DBG_PRINT(
+          s, "    cpu%02u                              %llu\n", cpu,
+          gpu->parent->isr.replayable_faults.stats.cpu_exec_count[cpu]);
+    }
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_buffer_entries       %u\n",
+                         gpu->parent->fault_buffer.replayable.max_faults);
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_cached_get           %u\n",
+                         gpu->parent->fault_buffer.replayable.cached_get);
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_cached_put           %u\n",
+                         gpu->parent->fault_buffer.replayable.cached_put);
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_get                  %u\n",
+                         gpu->parent->fault_buffer_hal->read_get(gpu->parent));
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_put                  %u\n",
+                         gpu->parent->fault_buffer_hal->read_put(gpu->parent));
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_fault_batch_size     %u\n",
+                         gpu->parent->fault_buffer.max_batch_size);
+    UVM_SEQ_OR_DBG_PRINT(
+        s, "replayable_faults_replay_policy        %s\n",
+        uvm_perf_fault_replay_policy_string(
+            gpu->parent->fault_buffer.replayable.replay_policy));
+    UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults_num_faults           %llu\n",
+                         gpu->parent->stats.num_replayable_faults);
+  }
+  if (gpu->parent->isr.non_replayable_faults.handling) {
+    UVM_SEQ_OR_DBG_PRINT(
+        s, "non_replayable_faults_bh               %llu\n",
+        gpu->parent->isr.non_replayable_faults.stats.bottom_half_count);
+    UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_bh/cpu\n");
+    for_each_cpu(cpu,
+                 &gpu->parent->isr.non_replayable_faults.stats.cpus_used_mask) {
+      UVM_SEQ_OR_DBG_PRINT(
+          s, "    cpu%02u                              %llu\n", cpu,
+          gpu->parent->isr.non_replayable_faults.stats.cpu_exec_count[cpu]);
+    }
+    UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_buffer_entries   %u\n",
+                         gpu->parent->fault_buffer.non_replayable.max_faults);
+    UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults_num_faults       %llu\n",
+                         gpu->parent->stats.num_non_replayable_faults);
+  }
+
+  for (i = 0; i < gpu_info->accessCntrBufferCount; i++) {
+    if (gpu->parent->access_counters_supported &&
+        gpu->parent->isr.access_counters[i].handling_ref_count > 0) {
+      UVM_SEQ_OR_DBG_PRINT(s, "access_counters_notif_buffer_index     %u\n", i);
+      UVM_SEQ_OR_DBG_PRINT(
+          s, "  access_counters_bh                   %llu\n",
+          gpu->parent->isr.access_counters[i].stats.bottom_half_count);
+      UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_bh/cpu\n");
+      for_each_cpu(cpu,
+                   &gpu->parent->isr.access_counters[i].stats.cpus_used_mask) {
+        UVM_SEQ_OR_DBG_PRINT(
+            s, "    cpu%02u                              %llu\n", cpu,
+            gpu->parent->isr.access_counters[i].stats.cpu_exec_count[cpu]);
+      }
+      UVM_SEQ_OR_DBG_PRINT(
+          s, "  access_counters_buffer_entries       %u\n",
+          gpu->parent->access_counters.buffer[i].max_notifications);
+      UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_cached_get           %u\n",
+                           gpu->parent->access_counters.buffer[i].cached_get);
+      UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_cached_put           %u\n",
+                           gpu->parent->access_counters.buffer[i].cached_put);
+
+      get = UVM_GPU_READ_ONCE(
+          *gpu->parent->access_counters.buffer[i].rm_info.pAccessCntrBufferGet);
+      put = UVM_GPU_READ_ONCE(
+          *gpu->parent->access_counters.buffer[i].rm_info.pAccessCntrBufferPut);
+
+      UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_get                  %u\n",
+                           get);
+      UVM_SEQ_OR_DBG_PRINT(s, "  access_counters_put                  %u\n",
+                           put);
+    }
+  }
+
+  num_pages_out = atomic64_read(&gpu->parent->stats.num_pages_out);
+  num_pages_in = atomic64_read(&gpu->parent->stats.num_pages_in);
+  mapped_cpu_pages_size = atomic64_read(&gpu->parent->mapped_cpu_pages_size);
+
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "migrated_pages_in                      %llu (%llu MB)\n",
+      num_pages_in, (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "migrated_pages_out                     %llu (%llu MB)\n",
+      num_pages_out, (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "mapped_cpu_pages_dma                   %llu (%llu MB)\n",
+      mapped_cpu_pages_size / PAGE_SIZE,
+      mapped_cpu_pages_size / (1024u * 1024u));
+
+  gpu_info_print_ce_caps(gpu, s);
+
+  if (g_uvm_global.conf_computing_enabled) {
+    UVM_SEQ_OR_DBG_PRINT(s, "dma_buffer_pool_num_buffers             %lu\n",
+                         gpu->conf_computing.dma_buffer_pool.num_dma_buffers);
+  }
+}
+
+static void gpu_fault_stats_print_common(uvm_parent_gpu_t *parent_gpu,
+                                         struct seq_file *s) {
+  NvU64 num_pages_in;
+  NvU64 num_pages_out;
+
+  UVM_ASSERT(uvm_procfs_is_debug_enabled());
+
+  UVM_SEQ_OR_DBG_PRINT(s, "replayable_faults      %llu\n",
+                       parent_gpu->stats.num_replayable_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "duplicates             %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_duplicate_faults);
+  UVM_SEQ_OR_DBG_PRINT(s, "faults_by_access_type:\n");
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  prefetch             %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_prefetch_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  read                 %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_read_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  write                %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_write_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  atomic               %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_atomic_faults);
+  num_pages_out =
+      atomic64_read(&parent_gpu->fault_buffer.replayable.stats.num_pages_out);
+  num_pages_in =
+      atomic64_read(&parent_gpu->fault_buffer.replayable.stats.num_pages_in);
+  UVM_SEQ_OR_DBG_PRINT(s, "migrations:\n");
+  UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n",
+                       num_pages_in,
+                       (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+  UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_out        %llu (%llu MB)\n",
+                       num_pages_out,
+                       (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+  UVM_SEQ_OR_DBG_PRINT(s, "replays:\n");
+  UVM_SEQ_OR_DBG_PRINT(s, "  start                %llu\n",
+                       parent_gpu->fault_buffer.replayable.stats.num_replays);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  start_ack_all        %llu\n",
+      parent_gpu->fault_buffer.replayable.stats.num_replays_ack_all);
+  UVM_SEQ_OR_DBG_PRINT(s, "non_replayable_faults  %llu\n",
+                       parent_gpu->stats.num_non_replayable_faults);
+  UVM_SEQ_OR_DBG_PRINT(s, "faults_by_access_type:\n");
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  read                 %llu\n",
+      parent_gpu->fault_buffer.non_replayable.stats.num_read_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  write                %llu\n",
+      parent_gpu->fault_buffer.non_replayable.stats.num_write_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  atomic               %llu\n",
+      parent_gpu->fault_buffer.non_replayable.stats.num_atomic_faults);
+  UVM_SEQ_OR_DBG_PRINT(s, "faults_by_addressing:\n");
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  virtual              %llu\n",
+      parent_gpu->stats.num_non_replayable_faults -
+          parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults);
+  UVM_SEQ_OR_DBG_PRINT(
+      s, "  physical             %llu\n",
+      parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults);
+  num_pages_out = atomic64_read(
+      &parent_gpu->fault_buffer.non_replayable.stats.num_pages_out);
+  num_pages_in = atomic64_read(
+      &parent_gpu->fault_buffer.non_replayable.stats.num_pages_in);
+  UVM_SEQ_OR_DBG_PRINT(s, "migrations:\n");
+  UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n",
+                       num_pages_in,
+                       (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+  UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_out        %llu (%llu MB)\n",
+                       num_pages_out,
+                       (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+}
+
+static void gpu_access_counters_print_common(uvm_parent_gpu_t *parent_gpu,
+                                             struct seq_file *s) {
+  NvU64 num_pages_in;
+  NvU64 num_pages_out;
+  NvU32 i;
+
+  UVM_ASSERT(uvm_procfs_is_debug_enabled());
+
+  // procfs_files are created before gpu_init_isr, we need to check if the
+  // access_counters.buffer is allocated.
+  if (parent_gpu->access_counters.buffer) {
+    for (i = 0; i < parent_gpu->rm_info.accessCntrBufferCount; i++) {
+      uvm_access_counter_buffer_t *access_counters =
+          &parent_gpu->access_counters.buffer[i];
+
+      num_pages_out = atomic64_read(&access_counters->stats.num_pages_out);
+      num_pages_in = atomic64_read(&access_counters->stats.num_pages_in);
+      UVM_SEQ_OR_DBG_PRINT(s, "migrations - buffer index %u:\n", i);
+      UVM_SEQ_OR_DBG_PRINT(s, "  num_pages_in         %llu (%llu MB)\n",
+                           num_pages_in,
+                           (num_pages_in * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+      UVM_SEQ_OR_DBG_PRINT(
+          s, "  num_pages_out        %llu (%llu MB)\n", num_pages_out,
+          (num_pages_out * (NvU64)PAGE_SIZE) / (1024u * 1024u));
+    }
+  }
 }
 
 // This function converts an index of 2D array of size [N x N] into an index
 // of upper triangular array of size [((N - 1) * ((N - 1) + 1)) / 2] which
 // does not include diagonal elements.
-static NvU32 peer_table_index(NvU32 index0, NvU32 index1, NvU32 N)
-{
-    NvU32 square_index, triangular_index, min_index;
+static NvU32 peer_table_index(NvU32 index0, NvU32 index1, NvU32 N) {
+  NvU32 square_index, triangular_index, min_index;
 
-    UVM_ASSERT(index0 != index1);
-    UVM_ASSERT(index0 < N);
-    UVM_ASSERT(index1 < N);
+  UVM_ASSERT(index0 != index1);
+  UVM_ASSERT(index0 < N);
+  UVM_ASSERT(index1 < N);
 
-    // Calculate an index of 2D array by re-ordering indices to always point
-    // to the same entry.
-    min_index = min(index0, index1);
-    square_index = min_index * N + max(index0, index1);
+  // Calculate an index of 2D array by re-ordering indices to always point
+  // to the same entry.
+  min_index = min(index0, index1);
+  square_index = min_index * N + max(index0, index1);
 
-    // Calculate and subtract number of lower triangular matrix elements till
-    // the current row (which includes diagonal elements) to get the correct
-    // index in an upper triangular matrix.
-    triangular_index = square_index - SUM_FROM_0_TO_N(min_index + 1);
+  // Calculate and subtract number of lower triangular matrix elements till
+  // the current row (which includes diagonal elements) to get the correct
+  // index in an upper triangular matrix.
+  triangular_index = square_index - SUM_FROM_0_TO_N(min_index + 1);
 
-    return triangular_index;
+  return triangular_index;
 }
 
 // This function converts an index of 2D array of size [N x N] into an index
 // of upper triangular array of size [(N * (N + 1)) / 2] which does include
 // diagonal elements.
-static NvU32 sub_processor_peer_table_index(NvU32 index0, NvU32 index1)
-{
-    NvU32 square_index, triangular_index, min_index;
+static NvU32 sub_processor_peer_table_index(NvU32 index0, NvU32 index1) {
+  NvU32 square_index, triangular_index, min_index;
 
-    UVM_ASSERT(index0 < UVM_PARENT_ID_MAX_SUB_PROCESSORS);
-    UVM_ASSERT(index1 < UVM_PARENT_ID_MAX_SUB_PROCESSORS);
+  UVM_ASSERT(index0 < UVM_PARENT_ID_MAX_SUB_PROCESSORS);
+  UVM_ASSERT(index1 < UVM_PARENT_ID_MAX_SUB_PROCESSORS);
 
-    // Calculate an index of 2D array by re-ordering indices to always point
-    // to the same entry.
-    min_index = min(index0, index1);
-    square_index = min_index * UVM_PARENT_ID_MAX_SUB_PROCESSORS + max(index0, index1);
+  // Calculate an index of 2D array by re-ordering indices to always point
+  // to the same entry.
+  min_index = min(index0, index1);
+  square_index =
+      min_index * UVM_PARENT_ID_MAX_SUB_PROCESSORS + max(index0, index1);
 
-    // Calculate and subtract number of lower triangular matrix elements till
-    // the current row (which doesn't include diagonal elements) to get the
-    // correct index in an upper triangular matrix.
-    triangular_index = square_index - SUM_FROM_0_TO_N(min_index);
+  // Calculate and subtract number of lower triangular matrix elements till
+  // the current row (which doesn't include diagonal elements) to get the
+  // correct index in an upper triangular matrix.
+  triangular_index = square_index - SUM_FROM_0_TO_N(min_index);
 
-    return triangular_index;
+  return triangular_index;
 }
 
-NvU32 uvm_gpu_pair_index(const uvm_gpu_id_t id0, const uvm_gpu_id_t id1)
-{
-    NvU32 index = peer_table_index(uvm_id_gpu_index(id0), uvm_id_gpu_index(id1), UVM_ID_MAX_GPUS);
+NvU32 uvm_gpu_pair_index(const uvm_gpu_id_t id0, const uvm_gpu_id_t id1) {
+  NvU32 index = peer_table_index(uvm_id_gpu_index(id0), uvm_id_gpu_index(id1),
+                                 UVM_ID_MAX_GPUS);
 
-    UVM_ASSERT(index < UVM_MAX_UNIQUE_GPU_PAIRS);
+  UVM_ASSERT(index < UVM_MAX_UNIQUE_GPU_PAIRS);
 
-    return index;
+  return index;
 }
 
-static NvU32 parent_gpu_peer_table_index(const uvm_parent_gpu_id_t id0, const uvm_parent_gpu_id_t id1)
-{
-    NvU32 index = peer_table_index(uvm_parent_id_gpu_index(id0), uvm_parent_id_gpu_index(id1), UVM_PARENT_ID_MAX_GPUS);
+static NvU32 parent_gpu_peer_table_index(const uvm_parent_gpu_id_t id0,
+                                         const uvm_parent_gpu_id_t id1) {
+  NvU32 index =
+      peer_table_index(uvm_parent_id_gpu_index(id0),
+                       uvm_parent_id_gpu_index(id1), UVM_PARENT_ID_MAX_GPUS);
 
-    UVM_ASSERT(index < UVM_MAX_UNIQUE_PARENT_GPU_PAIRS);
+  UVM_ASSERT(index < UVM_MAX_UNIQUE_PARENT_GPU_PAIRS);
 
-    return index;
+  return index;
 }
 
-static NvU32 sub_processor_pair_index(const uvm_gpu_id_t id0, const uvm_gpu_id_t id1)
-{
-    NvU32 index = sub_processor_peer_table_index(uvm_id_sub_processor_index(id0), uvm_id_sub_processor_index(id1));
+static NvU32 sub_processor_pair_index(const uvm_gpu_id_t id0,
+                                      const uvm_gpu_id_t id1) {
+  NvU32 index = sub_processor_peer_table_index(uvm_id_sub_processor_index(id0),
+                                               uvm_id_sub_processor_index(id1));
 
-    UVM_ASSERT(index < UVM_MAX_UNIQUE_SUB_PROCESSOR_PAIRS);
+  UVM_ASSERT(index < UVM_MAX_UNIQUE_SUB_PROCESSOR_PAIRS);
 
-    return index;
+  return index;
 }
 
 // Get the parent P2P capabilities between the given parent gpus.
-static uvm_parent_gpu_peer_t *parent_gpu_peer_caps(const uvm_parent_gpu_t *parent_gpu0,
-                                                   const uvm_parent_gpu_t *parent_gpu1)
-{
-    return &g_uvm_global.parent_gpu_peers[parent_gpu_peer_table_index(parent_gpu0->id, parent_gpu1->id)];
+static uvm_parent_gpu_peer_t *
+parent_gpu_peer_caps(const uvm_parent_gpu_t *parent_gpu0,
+                     const uvm_parent_gpu_t *parent_gpu1) {
+  return &g_uvm_global.parent_gpu_peers[parent_gpu_peer_table_index(
+      parent_gpu0->id, parent_gpu1->id)];
 }
 
 // Get the P2P capabilities between the given gpus.
-static uvm_gpu_peer_t *gpu_peer_caps(const uvm_gpu_t *gpu0, const uvm_gpu_t *gpu1)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps = parent_gpu_peer_caps(gpu0->parent, gpu1->parent);
+static uvm_gpu_peer_t *gpu_peer_caps(const uvm_gpu_t *gpu0,
+                                     const uvm_gpu_t *gpu1) {
+  uvm_parent_gpu_peer_t *parent_peer_caps =
+      parent_gpu_peer_caps(gpu0->parent, gpu1->parent);
 
-    return &parent_peer_caps->gpu_peers[sub_processor_pair_index(gpu0->id, gpu1->id)];
+  return &parent_peer_caps
+              ->gpu_peers[sub_processor_pair_index(gpu0->id, gpu1->id)];
 }
 
-static uvm_aperture_t parent_gpu_peer_aperture(uvm_parent_gpu_t *local,
-                                               uvm_parent_gpu_t *remote,
-                                               uvm_parent_gpu_peer_t *parent_peer_caps)
-{
-    size_t peer_index;
+static uvm_aperture_t
+parent_gpu_peer_aperture(uvm_parent_gpu_t *local, uvm_parent_gpu_t *remote,
+                         uvm_parent_gpu_peer_t *parent_peer_caps) {
+  size_t peer_index;
 
-    UVM_ASSERT(parent_peer_caps->ref_count);
-    UVM_ASSERT(parent_peer_caps->link_type != UVM_GPU_LINK_INVALID);
+  UVM_ASSERT(parent_peer_caps->ref_count);
+  UVM_ASSERT(parent_peer_caps->link_type != UVM_GPU_LINK_INVALID);
 
-    if (uvm_parent_id_value(local->id) < uvm_parent_id_value(remote->id))
-        peer_index = 0;
-    else
-        peer_index = 1;
-
-    if (parent_peer_caps->link_type == UVM_GPU_LINK_PCIE_BAR1) {
-        // UVM_APERTURE_SYS can be used if either the local (accessing) GPU
-        // _DOES NOT_ use PCIE atomics, or the remote (owning) GPU _DOES_
-        // accept PCIE atomics. Moreover, the bus topology needs to support
-        // routing of PCIe atomics between the devices.
-        //
-        // If either of the above conditions is not met we need to use
-        // UVM_APERTURE_SYS_NON_COHERENT to prevent use of PCIe atomics.
-        // RM provides the consolidated information in P2P properties.
-        const bool enable_atomics = parent_peer_caps->bar1_p2p_pcie_atomics_enabled[peer_index];
-        return enable_atomics ? UVM_APERTURE_SYS : UVM_APERTURE_SYS_NON_COHERENT;
-    }
+  if (uvm_parent_id_value(local->id) < uvm_parent_id_value(remote->id))
+    peer_index = 0;
+  else
+    peer_index = 1;
+
+  if (parent_peer_caps->link_type == UVM_GPU_LINK_PCIE_BAR1) {
+    // UVM_APERTURE_SYS can be used if either the local (accessing) GPU
+    // _DOES NOT_ use PCIE atomics, or the remote (owning) GPU _DOES_
+    // accept PCIE atomics. Moreover, the bus topology needs to support
+    // routing of PCIe atomics between the devices.
+    //
+    // If either of the above conditions is not met we need to use
+    // UVM_APERTURE_SYS_NON_COHERENT to prevent use of PCIe atomics.
+    // RM provides the consolidated information in P2P properties.
+    const bool enable_atomics =
+        parent_peer_caps->bar1_p2p_pcie_atomics_enabled[peer_index];
+    return enable_atomics ? UVM_APERTURE_SYS : UVM_APERTURE_SYS_NON_COHERENT;
+  }
 
-    return UVM_APERTURE_PEER(parent_peer_caps->peer_ids[peer_index]);
+  return UVM_APERTURE_PEER(parent_peer_caps->peer_ids[peer_index]);
 }
 
-static void parent_gpu_peer_caps_print(uvm_parent_gpu_t **parent_gpu_pair, struct seq_file *s)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps;
-    bool nvswitch_connected;
-    uvm_aperture_t aperture;
-    uvm_parent_gpu_t *local;
-    uvm_parent_gpu_t *remote;
-    const char *link_type;
-    NvU32 bandwidth;
+static void parent_gpu_peer_caps_print(uvm_parent_gpu_t **parent_gpu_pair,
+                                       struct seq_file *s) {
+  uvm_parent_gpu_peer_t *parent_peer_caps;
+  bool nvswitch_connected;
+  uvm_aperture_t aperture;
+  uvm_parent_gpu_t *local;
+  uvm_parent_gpu_t *remote;
+  const char *link_type;
+  NvU32 bandwidth;
 
-    UVM_ASSERT(uvm_procfs_is_debug_enabled());
+  UVM_ASSERT(uvm_procfs_is_debug_enabled());
 
-    local = parent_gpu_pair[0];
-    remote = parent_gpu_pair[1];
-    parent_peer_caps = parent_gpu_peer_caps(local, remote);
-    link_type = uvm_gpu_link_type_string(parent_peer_caps->link_type);
-    bandwidth = parent_peer_caps->total_link_line_rate_mbyte_per_s;
-    aperture = parent_gpu_peer_aperture(local, remote, parent_peer_caps);
-    nvswitch_connected = uvm_parent_gpus_are_nvswitch_connected(local, remote);
-    UVM_SEQ_OR_DBG_PRINT(s, "Link type                      %s\n", link_type);
-    UVM_SEQ_OR_DBG_PRINT(s, "Bandwidth                      %uMBps\n", bandwidth);
-    UVM_SEQ_OR_DBG_PRINT(s, "Aperture                       %s\n", uvm_aperture_string(aperture));
-    UVM_SEQ_OR_DBG_PRINT(s, "Connected through NVSWITCH     %s\n", nvswitch_connected ? "True" : "False");
-    UVM_SEQ_OR_DBG_PRINT(s, "Refcount                       %llu\n", READ_ONCE(parent_peer_caps->ref_count));
+  local = parent_gpu_pair[0];
+  remote = parent_gpu_pair[1];
+  parent_peer_caps = parent_gpu_peer_caps(local, remote);
+  link_type = uvm_gpu_link_type_string(parent_peer_caps->link_type);
+  bandwidth = parent_peer_caps->total_link_line_rate_mbyte_per_s;
+  aperture = parent_gpu_peer_aperture(local, remote, parent_peer_caps);
+  nvswitch_connected = uvm_parent_gpus_are_nvswitch_connected(local, remote);
+  UVM_SEQ_OR_DBG_PRINT(s, "Link type                      %s\n", link_type);
+  UVM_SEQ_OR_DBG_PRINT(s, "Bandwidth                      %uMBps\n", bandwidth);
+  UVM_SEQ_OR_DBG_PRINT(s, "Aperture                       %s\n",
+                       uvm_aperture_string(aperture));
+  UVM_SEQ_OR_DBG_PRINT(s, "Connected through NVSWITCH     %s\n",
+                       nvswitch_connected ? "True" : "False");
+  UVM_SEQ_OR_DBG_PRINT(s, "Refcount                       %llu\n",
+                       READ_ONCE(parent_peer_caps->ref_count));
 }
 
-static int nv_procfs_read_gpu_info(struct seq_file *s, void *v)
-{
-    uvm_gpu_t *gpu = (uvm_gpu_t *)s->private;
+static int nv_procfs_read_gpu_info(struct seq_file *s, void *v) {
+  uvm_gpu_t *gpu = (uvm_gpu_t *)s->private;
 
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
-        return -EAGAIN;
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
+    return -EAGAIN;
 
-    gpu_info_print_common(gpu, s);
+  gpu_info_print_common(gpu, s);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    return 0;
+  return 0;
 }
 
-static int nv_procfs_read_gpu_info_entry(struct seq_file *s, void *v)
-{
-    UVM_ENTRY_RET(nv_procfs_read_gpu_info(s, v));
+static int nv_procfs_read_gpu_info_entry(struct seq_file *s, void *v) {
+  UVM_ENTRY_RET(nv_procfs_read_gpu_info(s, v));
 }
 
-static int nv_procfs_read_gpu_fault_stats(struct seq_file *s, void *v)
-{
-    uvm_parent_gpu_t *parent_gpu = (uvm_parent_gpu_t *)s->private;
+static int nv_procfs_read_gpu_fault_stats(struct seq_file *s, void *v) {
+  uvm_parent_gpu_t *parent_gpu = (uvm_parent_gpu_t *)s->private;
 
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
-        return -EAGAIN;
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
+    return -EAGAIN;
 
-    gpu_fault_stats_print_common(parent_gpu, s);
+  gpu_fault_stats_print_common(parent_gpu, s);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    return 0;
+  return 0;
 }
 
-static int nv_procfs_read_gpu_fault_stats_entry(struct seq_file *s, void *v)
-{
-    UVM_ENTRY_RET(nv_procfs_read_gpu_fault_stats(s, v));
+static int nv_procfs_read_gpu_fault_stats_entry(struct seq_file *s, void *v) {
+  UVM_ENTRY_RET(nv_procfs_read_gpu_fault_stats(s, v));
 }
 
-static int nv_procfs_read_gpu_access_counters(struct seq_file *s, void *v)
-{
-    uvm_parent_gpu_t *parent_gpu = (uvm_parent_gpu_t *)s->private;
+static int nv_procfs_read_gpu_access_counters(struct seq_file *s, void *v) {
+  uvm_parent_gpu_t *parent_gpu = (uvm_parent_gpu_t *)s->private;
 
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
-        return -EAGAIN;
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
+    return -EAGAIN;
 
-    gpu_access_counters_print_common(parent_gpu, s);
+  gpu_access_counters_print_common(parent_gpu, s);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    return 0;
+  return 0;
 }
 
-static int nv_procfs_read_gpu_access_counters_entry(struct seq_file *s, void *v)
-{
-    UVM_ENTRY_RET(nv_procfs_read_gpu_access_counters(s, v));
+static int nv_procfs_read_gpu_access_counters_entry(struct seq_file *s,
+                                                    void *v) {
+  UVM_ENTRY_RET(nv_procfs_read_gpu_access_counters(s, v));
 }
 
 UVM_DEFINE_SINGLE_PROCFS_FILE(gpu_info_entry);
 UVM_DEFINE_SINGLE_PROCFS_FILE(gpu_fault_stats_entry);
 UVM_DEFINE_SINGLE_PROCFS_FILE(gpu_access_counters_entry);
 
-static void uvm_parent_gpu_uuid_string(char *buffer, const NvProcessorUuid *uuid)
-{
-    memcpy(buffer, UVM_PARENT_GPU_UUID_PREFIX, sizeof(UVM_PARENT_GPU_UUID_PREFIX) - 1);
-    uvm_uuid_string(buffer + sizeof(UVM_PARENT_GPU_UUID_PREFIX) - 1, uuid);
+static void uvm_parent_gpu_uuid_string(char *buffer,
+                                       const NvProcessorUuid *uuid) {
+  memcpy(buffer, UVM_PARENT_GPU_UUID_PREFIX,
+         sizeof(UVM_PARENT_GPU_UUID_PREFIX) - 1);
+  uvm_uuid_string(buffer + sizeof(UVM_PARENT_GPU_UUID_PREFIX) - 1, uuid);
 }
 
-static void uvm_gpu_uuid_string(char *buffer, const NvProcessorUuid *uuid)
-{
-    memcpy(buffer, UVM_GPU_UUID_PREFIX, sizeof(UVM_GPU_UUID_PREFIX) - 1);
-    uvm_uuid_string(buffer + sizeof(UVM_GPU_UUID_PREFIX) - 1, uuid);
+static void uvm_gpu_uuid_string(char *buffer, const NvProcessorUuid *uuid) {
+  memcpy(buffer, UVM_GPU_UUID_PREFIX, sizeof(UVM_GPU_UUID_PREFIX) - 1);
+  uvm_uuid_string(buffer + sizeof(UVM_GPU_UUID_PREFIX) - 1, uuid);
 }
 
-static NV_STATUS init_parent_procfs_dir(uvm_parent_gpu_t *parent_gpu)
-{
-    struct proc_dir_entry *gpu_base_dir_entry;
-    char gpu_dir_name[UVM_PARENT_GPU_UUID_STRING_LENGTH];
+static NV_STATUS init_parent_procfs_dir(uvm_parent_gpu_t *parent_gpu) {
+  struct proc_dir_entry *gpu_base_dir_entry;
+  char gpu_dir_name[UVM_PARENT_GPU_UUID_STRING_LENGTH];
 
-    if (!uvm_procfs_is_enabled())
-        return NV_OK;
+  if (!uvm_procfs_is_enabled())
+    return NV_OK;
 
-    gpu_base_dir_entry = uvm_procfs_get_gpu_base_dir();
+  gpu_base_dir_entry = uvm_procfs_get_gpu_base_dir();
 
-    // Create GPU-${physical-UUID} directory.
-    uvm_parent_gpu_uuid_string(gpu_dir_name, &parent_gpu->uuid);
+  // Create GPU-${physical-UUID} directory.
+  uvm_parent_gpu_uuid_string(gpu_dir_name, &parent_gpu->uuid);
 
-    parent_gpu->procfs.dir = NV_CREATE_PROC_DIR(gpu_dir_name, gpu_base_dir_entry);
-    if (parent_gpu->procfs.dir == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  parent_gpu->procfs.dir = NV_CREATE_PROC_DIR(gpu_dir_name, gpu_base_dir_entry);
+  if (parent_gpu->procfs.dir == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    // GPU peer files are debug only.
-    if (!uvm_procfs_is_debug_enabled())
-        return NV_OK;
+  // GPU peer files are debug only.
+  if (!uvm_procfs_is_debug_enabled())
+    return NV_OK;
 
-    parent_gpu->procfs.dir_peers = NV_CREATE_PROC_DIR(UVM_PROC_GPUS_PEER_DIR_NAME, parent_gpu->procfs.dir);
-    if (parent_gpu->procfs.dir_peers == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  parent_gpu->procfs.dir_peers =
+      NV_CREATE_PROC_DIR(UVM_PROC_GPUS_PEER_DIR_NAME, parent_gpu->procfs.dir);
+  if (parent_gpu->procfs.dir_peers == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static void deinit_parent_procfs_dir(uvm_parent_gpu_t *parent_gpu)
-{
-    proc_remove(parent_gpu->procfs.dir_peers);
-    proc_remove(parent_gpu->procfs.dir);
+static void deinit_parent_procfs_dir(uvm_parent_gpu_t *parent_gpu) {
+  proc_remove(parent_gpu->procfs.dir_peers);
+  proc_remove(parent_gpu->procfs.dir);
 }
 
-static NV_STATUS init_parent_procfs_files(uvm_parent_gpu_t *parent_gpu)
-{
-    // Fault and access counter files are debug only
-    if (!uvm_procfs_is_debug_enabled())
-        return NV_OK;
+static NV_STATUS init_parent_procfs_files(uvm_parent_gpu_t *parent_gpu) {
+  // Fault and access counter files are debug only
+  if (!uvm_procfs_is_debug_enabled())
+    return NV_OK;
 
-    parent_gpu->procfs.fault_stats_file = NV_CREATE_PROC_FILE("fault_stats",
-                                                              parent_gpu->procfs.dir,
-                                                              gpu_fault_stats_entry,
-                                                              parent_gpu);
-    if (parent_gpu->procfs.fault_stats_file == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  parent_gpu->procfs.fault_stats_file = NV_CREATE_PROC_FILE(
+      "fault_stats", parent_gpu->procfs.dir, gpu_fault_stats_entry, parent_gpu);
+  if (parent_gpu->procfs.fault_stats_file == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    parent_gpu->procfs.access_counters_file = NV_CREATE_PROC_FILE("access_counters",
-                                                                  parent_gpu->procfs.dir,
-                                                                  gpu_access_counters_entry,
-                                                                  parent_gpu);
-    if (parent_gpu->procfs.access_counters_file == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  parent_gpu->procfs.access_counters_file =
+      NV_CREATE_PROC_FILE("access_counters", parent_gpu->procfs.dir,
+                          gpu_access_counters_entry, parent_gpu);
+  if (parent_gpu->procfs.access_counters_file == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static void deinit_parent_procfs_files(uvm_parent_gpu_t *parent_gpu)
-{
-    proc_remove(parent_gpu->procfs.access_counters_file);
-    proc_remove(parent_gpu->procfs.fault_stats_file);
+static void deinit_parent_procfs_files(uvm_parent_gpu_t *parent_gpu) {
+  proc_remove(parent_gpu->procfs.access_counters_file);
+  proc_remove(parent_gpu->procfs.fault_stats_file);
 }
 
-static NV_STATUS init_procfs_dirs(uvm_gpu_t *gpu)
-{
-    struct proc_dir_entry *gpu_base_dir_entry;
-    char symlink_name[16]; // Hold a uvm_gpu_id_t value in decimal.
-    char uuid_buffer[NV_MAX(UVM_PARENT_GPU_UUID_STRING_LENGTH, UVM_GPU_UUID_STRING_LENGTH)];
-    char gpu_dir_name[sizeof(symlink_name) + sizeof(uuid_buffer) + 1];
+static NV_STATUS init_procfs_dirs(uvm_gpu_t *gpu) {
+  struct proc_dir_entry *gpu_base_dir_entry;
+  char symlink_name[16]; // Hold a uvm_gpu_id_t value in decimal.
+  char uuid_buffer[NV_MAX(UVM_PARENT_GPU_UUID_STRING_LENGTH,
+                          UVM_GPU_UUID_STRING_LENGTH)];
+  char gpu_dir_name[sizeof(symlink_name) + sizeof(uuid_buffer) + 1];
 
-    if (!uvm_procfs_is_enabled())
-        return NV_OK;
+  if (!uvm_procfs_is_enabled())
+    return NV_OK;
 
-    uvm_parent_gpu_uuid_string(uuid_buffer, &gpu->parent->uuid);
+  uvm_parent_gpu_uuid_string(uuid_buffer, &gpu->parent->uuid);
 
-    gpu_base_dir_entry = uvm_procfs_get_gpu_base_dir();
+  gpu_base_dir_entry = uvm_procfs_get_gpu_base_dir();
 
-    // Create GPU-${physical-UUID}/${sub_processor_index} directory
-    snprintf(gpu_dir_name, sizeof(gpu_dir_name), "%u", uvm_id_sub_processor_index(gpu->id));
+  // Create GPU-${physical-UUID}/${sub_processor_index} directory
+  snprintf(gpu_dir_name, sizeof(gpu_dir_name), "%u",
+           uvm_id_sub_processor_index(gpu->id));
 
-    gpu->procfs.dir = NV_CREATE_PROC_DIR(gpu_dir_name, gpu->parent->procfs.dir);
-    if (gpu->procfs.dir == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  gpu->procfs.dir = NV_CREATE_PROC_DIR(gpu_dir_name, gpu->parent->procfs.dir);
+  if (gpu->procfs.dir == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    // Create symlink from ${gpu_id} to
-    // GPU-${physical-UUID}/${sub_processor_index}
-    snprintf(symlink_name, sizeof(symlink_name), "%u", uvm_id_value(gpu->id));
-    snprintf(gpu_dir_name,
-             sizeof(gpu_dir_name),
-             "%s/%u",
-             uuid_buffer,
-             uvm_id_sub_processor_index(gpu->id));
+  // Create symlink from ${gpu_id} to
+  // GPU-${physical-UUID}/${sub_processor_index}
+  snprintf(symlink_name, sizeof(symlink_name), "%u", uvm_id_value(gpu->id));
+  snprintf(gpu_dir_name, sizeof(gpu_dir_name), "%s/%u", uuid_buffer,
+           uvm_id_sub_processor_index(gpu->id));
 
-    gpu->procfs.dir_symlink = proc_symlink(symlink_name, gpu_base_dir_entry, gpu_dir_name);
-    if (gpu->procfs.dir_symlink == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  gpu->procfs.dir_symlink =
+      proc_symlink(symlink_name, gpu_base_dir_entry, gpu_dir_name);
+  if (gpu->procfs.dir_symlink == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    // Create symlink from GI-${GI-UUID} to
-    // GPU-${physical-UUID}/${sub_processor_index}
-    uvm_gpu_uuid_string(uuid_buffer, &gpu->uuid);
+  // Create symlink from GI-${GI-UUID} to
+  // GPU-${physical-UUID}/${sub_processor_index}
+  uvm_gpu_uuid_string(uuid_buffer, &gpu->uuid);
 
-    gpu->procfs.gpu_instance_uuid_symlink = proc_symlink(uuid_buffer, gpu_base_dir_entry, gpu_dir_name);
-    if (gpu->procfs.gpu_instance_uuid_symlink == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+  gpu->procfs.gpu_instance_uuid_symlink =
+      proc_symlink(uuid_buffer, gpu_base_dir_entry, gpu_dir_name);
+  if (gpu->procfs.gpu_instance_uuid_symlink == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    return NV_OK;
+  return NV_OK;
 }
 
 // The kernel waits on readers to finish before returning from those calls
-static void deinit_procfs_dirs(uvm_gpu_t *gpu)
-{
-    proc_remove(gpu->procfs.gpu_instance_uuid_symlink);
-    proc_remove(gpu->procfs.dir_symlink);
-    proc_remove(gpu->procfs.dir);
+static void deinit_procfs_dirs(uvm_gpu_t *gpu) {
+  proc_remove(gpu->procfs.gpu_instance_uuid_symlink);
+  proc_remove(gpu->procfs.dir_symlink);
+  proc_remove(gpu->procfs.dir);
 }
 
-static NV_STATUS init_procfs_files(uvm_gpu_t *gpu)
-{
-    gpu->procfs.info_file = NV_CREATE_PROC_FILE("info", gpu->procfs.dir, gpu_info_entry, gpu);
-    if (gpu->procfs.info_file == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
+static NV_STATUS init_procfs_files(uvm_gpu_t *gpu) {
+  gpu->procfs.info_file =
+      NV_CREATE_PROC_FILE("info", gpu->procfs.dir, gpu_info_entry, gpu);
+  if (gpu->procfs.info_file == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static void deinit_procfs_files(uvm_gpu_t *gpu)
-{
-    proc_remove(gpu->procfs.info_file);
+static void deinit_procfs_files(uvm_gpu_t *gpu) {
+  proc_remove(gpu->procfs.info_file);
 }
 
-static void deinit_procfs_parent_peer_cap_files(uvm_parent_gpu_peer_t *parent_peer_caps)
-{
-    proc_remove(parent_peer_caps->procfs.peer_symlink_file[0]);
-    proc_remove(parent_peer_caps->procfs.peer_symlink_file[1]);
-    proc_remove(parent_peer_caps->procfs.peer_file[0]);
-    proc_remove(parent_peer_caps->procfs.peer_file[1]);
+static void
+deinit_procfs_parent_peer_cap_files(uvm_parent_gpu_peer_t *parent_peer_caps) {
+  proc_remove(parent_peer_caps->procfs.peer_symlink_file[0]);
+  proc_remove(parent_peer_caps->procfs.peer_symlink_file[1]);
+  proc_remove(parent_peer_caps->procfs.peer_file[0]);
+  proc_remove(parent_peer_caps->procfs.peer_file[1]);
 }
 
-static NV_STATUS init_semaphore_pools(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    uvm_gpu_t *other_gpu;
+static NV_STATUS init_semaphore_pools(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  uvm_gpu_t *other_gpu;
 
-    status = uvm_gpu_semaphore_pool_create(gpu, &gpu->semaphore_pool);
-    if (status != NV_OK)
-        return status;
+  status = uvm_gpu_semaphore_pool_create(gpu, &gpu->semaphore_pool);
+  if (status != NV_OK)
+    return status;
 
-    // When the Confidential Computing feature is enabled, a separate secure
-    // pool is created that holds page allocated in the CPR of vidmem.
-    if (g_uvm_global.conf_computing_enabled)
-        return uvm_gpu_semaphore_secure_pool_create(gpu, &gpu->secure_semaphore_pool);
+  // When the Confidential Computing feature is enabled, a separate secure
+  // pool is created that holds page allocated in the CPR of vidmem.
+  if (g_uvm_global.conf_computing_enabled)
+    return uvm_gpu_semaphore_secure_pool_create(gpu,
+                                                &gpu->secure_semaphore_pool);
 
-    for_each_gpu(other_gpu) {
-        if (other_gpu == gpu)
-            continue;
-        status = uvm_gpu_semaphore_pool_map_gpu(other_gpu->semaphore_pool, gpu);
-        if (status != NV_OK)
-            return status;
-    }
+  for_each_gpu(other_gpu) {
+    if (other_gpu == gpu)
+      continue;
+    status = uvm_gpu_semaphore_pool_map_gpu(other_gpu->semaphore_pool, gpu);
+    if (status != NV_OK)
+      return status;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static void deinit_semaphore_pools(uvm_gpu_t *gpu)
-{
-    uvm_gpu_t *other_gpu;
+static void deinit_semaphore_pools(uvm_gpu_t *gpu) {
+  uvm_gpu_t *other_gpu;
 
-    for_each_gpu(other_gpu) {
-        if (other_gpu == gpu)
-            continue;
-        uvm_gpu_semaphore_pool_unmap_gpu(other_gpu->semaphore_pool, gpu);
-    }
+  for_each_gpu(other_gpu) {
+    if (other_gpu == gpu)
+      continue;
+    uvm_gpu_semaphore_pool_unmap_gpu(other_gpu->semaphore_pool, gpu);
+  }
 
-    uvm_gpu_semaphore_pool_destroy(gpu->semaphore_pool);
-    uvm_gpu_semaphore_pool_destroy(gpu->secure_semaphore_pool);
+  uvm_gpu_semaphore_pool_destroy(gpu->semaphore_pool);
+  uvm_gpu_semaphore_pool_destroy(gpu->secure_semaphore_pool);
 }
 
-static void init_access_counters_serialize_clear_tracker(uvm_parent_gpu_t *parent)
-{
-    NvU32 i;
+static void
+init_access_counters_serialize_clear_tracker(uvm_parent_gpu_t *parent) {
+  NvU32 i;
 
-    for (i = 0; i < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; i++)
-        uvm_tracker_init(&parent->access_counters.serialize_clear_tracker[i]);
+  for (i = 0; i < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; i++)
+    uvm_tracker_init(&parent->access_counters.serialize_clear_tracker[i]);
 }
 
-static void deinit_access_counters_serialize_clear_tracker(uvm_parent_gpu_t *parent)
-{
-    NvU32 i;
+static void
+deinit_access_counters_serialize_clear_tracker(uvm_parent_gpu_t *parent) {
+  NvU32 i;
 
-    for (i = 0; i < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; i++)
-        uvm_tracker_deinit(&parent->access_counters.serialize_clear_tracker[i]);
+  for (i = 0; i < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; i++)
+    uvm_tracker_deinit(&parent->access_counters.serialize_clear_tracker[i]);
 }
 
-static NV_STATUS find_unused_gpu_id(uvm_parent_gpu_t *parent_gpu, uvm_gpu_id_t *out_id)
-{
-    NvU32 i;
+static NV_STATUS find_unused_gpu_id(uvm_parent_gpu_t *parent_gpu,
+                                    uvm_gpu_id_t *out_id) {
+  NvU32 i;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    if (!parent_gpu) {
-        for (i = 0; i < UVM_PARENT_ID_MAX_GPUS; i++) {
-            if (!g_uvm_global.parent_gpus[i]) {
-                *out_id = uvm_gpu_id_from_sub_processor_index(i, 0);
-                return NV_OK;
-            }
-        }
+  if (!parent_gpu) {
+    for (i = 0; i < UVM_PARENT_ID_MAX_GPUS; i++) {
+      if (!g_uvm_global.parent_gpus[i]) {
+        *out_id = uvm_gpu_id_from_sub_processor_index(i, 0);
+        return NV_OK;
+      }
     }
-    else {
-        NvU32 sub_processor_index = find_first_zero_bit(parent_gpu->valid_gpus, UVM_PARENT_ID_MAX_SUB_PROCESSORS);
-
-        if (sub_processor_index < UVM_PARENT_ID_MAX_SUB_PROCESSORS) {
-            i = uvm_parent_id_gpu_index(parent_gpu->id);
-            *out_id = uvm_gpu_id_from_sub_processor_index(i, sub_processor_index);
-            return NV_OK;
-        }
+  } else {
+    NvU32 sub_processor_index = find_first_zero_bit(
+        parent_gpu->valid_gpus, UVM_PARENT_ID_MAX_SUB_PROCESSORS);
+
+    if (sub_processor_index < UVM_PARENT_ID_MAX_SUB_PROCESSORS) {
+      i = uvm_parent_id_gpu_index(parent_gpu->id);
+      *out_id = uvm_gpu_id_from_sub_processor_index(i, sub_processor_index);
+      return NV_OK;
     }
+  }
 
-    return NV_ERR_INSUFFICIENT_RESOURCES;
+  return NV_ERR_INSUFFICIENT_RESOURCES;
 }
 
 // Allocates a uvm_parent_gpu_t, assigns the GPU ID, and sets up basic data
 // structures, but leaves all other initialization up to the caller.
 static NV_STATUS alloc_parent_gpu(const NvProcessorUuid *gpu_uuid,
                                   uvm_parent_gpu_id_t gpu_id,
-                                  uvm_parent_gpu_t **parent_gpu_out)
-{
-    uvm_parent_gpu_t *parent_gpu;
-    NV_STATUS status;
-
-    parent_gpu = uvm_kvmalloc_zero(sizeof(*parent_gpu));
-    if (!parent_gpu)
-        return NV_ERR_NO_MEMORY;
-
-    parent_gpu->id = gpu_id;
-
-    uvm_uuid_copy(&parent_gpu->uuid, gpu_uuid);
-    uvm_sema_init(&parent_gpu->isr.replayable_faults.service_lock, 1, UVM_LOCK_ORDER_ISR);
-    uvm_sema_init(&parent_gpu->isr.non_replayable_faults.service_lock, 1, UVM_LOCK_ORDER_ISR);
-    uvm_mutex_init(&parent_gpu->access_counters.enablement_lock, UVM_LOCK_ORDER_ACCESS_COUNTERS);
-    uvm_mutex_init(&parent_gpu->access_counters.clear_tracker_lock, UVM_LOCK_ACCESS_COUNTERS_CLEAR_OPS);
-    uvm_mutex_init(&parent_gpu->access_counters.serialize_clear_lock, UVM_LOCK_ACCESS_COUNTERS_CLEAR_OPS);
-    uvm_tracker_init(&parent_gpu->access_counters.clear_tracker);
-    init_access_counters_serialize_clear_tracker(parent_gpu);
-    uvm_spin_lock_irqsave_init(&parent_gpu->isr.interrupts_lock, UVM_LOCK_ORDER_LEAF);
-    uvm_spin_lock_init(&parent_gpu->instance_ptr_table_lock, UVM_LOCK_ORDER_LEAF);
-    uvm_rb_tree_init(&parent_gpu->instance_ptr_table);
-    uvm_rb_tree_init(&parent_gpu->tsg_table);
-
-    // TODO: Bug 3881835: revisit whether to use nv_kthread_q_t or workqueue.
-    status = errno_to_nv_status(nv_kthread_q_init(&parent_gpu->lazy_free_q, "vidmem lazy free"));
-    if (status != NV_OK)
-        goto cleanup;
-
-    nv_kref_init(&parent_gpu->gpu_kref);
-
-    *parent_gpu_out = parent_gpu;
-
-    return NV_OK;
+                                  uvm_parent_gpu_t **parent_gpu_out) {
+  uvm_parent_gpu_t *parent_gpu;
+  NV_STATUS status;
+
+  parent_gpu = uvm_kvmalloc_zero(sizeof(*parent_gpu));
+  if (!parent_gpu)
+    return NV_ERR_NO_MEMORY;
+
+  parent_gpu->id = gpu_id;
+
+  uvm_uuid_copy(&parent_gpu->uuid, gpu_uuid);
+  uvm_sema_init(&parent_gpu->isr.replayable_faults.service_lock, 1,
+                UVM_LOCK_ORDER_ISR);
+  uvm_sema_init(&parent_gpu->isr.non_replayable_faults.service_lock, 1,
+                UVM_LOCK_ORDER_ISR);
+  uvm_mutex_init(&parent_gpu->access_counters.enablement_lock,
+                 UVM_LOCK_ORDER_ACCESS_COUNTERS);
+  uvm_mutex_init(&parent_gpu->access_counters.clear_tracker_lock,
+                 UVM_LOCK_ACCESS_COUNTERS_CLEAR_OPS);
+  uvm_mutex_init(&parent_gpu->access_counters.serialize_clear_lock,
+                 UVM_LOCK_ACCESS_COUNTERS_CLEAR_OPS);
+  uvm_tracker_init(&parent_gpu->access_counters.clear_tracker);
+  init_access_counters_serialize_clear_tracker(parent_gpu);
+  uvm_spin_lock_irqsave_init(&parent_gpu->isr.interrupts_lock,
+                             UVM_LOCK_ORDER_LEAF);
+  uvm_spin_lock_init(&parent_gpu->instance_ptr_table_lock, UVM_LOCK_ORDER_LEAF);
+  uvm_rb_tree_init(&parent_gpu->instance_ptr_table);
+  uvm_rb_tree_init(&parent_gpu->tsg_table);
+
+  // TODO: Bug 3881835: revisit whether to use nv_kthread_q_t or workqueue.
+  status = errno_to_nv_status(
+      nv_kthread_q_init(&parent_gpu->lazy_free_q, "vidmem lazy free"));
+  if (status != NV_OK)
+    goto cleanup;
+
+  nv_kref_init(&parent_gpu->gpu_kref);
+
+  *parent_gpu_out = parent_gpu;
+
+  return NV_OK;
 
 cleanup:
-    uvm_tracker_deinit(&parent_gpu->access_counters.clear_tracker);
-    deinit_access_counters_serialize_clear_tracker(parent_gpu);
-    uvm_kvfree(parent_gpu);
+  uvm_tracker_deinit(&parent_gpu->access_counters.clear_tracker);
+  deinit_access_counters_serialize_clear_tracker(parent_gpu);
+  uvm_kvfree(parent_gpu);
 
-    return status;
+  return status;
 }
 
 // Allocates a uvm_gpu_t struct and initializes the basic fields and leaves all
 // other initialization up to the caller.
-static uvm_gpu_t *alloc_gpu(uvm_parent_gpu_t *parent_gpu, uvm_gpu_id_t gpu_id)
-{
-    NvU32 sub_processor_index;
-    uvm_gpu_t *gpu;
+static uvm_gpu_t *alloc_gpu(uvm_parent_gpu_t *parent_gpu, uvm_gpu_id_t gpu_id) {
+  NvU32 sub_processor_index;
+  uvm_gpu_t *gpu;
 
-    gpu = uvm_kvmalloc_zero(sizeof(*gpu));
-    if (!gpu)
-        return gpu;
+  gpu = uvm_kvmalloc_zero(sizeof(*gpu));
+  if (!gpu)
+    return gpu;
 
-    gpu->id = gpu_id;
-    gpu->parent = parent_gpu;
+  gpu->id = gpu_id;
+  gpu->parent = parent_gpu;
 
-    // Initialize enough of the gpu struct for remove_gpu to be called
-    gpu->magic = UVM_GPU_MAGIC_VALUE;
-    uvm_spin_lock_init(&gpu->peer_info.peer_gpu_lock, UVM_LOCK_ORDER_LEAF);
+  // Initialize enough of the gpu struct for remove_gpu to be called
+  gpu->magic = UVM_GPU_MAGIC_VALUE;
+  uvm_spin_lock_init(&gpu->peer_info.peer_gpu_lock, UVM_LOCK_ORDER_LEAF);
 
-    sub_processor_index = uvm_id_sub_processor_index(gpu_id);
-    parent_gpu->gpus[sub_processor_index] = gpu;
+  sub_processor_index = uvm_id_sub_processor_index(gpu_id);
+  parent_gpu->gpus[sub_processor_index] = gpu;
 
-    return gpu;
+  return gpu;
 }
 
-static NV_STATUS configure_address_space(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-    NvU32 num_entries;
-    NvU64 va_size;
-    NvU64 va_per_entry;
-    NvU64 physical_address;
-    NvU64 dma_address;
-    uvm_mmu_page_table_alloc_t *tree_alloc;
-
-    status = uvm_page_tree_init(gpu,
-                                NULL,
-                                UVM_PAGE_TREE_TYPE_KERNEL,
-                                gpu->big_page.internal_size,
-                                uvm_get_page_tree_location(gpu),
-                                &gpu->address_space_tree);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Initializing the page tree failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+static NV_STATUS configure_address_space(uvm_gpu_t *gpu) {
+  NV_STATUS status;
+  NvU32 num_entries;
+  NvU64 va_size;
+  NvU64 va_per_entry;
+  NvU64 physical_address;
+  NvU64 dma_address;
+  uvm_mmu_page_table_alloc_t *tree_alloc;
 
-    num_entries = uvm_mmu_page_tree_entries(&gpu->address_space_tree, 0, UVM_PAGE_SIZE_AGNOSTIC);
-
-    UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
-    va_size = 1ull << gpu->address_space_tree.hal->num_va_bits();
-    va_per_entry = va_size / num_entries;
-
-    // Make sure that RM's part of the VA is aligned to the VA covered by a
-    // single top level PDE.
-    UVM_ASSERT_MSG(gpu->parent->rm_va_base % va_per_entry == 0,
-                   "va_base 0x%llx va_per_entry 0x%llx\n",
-                   gpu->parent->rm_va_base,
-                   va_per_entry);
-    UVM_ASSERT_MSG(gpu->parent->rm_va_size % va_per_entry == 0,
-                   "va_size 0x%llx va_per_entry 0x%llx\n",
-                   gpu->parent->rm_va_size,
-                   va_per_entry);
-
-    tree_alloc = uvm_page_tree_pdb_internal(&gpu->address_space_tree);
-    if (tree_alloc->addr.aperture == UVM_APERTURE_VID)
-        physical_address = tree_alloc->addr.address;
-    else
-        physical_address = page_to_phys(tree_alloc->handle.page);
-    status = uvm_rm_locked_call(nvUvmInterfaceSetPageDirectory(gpu->rm_address_space,
-                                                               physical_address,
-                                                               num_entries,
-                                                               tree_alloc->addr.aperture == UVM_APERTURE_VID,
-                                                               gpu_get_internal_pasid(gpu),
-                                                               &dma_address));
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("nvUvmInterfaceSetPageDirectory() failed: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_page_tree_init(
+      gpu, NULL, UVM_PAGE_TREE_TYPE_KERNEL, gpu->big_page.internal_size,
+      uvm_get_page_tree_location(gpu), &gpu->address_space_tree);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Initializing the page tree failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
+
+  num_entries = uvm_mmu_page_tree_entries(&gpu->address_space_tree, 0,
+                                          UVM_PAGE_SIZE_AGNOSTIC);
+
+  UVM_ASSERT(gpu->address_space_tree.hal->num_va_bits() < 64);
+  va_size = 1ull << gpu->address_space_tree.hal->num_va_bits();
+  va_per_entry = va_size / num_entries;
+
+  // Make sure that RM's part of the VA is aligned to the VA covered by a
+  // single top level PDE.
+  UVM_ASSERT_MSG(gpu->parent->rm_va_base % va_per_entry == 0,
+                 "va_base 0x%llx va_per_entry 0x%llx\n",
+                 gpu->parent->rm_va_base, va_per_entry);
+  UVM_ASSERT_MSG(gpu->parent->rm_va_size % va_per_entry == 0,
+                 "va_size 0x%llx va_per_entry 0x%llx\n",
+                 gpu->parent->rm_va_size, va_per_entry);
+
+  tree_alloc = uvm_page_tree_pdb_internal(&gpu->address_space_tree);
+  if (tree_alloc->addr.aperture == UVM_APERTURE_VID)
+    physical_address = tree_alloc->addr.address;
+  else
+    physical_address = page_to_phys(tree_alloc->handle.page);
+  status = uvm_rm_locked_call(nvUvmInterfaceSetPageDirectory(
+      gpu->rm_address_space, physical_address, num_entries,
+      tree_alloc->addr.aperture == UVM_APERTURE_VID,
+      gpu_get_internal_pasid(gpu), &dma_address));
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("nvUvmInterfaceSetPageDirectory() failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    // The aperture here refers to sysmem, which only uses UVM_APERTURE_SYS
-    if (tree_alloc->addr.aperture == UVM_APERTURE_SYS)
-        gpu->address_space_tree.pdb_rm_dma_address = uvm_gpu_phys_address(UVM_APERTURE_SYS, dma_address);
+  // The aperture here refers to sysmem, which only uses UVM_APERTURE_SYS
+  if (tree_alloc->addr.aperture == UVM_APERTURE_SYS)
+    gpu->address_space_tree.pdb_rm_dma_address =
+        uvm_gpu_phys_address(UVM_APERTURE_SYS, dma_address);
 
-    gpu->rm_address_space_moved_to_page_tree = true;
+  gpu->rm_address_space_moved_to_page_tree = true;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static void deconfigure_address_space(uvm_gpu_t *gpu)
-{
-    if (gpu->rm_address_space_moved_to_page_tree)
-        uvm_rm_locked_call_void(nvUvmInterfaceUnsetPageDirectory(gpu->rm_address_space));
+static void deconfigure_address_space(uvm_gpu_t *gpu) {
+  if (gpu->rm_address_space_moved_to_page_tree)
+    uvm_rm_locked_call_void(
+        nvUvmInterfaceUnsetPageDirectory(gpu->rm_address_space));
 
-    if (gpu->address_space_tree.root)
-        uvm_page_tree_deinit(&gpu->address_space_tree);
+  if (gpu->address_space_tree.root)
+    uvm_page_tree_deinit(&gpu->address_space_tree);
 }
 
-static NV_STATUS service_interrupts(uvm_parent_gpu_t *parent_gpu)
-{
-    // Asking RM to service interrupts from top half interrupt handler would
-    // very likely deadlock.
-    UVM_ASSERT(!in_interrupt());
+static NV_STATUS service_interrupts(uvm_parent_gpu_t *parent_gpu) {
+  // Asking RM to service interrupts from top half interrupt handler would
+  // very likely deadlock.
+  UVM_ASSERT(!in_interrupt());
 
-    return uvm_rm_locked_call(nvUvmInterfaceServiceDeviceInterruptsRM(parent_gpu->rm_device));
+  return uvm_rm_locked_call(
+      nvUvmInterfaceServiceDeviceInterruptsRM(parent_gpu->rm_device));
 }
 
-NV_STATUS uvm_gpu_check_ecc_error(uvm_gpu_t *gpu)
-{
-    NV_STATUS status = uvm_gpu_check_ecc_error_no_rm(gpu);
+NV_STATUS uvm_gpu_check_ecc_error(uvm_gpu_t *gpu) {
+  NV_STATUS status = uvm_gpu_check_ecc_error_no_rm(gpu);
 
-    if (status == NV_OK || status != NV_WARN_MORE_PROCESSING_REQUIRED)
-        return status;
+  if (status == NV_OK || status != NV_WARN_MORE_PROCESSING_REQUIRED)
+    return status;
 
-    // An interrupt that might mean an ECC error needs to be serviced.
-    UVM_ASSERT(status == NV_WARN_MORE_PROCESSING_REQUIRED);
+  // An interrupt that might mean an ECC error needs to be serviced.
+  UVM_ASSERT(status == NV_WARN_MORE_PROCESSING_REQUIRED);
 
-    status = service_interrupts(gpu->parent);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Servicing interrupts failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = service_interrupts(gpu->parent);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Servicing interrupts failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    // After servicing interrupts the ECC error notifier should be current.
-    if (*gpu->ecc.error_notifier) {
-        UVM_ERR_PRINT("ECC error encountered, GPU %s\n", uvm_gpu_name(gpu));
-        uvm_global_set_fatal_error(NV_ERR_ECC_ERROR);
-        return NV_ERR_ECC_ERROR;
-    }
+  // After servicing interrupts the ECC error notifier should be current.
+  if (*gpu->ecc.error_notifier) {
+    UVM_ERR_PRINT("ECC error encountered, GPU %s\n", uvm_gpu_name(gpu));
+    uvm_global_set_fatal_error(NV_ERR_ECC_ERROR);
+    return NV_ERR_ECC_ERROR;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-NV_STATUS uvm_gpu_check_nvlink_error(uvm_gpu_t *gpu)
-{
-    NV_STATUS status = uvm_gpu_check_nvlink_error_no_rm(gpu);
+NV_STATUS uvm_gpu_check_nvlink_error(uvm_gpu_t *gpu) {
+  NV_STATUS status = uvm_gpu_check_nvlink_error_no_rm(gpu);
 
-    // Either NV_OK, or a resolved error code.
-    if (status != NV_WARN_MORE_PROCESSING_REQUIRED)
-        return status;
+  // Either NV_OK, or a resolved error code.
+  if (status != NV_WARN_MORE_PROCESSING_REQUIRED)
+    return status;
 
-    // An interrupt that might mean an NVLINK error needs to be serviced.
-    UVM_ASSERT(status == NV_WARN_MORE_PROCESSING_REQUIRED);
+  // An interrupt that might mean an NVLINK error needs to be serviced.
+  UVM_ASSERT(status == NV_WARN_MORE_PROCESSING_REQUIRED);
 
-    status = service_interrupts(gpu->parent);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Servicing interrupts failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = service_interrupts(gpu->parent);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Servicing interrupts failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    // Check injected error if present, this works even if nvlink_status is not
-    // supported.
-    if (uvm_enable_builtin_tests) {
-        if (atomic_read(&gpu->nvlink_status.injected_error) != UVM_TEST_NVLINK_ERROR_NONE)
-            return NV_ERR_MEMORY_ERROR;
+  // Check injected error if present, this works even if nvlink_status is not
+  // supported.
+  if (uvm_enable_builtin_tests) {
+    if (atomic_read(&gpu->nvlink_status.injected_error) !=
+        UVM_TEST_NVLINK_ERROR_NONE)
+      return NV_ERR_MEMORY_ERROR;
 
-        UVM_ASSERT(gpu->nvlink_status.error_notifier);
-    }
+    UVM_ASSERT(gpu->nvlink_status.error_notifier);
+  }
 
-    // After servicing interrupts, the NVLINK error notifier should be current.
-    if (*gpu->nvlink_status.error_notifier) {
-        UVM_ERR_PRINT("NVLINK error encountered, GPU %s\n", uvm_gpu_name(gpu));
-        return NV_ERR_MEMORY_ERROR;
-    }
+  // After servicing interrupts, the NVLINK error notifier should be current.
+  if (*gpu->nvlink_status.error_notifier) {
+    UVM_ERR_PRINT("NVLINK error encountered, GPU %s\n", uvm_gpu_name(gpu));
+    return NV_ERR_MEMORY_ERROR;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
 static NV_STATUS init_parent_gpu(uvm_parent_gpu_t *parent_gpu,
                                  const NvProcessorUuid *gpu_uuid,
                                  const UvmGpuInfo *gpu_info,
-                                 const UvmGpuPlatformInfo *gpu_platform_info)
-{
-    NV_STATUS status;
-    UvmGpuFbInfo fb_info = {0};
-
-    status = uvm_rm_locked_call(nvUvmInterfaceDeviceCreate(uvm_global_session_handle(),
-                                                           gpu_info,
-                                                           gpu_uuid,
-                                                           &parent_gpu->rm_device,
-                                                           NV_FALSE));
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Creating RM device failed: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+                                 const UvmGpuPlatformInfo *gpu_platform_info) {
+  NV_STATUS status;
+  UvmGpuFbInfo fb_info = {0};
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceDeviceCreate(uvm_global_session_handle(), gpu_info,
+                                 gpu_uuid, &parent_gpu->rm_device, NV_FALSE));
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Creating RM device failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
+
+  uvm_conf_computing_check_parent_gpu(parent_gpu);
+
+  parent_gpu->pci_dev = gpu_platform_info->pci_dev;
+  parent_gpu->closest_cpu_numa_node = dev_to_node(&parent_gpu->pci_dev->dev);
+  parent_gpu->dma_addressable_start = gpu_platform_info->dma_addressable_start;
+  parent_gpu->dma_addressable_limit = gpu_platform_info->dma_addressable_limit;
+  parent_gpu->egm.enabled = gpu_info->egmEnabled;
+  parent_gpu->egm.local_peer_id = gpu_info->egmPeerId;
+  parent_gpu->egm.base_address = gpu_info->egmBaseAddr;
+  parent_gpu->access_counters_supported =
+      (gpu_info->accessCntrBufferCount != 0);
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetFbInfo(parent_gpu->rm_device, &fb_info));
+  if (status != NV_OK)
+    return status;
 
-    uvm_conf_computing_check_parent_gpu(parent_gpu);
+  if (!fb_info.bZeroFb) {
+    unsigned long pci_bar1_addr = pci_resource_start(
+        parent_gpu->pci_dev, uvm_device_p2p_static_bar(parent_gpu));
+
+    parent_gpu->max_allocatable_address = fb_info.maxAllocatableAddress;
+    parent_gpu->static_bar1_start =
+        pci_bar1_addr + fb_info.staticBar1StartOffset;
+    parent_gpu->static_bar1_size = fb_info.staticBar1Size;
+    parent_gpu->static_bar1_write_combined = fb_info.bStaticBar1WriteCombined;
+  }
+
+  parent_gpu->cdmm_enabled = gpu_info->cdmmEnabled;
+
+  parent_gpu->virt_mode = gpu_info->virtMode;
+  if (parent_gpu->virt_mode == UVM_VIRT_MODE_LEGACY) {
+    UVM_ERR_PRINT("Failed to init GPU %s. UVM is not supported in legacy "
+                  "virtualization mode\n",
+                  uvm_parent_gpu_name(parent_gpu));
+    return NV_ERR_NOT_SUPPORTED;
+  }
+
+  if (gpu_info->isSimulated)
+    ++g_uvm_global.num_simulated_devices;
+
+  status = init_parent_procfs_dir(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init parent procfs dir: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    parent_gpu->pci_dev = gpu_platform_info->pci_dev;
-    parent_gpu->closest_cpu_numa_node = dev_to_node(&parent_gpu->pci_dev->dev);
-    parent_gpu->dma_addressable_start = gpu_platform_info->dma_addressable_start;
-    parent_gpu->dma_addressable_limit = gpu_platform_info->dma_addressable_limit;
-    parent_gpu->egm.enabled = gpu_info->egmEnabled;
-    parent_gpu->egm.local_peer_id = gpu_info->egmPeerId;
-    parent_gpu->egm.base_address = gpu_info->egmBaseAddr;
-    parent_gpu->access_counters_supported = (gpu_info->accessCntrBufferCount != 0);
+  status = uvm_hal_init_gpu(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init GPU hal: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    status = uvm_rm_locked_call(nvUvmInterfaceGetFbInfo(parent_gpu->rm_device, &fb_info));
-    if (status != NV_OK)
-        return status;
+  uvm_hal_init_properties(parent_gpu);
 
-    if (!fb_info.bZeroFb) {
-        unsigned long pci_bar1_addr = pci_resource_start(parent_gpu->pci_dev, uvm_device_p2p_static_bar(parent_gpu));
+  // On SoCs which are not NUMA aware use the linux kernel's fake NUMA node.
+  if (parent_gpu->is_integrated_gpu &&
+      parent_gpu->closest_cpu_numa_node == -1) {
+    UVM_ASSERT(num_possible_nodes() == 1);
+    parent_gpu->closest_cpu_numa_node = numa_mem_id();
+  }
 
-        parent_gpu->max_allocatable_address = fb_info.maxAllocatableAddress;
-        parent_gpu->static_bar1_start = pci_bar1_addr + fb_info.staticBar1StartOffset;
-        parent_gpu->static_bar1_size = fb_info.staticBar1Size;
-        parent_gpu->static_bar1_write_combined = fb_info.bStaticBar1WriteCombined;
-    }
+  UVM_ASSERT(!parent_gpu->rm_info.smcEnabled || parent_gpu->smc.supported);
+  parent_gpu->smc.enabled = !!parent_gpu->rm_info.smcEnabled;
 
-    parent_gpu->cdmm_enabled = gpu_info->cdmmEnabled;
+  uvm_mmu_init_gpu_chunk_sizes(parent_gpu);
 
-    parent_gpu->virt_mode = gpu_info->virtMode;
-    if (parent_gpu->virt_mode == UVM_VIRT_MODE_LEGACY) {
-        UVM_ERR_PRINT("Failed to init GPU %s. UVM is not supported in legacy virtualization mode\n",
-                      uvm_parent_gpu_name(parent_gpu));
-        return NV_ERR_NOT_SUPPORTED;
-    }
-
-    if (gpu_info->isSimulated)
-        ++g_uvm_global.num_simulated_devices;
+  status = uvm_pmm_devmem_init(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("failed to intialize device private memory: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    status = init_parent_procfs_dir(parent_gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init parent procfs dir: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+  uvm_pmm_gpu_device_p2p_init(parent_gpu);
 
-    status = uvm_hal_init_gpu(parent_gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init GPU hal: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+  status = uvm_ats_add_gpu(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("uvm_ats_add_gpu failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    uvm_hal_init_properties(parent_gpu);
+  status = init_parent_procfs_files(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init parent procfs files: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    // On SoCs which are not NUMA aware use the linux kernel's fake NUMA node.
-    if (parent_gpu->is_integrated_gpu && parent_gpu->closest_cpu_numa_node == -1) {
-        UVM_ASSERT(num_possible_nodes() == 1);
-        parent_gpu->closest_cpu_numa_node = numa_mem_id();
-    }
+  status = uvm_parent_gpu_init_isr(parent_gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init ISR: %s, GPU %s\n", nvstatusToString(status),
+                  uvm_parent_gpu_name(parent_gpu));
+    return status;
+  }
 
-    UVM_ASSERT(!parent_gpu->rm_info.smcEnabled || parent_gpu->smc.supported);
-    parent_gpu->smc.enabled = !!parent_gpu->rm_info.smcEnabled;
+  return NV_OK;
+}
 
-    uvm_mmu_init_gpu_chunk_sizes(parent_gpu);
+static NV_STATUS init_gpu(uvm_gpu_t *gpu, const UvmGpuInfo *gpu_info) {
+  char parent_uuid_buffer[UVM_UUID_STRING_LENGTH];
+  char gi_uuid_buffer[UVM_UUID_STRING_LENGTH];
+  NV_STATUS status;
 
-    status = uvm_pmm_devmem_init(parent_gpu);
+  if (gpu->parent->smc.enabled) {
+    status = uvm_rm_locked_call(nvUvmInterfaceDeviceCreate(
+        uvm_global_session_handle(), gpu_info, &gpu->parent->uuid,
+        &gpu->smc.rm_device, NV_TRUE));
     if (status != NV_OK) {
-        UVM_ERR_PRINT("failed to intialize device private memory: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
+      UVM_ERR_PRINT("Creating RM device failed: %s, GPU %s\n",
+                    nvstatusToString(status), uvm_gpu_name(gpu));
+      return status;
     }
+  }
 
-    uvm_pmm_gpu_device_p2p_init(parent_gpu);
-
-    status = uvm_ats_add_gpu(parent_gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("uvm_ats_add_gpu failed: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+  uvm_uuid_copy(&gpu->uuid, &gpu_info->uuid);
+  gpu->smc.swizz_id = gpu_info->smcSwizzId;
 
-    status = init_parent_procfs_files(parent_gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init parent procfs files: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+  uvm_uuid_string(parent_uuid_buffer, &gpu->parent->uuid);
+  uvm_uuid_string(gi_uuid_buffer, &gpu->uuid);
+  snprintf(gpu->name, sizeof(gpu->name),
+           "ID %u: " UVM_PARENT_GPU_UUID_PREFIX "%s " UVM_GPU_UUID_PREFIX "%s",
+           uvm_id_value(gpu->id), parent_uuid_buffer, gi_uuid_buffer);
 
-    status = uvm_parent_gpu_init_isr(parent_gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init ISR: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_parent_gpu_name(parent_gpu));
-        return status;
-    }
+  printk(KERN_INFO "init_gpu gpu->name: %s\n", gpu->name);
 
-    return NV_OK;
-}
-
-static NV_STATUS init_gpu(uvm_gpu_t *gpu, const UvmGpuInfo *gpu_info)
-{
-    char parent_uuid_buffer[UVM_UUID_STRING_LENGTH];
-    char gi_uuid_buffer[UVM_UUID_STRING_LENGTH];
-    NV_STATUS status;
-
-    if (gpu->parent->smc.enabled) {
-        status = uvm_rm_locked_call(nvUvmInterfaceDeviceCreate(uvm_global_session_handle(),
-                                                               gpu_info,
-                                                               &gpu->parent->uuid,
-                                                               &gpu->smc.rm_device,
-                                                               NV_TRUE));
-        if (status != NV_OK) {
-            UVM_ERR_PRINT("Creating RM device failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-            return status;
-        }
-    }
-
-    uvm_uuid_copy(&gpu->uuid, &gpu_info->uuid);
-    gpu->smc.swizz_id = gpu_info->smcSwizzId;
-
-    uvm_uuid_string(parent_uuid_buffer, &gpu->parent->uuid);
-    uvm_uuid_string(gi_uuid_buffer, &gpu->uuid);
-    snprintf(gpu->name,
-             sizeof(gpu->name),
-             "ID %u: " UVM_PARENT_GPU_UUID_PREFIX "%s " UVM_GPU_UUID_PREFIX "%s",
-             uvm_id_value(gpu->id),
-             parent_uuid_buffer,
-             gi_uuid_buffer);
-
-    // Initialize the per-GPU procfs dirs as early as possible so that other
-    // parts of the driver can add files in them as part of their per-GPU init.
-    status = init_procfs_dirs(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init procfs dirs: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  // Initialize the per-GPU procfs dirs as early as possible so that other
+  // parts of the driver can add files in them as part of their per-GPU init.
+  status = init_procfs_dirs(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init procfs dirs: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = get_gpu_caps(gpu);
-    if (status != NV_OK) {
-        if (status != NV_ERR_NVLINK_FABRIC_NOT_READY)
-            UVM_ERR_PRINT("Failed to get GPU caps: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = get_gpu_caps(gpu);
+  if (status != NV_OK) {
+    if (status != NV_ERR_NVLINK_FABRIC_NOT_READY)
+      UVM_ERR_PRINT("Failed to get GPU caps: %s, GPU %s\n",
+                    nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    uvm_mmu_init_gpu_peer_addresses(gpu);
+  uvm_mmu_init_gpu_peer_addresses(gpu);
 
-    status = alloc_and_init_address_space(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Creating RM address space failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = alloc_and_init_address_space(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Creating RM address space failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = get_gpu_fb_info(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to get GPU FB info: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = get_gpu_fb_info(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to get GPU FB info: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = get_gpu_ecc_info(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to get GPU ECC info: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = get_gpu_ecc_info(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to get GPU ECC info: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = get_gpu_nvlink_info(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to get GPU NVLINK RECOVERY info: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = get_gpu_nvlink_info(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to get GPU NVLINK RECOVERY info: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = uvm_pmm_gpu_init(&gpu->pmm);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("PMM initialization failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_pmm_gpu_init(&gpu->pmm);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("PMM initialization failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    uvm_mutex_init(&gpu->device_p2p_lock, UVM_LOCK_ORDER_GLOBAL);
+  uvm_mutex_init(&gpu->device_p2p_lock, UVM_LOCK_ORDER_GLOBAL);
 
-    status = init_semaphore_pools(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to initialize the semaphore pool: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_gpu_name(gpu));
-        return status;
-    }
+  status = init_semaphore_pools(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to initialize the semaphore pool: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = uvm_channel_manager_create(gpu, &gpu->channel_manager);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to initialize the channel manager: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_channel_manager_create(gpu, &gpu->channel_manager);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to initialize the channel manager: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = configure_address_space(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to configure the GPU address space: %s, GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_gpu_name(gpu));
-        return status;
-    }
+  status = configure_address_space(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to configure the GPU address space: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = uvm_mmu_create_flat_mappings(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Creating flat mappings failed: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_mmu_create_flat_mappings(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Creating flat mappings failed: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = uvm_conf_computing_gpu_init(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to initialize Confidential Compute: %s for GPU %s\n",
-                      nvstatusToString(status),
-                      uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_conf_computing_gpu_init(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to initialize Confidential Compute: %s for GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = init_procfs_files(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init procfs files: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = init_procfs_files(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init procfs files: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    status = uvm_perf_heuristics_add_gpu(gpu);
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("Failed to init heuristics: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
-        return status;
-    }
+  status = uvm_perf_heuristics_add_gpu(gpu);
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("Failed to init heuristics: %s, GPU %s\n",
+                  nvstatusToString(status), uvm_gpu_name(gpu));
+    return status;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
 static void sync_parent_gpu_trackers(uvm_parent_gpu_t *parent_gpu,
                                      bool sync_replay_tracker,
-                                     bool sync_clear_faulted_tracker)
-{
-    NV_STATUS status;
+                                     bool sync_clear_faulted_tracker) {
+  NV_STATUS status;
 
-    // Sync the replay tracker since it inherits dependencies from the VA block
-    // trackers.
-    if (sync_replay_tracker) {
-        uvm_parent_gpu_replayable_faults_isr_lock(parent_gpu);
-        status = uvm_tracker_wait(&parent_gpu->fault_buffer.replayable.replay_tracker);
-        uvm_parent_gpu_replayable_faults_isr_unlock(parent_gpu);
+  // Sync the replay tracker since it inherits dependencies from the VA block
+  // trackers.
+  if (sync_replay_tracker) {
+    uvm_parent_gpu_replayable_faults_isr_lock(parent_gpu);
+    status =
+        uvm_tracker_wait(&parent_gpu->fault_buffer.replayable.replay_tracker);
+    uvm_parent_gpu_replayable_faults_isr_unlock(parent_gpu);
 
-        if (status != NV_OK)
-            UVM_ASSERT(status == uvm_global_get_status());
-    }
+    if (status != NV_OK)
+      UVM_ASSERT(status == uvm_global_get_status());
+  }
 
-    // Sync the clear_faulted tracker since it inherits dependencies from the
-    // VA block trackers, too.
-    if (sync_clear_faulted_tracker) {
-        uvm_parent_gpu_non_replayable_faults_isr_lock(parent_gpu);
-        status = uvm_tracker_wait(&parent_gpu->fault_buffer.non_replayable.clear_faulted_tracker);
-        uvm_parent_gpu_non_replayable_faults_isr_unlock(parent_gpu);
+  // Sync the clear_faulted tracker since it inherits dependencies from the
+  // VA block trackers, too.
+  if (sync_clear_faulted_tracker) {
+    uvm_parent_gpu_non_replayable_faults_isr_lock(parent_gpu);
+    status = uvm_tracker_wait(
+        &parent_gpu->fault_buffer.non_replayable.clear_faulted_tracker);
+    uvm_parent_gpu_non_replayable_faults_isr_unlock(parent_gpu);
 
-        if (status != NV_OK)
-            UVM_ASSERT(status == uvm_global_get_status());
-    }
+    if (status != NV_OK)
+      UVM_ASSERT(status == uvm_global_get_status());
+  }
+
+  // Sync the access counter clear tracker too.
+  if (parent_gpu->access_counters_supported &&
+      parent_gpu->access_counters.buffer) {
+    uvm_mutex_lock(&parent_gpu->access_counters.clear_tracker_lock);
+    status = uvm_tracker_wait(&parent_gpu->access_counters.clear_tracker);
+    uvm_mutex_unlock(&parent_gpu->access_counters.clear_tracker_lock);
 
-    // Sync the access counter clear tracker too.
-    if (parent_gpu->access_counters_supported && parent_gpu->access_counters.buffer) {
-        uvm_mutex_lock(&parent_gpu->access_counters.clear_tracker_lock);
-        status = uvm_tracker_wait(&parent_gpu->access_counters.clear_tracker);
-        uvm_mutex_unlock(&parent_gpu->access_counters.clear_tracker_lock);
+    if (status != NV_OK)
+      UVM_ASSERT(status == uvm_global_get_status());
+
+    if (parent_gpu->access_counters_serialize_clear_ops_by_type) {
+      uvm_access_counter_clear_op_t op;
+      uvm_mutex_lock(&parent_gpu->access_counters.serialize_clear_lock);
+      for (op = 0; op < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; op++) {
+        status = uvm_tracker_wait(
+            &parent_gpu->access_counters.serialize_clear_tracker[op]);
 
         if (status != NV_OK)
-            UVM_ASSERT(status == uvm_global_get_status());
-
-        if (parent_gpu->access_counters_serialize_clear_ops_by_type) {
-            uvm_access_counter_clear_op_t op;
-            uvm_mutex_lock(&parent_gpu->access_counters.serialize_clear_lock);
-            for (op = 0; op < UVM_ACCESS_COUNTER_CLEAR_OP_COUNT; op++) {
-                status = uvm_tracker_wait(&parent_gpu->access_counters.serialize_clear_tracker[op]);
-
-                if (status != NV_OK)
-                    UVM_ASSERT(status == uvm_global_get_status());
-            }
-            uvm_mutex_unlock(&parent_gpu->access_counters.serialize_clear_lock);
-        }
+          UVM_ASSERT(status == uvm_global_get_status());
+      }
+      uvm_mutex_unlock(&parent_gpu->access_counters.serialize_clear_lock);
     }
+  }
 }
 
-void uvm_parent_gpu_sync_trackers(uvm_parent_gpu_t *parent_gpu)
-{
-    sync_parent_gpu_trackers(parent_gpu,
-                             parent_gpu->isr.replayable_faults.handling,
-                             parent_gpu->isr.non_replayable_faults.handling);
+void uvm_parent_gpu_sync_trackers(uvm_parent_gpu_t *parent_gpu) {
+  sync_parent_gpu_trackers(parent_gpu,
+                           parent_gpu->isr.replayable_faults.handling,
+                           parent_gpu->isr.non_replayable_faults.handling);
 }
 
 // Remove all references the given GPU has to other GPUs, since one of those
 // other GPUs is getting removed. This involves waiting for any unfinished
 // trackers contained by this GPU.
-static void remove_gpus_from_gpu(uvm_gpu_t *gpu)
-{
-    uvm_parent_gpu_sync_trackers(gpu->parent);
+static void remove_gpus_from_gpu(uvm_gpu_t *gpu) {
+  uvm_parent_gpu_sync_trackers(gpu->parent);
 
-    // Sync all trackers in PMM
-    uvm_pmm_gpu_sync(&gpu->pmm);
+  // Sync all trackers in PMM
+  uvm_pmm_gpu_sync(&gpu->pmm);
 
-    // Sync all trackers in the GPU's DMA allocation pool
-    uvm_conf_computing_dma_buffer_pool_sync(&gpu->conf_computing.dma_buffer_pool);
+  // Sync all trackers in the GPU's DMA allocation pool
+  uvm_conf_computing_dma_buffer_pool_sync(&gpu->conf_computing.dma_buffer_pool);
 }
 
 // Remove all references to the given GPU from its parent, since it is being
 // removed. This involves waiting for any unfinished trackers contained
 // by the parent GPU.
-static void remove_gpu_from_parent_gpu(uvm_gpu_t *gpu)
-{
-    // We use *.was_handling instead of *.handling here since this function is
-    // called after uvm_gpu_disable_isr(), and the *.handling flags will
-    // already have been copied to *.was_handling, and then set to false.
-    sync_parent_gpu_trackers(gpu->parent,
-                             gpu->parent->isr.replayable_faults.was_handling,
-                             gpu->parent->isr.non_replayable_faults.was_handling);
+static void remove_gpu_from_parent_gpu(uvm_gpu_t *gpu) {
+  // We use *.was_handling instead of *.handling here since this function is
+  // called after uvm_gpu_disable_isr(), and the *.handling flags will
+  // already have been copied to *.was_handling, and then set to false.
+  sync_parent_gpu_trackers(gpu->parent,
+                           gpu->parent->isr.replayable_faults.was_handling,
+                           gpu->parent->isr.non_replayable_faults.was_handling);
 }
 
-static void deinit_parent_gpu(uvm_parent_gpu_t *parent_gpu)
-{
-    // All channels should have been removed before the retained count went to 0
-    UVM_ASSERT(uvm_rb_tree_empty(&parent_gpu->instance_ptr_table));
-    UVM_ASSERT(uvm_rb_tree_empty(&parent_gpu->tsg_table));
+static void deinit_parent_gpu(uvm_parent_gpu_t *parent_gpu) {
+  // All channels should have been removed before the retained count went to 0
+  UVM_ASSERT(uvm_rb_tree_empty(&parent_gpu->instance_ptr_table));
+  UVM_ASSERT(uvm_rb_tree_empty(&parent_gpu->tsg_table));
 
-    deinit_parent_procfs_files(parent_gpu);
+  deinit_parent_procfs_files(parent_gpu);
 
-    // Return ownership to RM
-    uvm_parent_gpu_deinit_isr(parent_gpu);
+  // Return ownership to RM
+  uvm_parent_gpu_deinit_isr(parent_gpu);
 
-    uvm_pmm_gpu_device_p2p_deinit(parent_gpu);
+  uvm_pmm_gpu_device_p2p_deinit(parent_gpu);
 
-    uvm_pmm_devmem_deinit(parent_gpu);
-    uvm_ats_remove_gpu(parent_gpu);
+  uvm_pmm_devmem_deinit(parent_gpu);
+  uvm_ats_remove_gpu(parent_gpu);
 
-    UVM_ASSERT(atomic64_read(&parent_gpu->mapped_cpu_pages_size) == 0);
+  UVM_ASSERT(atomic64_read(&parent_gpu->mapped_cpu_pages_size) == 0);
 
-    // After calling nvUvmInterfaceUnregisterGpu() the reference to pci_dev may
-    // not be valid any more so clear it ahead of time.
-    parent_gpu->pci_dev = NULL;
+  // After calling nvUvmInterfaceUnregisterGpu() the reference to pci_dev may
+  // not be valid any more so clear it ahead of time.
+  parent_gpu->pci_dev = NULL;
 
-    deinit_parent_procfs_dir(parent_gpu);
+  deinit_parent_procfs_dir(parent_gpu);
 
-    if (parent_gpu->rm_info.isSimulated)
-        --g_uvm_global.num_simulated_devices;
+  if (parent_gpu->rm_info.isSimulated)
+    --g_uvm_global.num_simulated_devices;
 
-    if (parent_gpu->rm_device != 0)
-        uvm_rm_locked_call_void(nvUvmInterfaceDeviceDestroy(parent_gpu->rm_device));
+  if (parent_gpu->rm_device != 0)
+    uvm_rm_locked_call_void(nvUvmInterfaceDeviceDestroy(parent_gpu->rm_device));
 
-    uvm_parent_gpu_kref_put(parent_gpu);
+  uvm_parent_gpu_kref_put(parent_gpu);
 }
 
-static void deinit_gpu(uvm_gpu_t *gpu)
-{
-    uvm_gpu_t *other_gpu;
+static void deinit_gpu(uvm_gpu_t *gpu) {
+  uvm_gpu_t *other_gpu;
 
-    // Remove any pointers to this GPU from other GPUs' trackers.
-    for_each_gpu(other_gpu) {
-        UVM_ASSERT(other_gpu != gpu);
-        remove_gpus_from_gpu(other_gpu);
-    }
+  // Remove any pointers to this GPU from other GPUs' trackers.
+  for_each_gpu(other_gpu) {
+    UVM_ASSERT(other_gpu != gpu);
+    remove_gpus_from_gpu(other_gpu);
+  }
 
-    // Further, remove any pointers to this GPU from its parent's trackers.
-    remove_gpu_from_parent_gpu(gpu);
+  // Further, remove any pointers to this GPU from its parent's trackers.
+  remove_gpu_from_parent_gpu(gpu);
 
-    uvm_perf_heuristics_remove_gpu(gpu);
+  uvm_perf_heuristics_remove_gpu(gpu);
 
-    deinit_procfs_files(gpu);
+  deinit_procfs_files(gpu);
 
-    // TODO Bug 3429163: [UVM] Move uvm_mmu_destroy_flat_mapping() to the
-    // correct spot
-    uvm_mmu_destroy_flat_mappings(gpu);
+  // TODO Bug 3429163: [UVM] Move uvm_mmu_destroy_flat_mapping() to the
+  // correct spot
+  uvm_mmu_destroy_flat_mappings(gpu);
 
-    // Wait for any deferred frees and their associated trackers to be finished
-    // before tearing down channels.
-    uvm_pmm_gpu_sync(&gpu->pmm);
+  // Wait for any deferred frees and their associated trackers to be finished
+  // before tearing down channels.
+  uvm_pmm_gpu_sync(&gpu->pmm);
 
-    uvm_channel_manager_destroy(gpu->channel_manager);
+  uvm_channel_manager_destroy(gpu->channel_manager);
 
-    // Deconfigure the address space only after destroying all the channels as
-    // in case any of them hit fatal errors, RM will assert that they are not
-    // idle during nvUvmInterfaceUnsetPageDirectory() and that's an unnecessary
-    // pain during development.
-    deconfigure_address_space(gpu);
+  // Deconfigure the address space only after destroying all the channels as
+  // in case any of them hit fatal errors, RM will assert that they are not
+  // idle during nvUvmInterfaceUnsetPageDirectory() and that's an unnecessary
+  // pain during development.
+  deconfigure_address_space(gpu);
 
-    deinit_semaphore_pools(gpu);
+  deinit_semaphore_pools(gpu);
 
-    uvm_pmm_gpu_deinit(&gpu->pmm);
+  uvm_pmm_gpu_deinit(&gpu->pmm);
 
-    if (gpu->rm_address_space != 0)
-        uvm_rm_locked_call_void(nvUvmInterfaceAddressSpaceDestroy(gpu->rm_address_space));
+  if (gpu->rm_address_space != 0)
+    uvm_rm_locked_call_void(
+        nvUvmInterfaceAddressSpaceDestroy(gpu->rm_address_space));
 
-    deinit_procfs_dirs(gpu);
+  deinit_procfs_dirs(gpu);
 
-    if (gpu->parent->smc.enabled) {
-        if (gpu->smc.rm_device != 0)
-            uvm_rm_locked_call_void(nvUvmInterfaceDeviceDestroy(gpu->smc.rm_device));
-    }
+  if (gpu->parent->smc.enabled) {
+    if (gpu->smc.rm_device != 0)
+      uvm_rm_locked_call_void(nvUvmInterfaceDeviceDestroy(gpu->smc.rm_device));
+  }
 
-    gpu->magic = 0;
+  gpu->magic = 0;
 }
 
 // Do not not call this directly. It is called by nv_kref_put, when the
 // GPU's ref count drops to zero.
-static void uvm_parent_gpu_destroy(nv_kref_t *nv_kref)
-{
-    uvm_parent_gpu_t *parent_gpu = container_of(nv_kref, uvm_parent_gpu_t, gpu_kref);
-    NvU32 sub_processor_index;
-
-    UVM_ASSERT(parent_gpu->num_retained_gpus == 0);
-    UVM_ASSERT(bitmap_empty(parent_gpu->valid_gpus, UVM_PARENT_ID_MAX_SUB_PROCESSORS));
-
-    nv_kthread_q_stop(&parent_gpu->lazy_free_q);
-
-    for_each_sub_processor_index(sub_processor_index)
-        UVM_ASSERT(!parent_gpu->gpus[sub_processor_index]);
-
-    uvm_tracker_deinit(&parent_gpu->access_counters.clear_tracker);
-    deinit_access_counters_serialize_clear_tracker(parent_gpu);
-
-    uvm_kvfree(parent_gpu);
-}
-
-void uvm_parent_gpu_kref_put(uvm_parent_gpu_t *parent_gpu)
-{
-    nv_kref_put(&parent_gpu->gpu_kref, uvm_parent_gpu_destroy);
-}
-
-static void update_stats_parent_gpu_fault_instance(uvm_parent_gpu_t *parent_gpu,
-                                                   const uvm_fault_buffer_entry_t *fault_entry,
-                                                   bool is_duplicate)
-{
-    if (!fault_entry->is_replayable) {
-        switch (fault_entry->fault_access_type)
-        {
-            case UVM_FAULT_ACCESS_TYPE_READ:
-                ++parent_gpu->fault_buffer.non_replayable.stats.num_read_faults;
-                break;
-            case UVM_FAULT_ACCESS_TYPE_WRITE:
-                ++parent_gpu->fault_buffer.non_replayable.stats.num_write_faults;
-                break;
-            case UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK:
-            case UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG:
-                ++parent_gpu->fault_buffer.non_replayable.stats.num_atomic_faults;
-                break;
-            default:
-                UVM_ASSERT_MSG(false, "Invalid access type for non-replayable faults\n");
-                break;
-        }
-
-        if (!fault_entry->is_virtual)
-            ++parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults;
-
-        ++parent_gpu->stats.num_non_replayable_faults;
-
-        return;
-    }
+static void uvm_parent_gpu_destroy(nv_kref_t *nv_kref) {
+  uvm_parent_gpu_t *parent_gpu =
+      container_of(nv_kref, uvm_parent_gpu_t, gpu_kref);
+  NvU32 sub_processor_index;
+
+  UVM_ASSERT(parent_gpu->num_retained_gpus == 0);
+  UVM_ASSERT(
+      bitmap_empty(parent_gpu->valid_gpus, UVM_PARENT_ID_MAX_SUB_PROCESSORS));
+
+  nv_kthread_q_stop(&parent_gpu->lazy_free_q);
+
+  for_each_sub_processor_index(sub_processor_index)
+      UVM_ASSERT(!parent_gpu->gpus[sub_processor_index]);
+
+  uvm_tracker_deinit(&parent_gpu->access_counters.clear_tracker);
+  deinit_access_counters_serialize_clear_tracker(parent_gpu);
+
+  uvm_kvfree(parent_gpu);
+}
+
+void uvm_parent_gpu_kref_put(uvm_parent_gpu_t *parent_gpu) {
+  nv_kref_put(&parent_gpu->gpu_kref, uvm_parent_gpu_destroy);
+}
 
-    UVM_ASSERT(fault_entry->is_virtual);
-
-    switch (fault_entry->fault_access_type)
-    {
-        case UVM_FAULT_ACCESS_TYPE_PREFETCH:
-            ++parent_gpu->fault_buffer.replayable.stats.num_prefetch_faults;
-            break;
-        case UVM_FAULT_ACCESS_TYPE_READ:
-            ++parent_gpu->fault_buffer.replayable.stats.num_read_faults;
-            break;
-        case UVM_FAULT_ACCESS_TYPE_WRITE:
-            ++parent_gpu->fault_buffer.replayable.stats.num_write_faults;
-            break;
-        case UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK:
-        case UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG:
-            ++parent_gpu->fault_buffer.replayable.stats.num_atomic_faults;
-            break;
-        default:
-            break;
+static void update_stats_parent_gpu_fault_instance(
+    uvm_parent_gpu_t *parent_gpu, const uvm_fault_buffer_entry_t *fault_entry,
+    bool is_duplicate) {
+  if (!fault_entry->is_replayable) {
+    switch (fault_entry->fault_access_type) {
+    case UVM_FAULT_ACCESS_TYPE_READ:
+      ++parent_gpu->fault_buffer.non_replayable.stats.num_read_faults;
+      break;
+    case UVM_FAULT_ACCESS_TYPE_WRITE:
+      ++parent_gpu->fault_buffer.non_replayable.stats.num_write_faults;
+      break;
+    case UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK:
+    case UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG:
+      ++parent_gpu->fault_buffer.non_replayable.stats.num_atomic_faults;
+      break;
+    default:
+      UVM_ASSERT_MSG(false, "Invalid access type for non-replayable faults\n");
+      break;
     }
-    if (is_duplicate || fault_entry->filtered)
-        ++parent_gpu->fault_buffer.replayable.stats.num_duplicate_faults;
 
-    ++parent_gpu->stats.num_replayable_faults;
+    if (!fault_entry->is_virtual)
+      ++parent_gpu->fault_buffer.non_replayable.stats.num_physical_faults;
+
+    ++parent_gpu->stats.num_non_replayable_faults;
+
+    return;
+  }
+
+  UVM_ASSERT(fault_entry->is_virtual);
+
+  switch (fault_entry->fault_access_type) {
+  case UVM_FAULT_ACCESS_TYPE_PREFETCH:
+    ++parent_gpu->fault_buffer.replayable.stats.num_prefetch_faults;
+    break;
+  case UVM_FAULT_ACCESS_TYPE_READ:
+    ++parent_gpu->fault_buffer.replayable.stats.num_read_faults;
+    break;
+  case UVM_FAULT_ACCESS_TYPE_WRITE:
+    ++parent_gpu->fault_buffer.replayable.stats.num_write_faults;
+    break;
+  case UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK:
+  case UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG:
+    ++parent_gpu->fault_buffer.replayable.stats.num_atomic_faults;
+    break;
+  default:
+    break;
+  }
+  if (is_duplicate || fault_entry->filtered)
+    ++parent_gpu->fault_buffer.replayable.stats.num_duplicate_faults;
+
+  ++parent_gpu->stats.num_replayable_faults;
 }
 
 static void update_stats_fault_cb(uvm_va_space_t *va_space,
                                   uvm_perf_event_t event_id,
-                                  uvm_perf_event_data_t *event_data)
-{
-    uvm_parent_gpu_t *parent_gpu;
-    const uvm_fault_buffer_entry_t *fault_entry, *fault_instance;
+                                  uvm_perf_event_data_t *event_data) {
+  uvm_parent_gpu_t *parent_gpu;
+  const uvm_fault_buffer_entry_t *fault_entry, *fault_instance;
 
-    UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
+  UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
 
-    if (UVM_ID_IS_CPU(event_data->fault.proc_id))
-        return;
+  if (UVM_ID_IS_CPU(event_data->fault.proc_id))
+    return;
 
-    // The reported fault entry must be the "representative" fault entry
-    UVM_ASSERT(!event_data->fault.gpu.buffer_entry->filtered);
+  // The reported fault entry must be the "representative" fault entry
+  UVM_ASSERT(!event_data->fault.gpu.buffer_entry->filtered);
 
-    parent_gpu = uvm_gpu_get(event_data->fault.proc_id)->parent;
+  parent_gpu = uvm_gpu_get(event_data->fault.proc_id)->parent;
 
-    fault_entry = event_data->fault.gpu.buffer_entry;
+  fault_entry = event_data->fault.gpu.buffer_entry;
 
-    // Update the stats using the representative fault entry and the rest of
-    // instances
-    update_stats_parent_gpu_fault_instance(parent_gpu, fault_entry, event_data->fault.gpu.is_duplicate);
+  // Update the stats using the representative fault entry and the rest of
+  // instances
+  update_stats_parent_gpu_fault_instance(parent_gpu, fault_entry,
+                                         event_data->fault.gpu.is_duplicate);
 
-    list_for_each_entry(fault_instance, &fault_entry->merged_instances_list, merged_instances_list)
-        update_stats_parent_gpu_fault_instance(parent_gpu, fault_instance, event_data->fault.gpu.is_duplicate);
+  list_for_each_entry(fault_instance, &fault_entry->merged_instances_list,
+                      merged_instances_list)
+      update_stats_parent_gpu_fault_instance(
+          parent_gpu, fault_instance, event_data->fault.gpu.is_duplicate);
 }
 
 static void update_stats_migration_cb(uvm_va_space_t *va_space,
                                       uvm_perf_event_t event_id,
-                                      uvm_perf_event_data_t *event_data)
-{
-    uvm_gpu_t *gpu_dst = NULL;
-    uvm_gpu_t *gpu_src = NULL;
-    NvU64 pages;
-    bool is_replayable_fault;
-    bool is_non_replayable_fault;
-    bool is_access_counter;
-
-    UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
-
-    if (UVM_ID_IS_GPU(event_data->migration.dst))
-        gpu_dst = uvm_gpu_get(event_data->migration.dst);
-
-    if (UVM_ID_IS_GPU(event_data->migration.src))
-        gpu_src = uvm_gpu_get(event_data->migration.src);
-
-    if (!gpu_dst && !gpu_src)
-        return;
-
-    // Page prefetching is also triggered by faults
-    is_replayable_fault =
-        event_data->migration.cause == UVM_MAKE_RESIDENT_CAUSE_REPLAYABLE_FAULT;
-    is_non_replayable_fault =
-        event_data->migration.cause == UVM_MAKE_RESIDENT_CAUSE_NON_REPLAYABLE_FAULT;
-    is_access_counter =
-        event_data->migration.cause == UVM_MAKE_RESIDENT_CAUSE_ACCESS_COUNTER;
-
-    pages = event_data->migration.bytes / PAGE_SIZE;
-    UVM_ASSERT(event_data->migration.bytes % PAGE_SIZE == 0);
-    UVM_ASSERT(pages > 0);
-
-    if (gpu_dst) {
-        atomic64_add(pages, &gpu_dst->parent->stats.num_pages_in);
-        if (is_replayable_fault) {
-            atomic64_add(pages, &gpu_dst->parent->fault_buffer.replayable.stats.num_pages_in);
-        }
-        else if (is_non_replayable_fault) {
-            atomic64_add(pages, &gpu_dst->parent->fault_buffer.non_replayable.stats.num_pages_in);
-        }
-        else if (is_access_counter) {
-            NvU32 index = event_data->migration.access_counters_buffer_index;
-            atomic64_add(pages, &gpu_dst->parent->access_counters.buffer[index].stats.num_pages_in);
-        }
-    }
-    if (gpu_src) {
-        atomic64_add(pages, &gpu_src->parent->stats.num_pages_out);
-        if (is_replayable_fault) {
-            atomic64_add(pages, &gpu_src->parent->fault_buffer.replayable.stats.num_pages_out);
-        }
-        else if (is_non_replayable_fault) {
-            atomic64_add(pages, &gpu_src->parent->fault_buffer.non_replayable.stats.num_pages_out);
-        }
-        else if (is_access_counter) {
-            NvU32 index = event_data->migration.access_counters_buffer_index;
-            atomic64_add(pages, &gpu_src->parent->access_counters.buffer[index].stats.num_pages_out);
-        }
-    }
+                                      uvm_perf_event_data_t *event_data) {
+  uvm_gpu_t *gpu_dst = NULL;
+  uvm_gpu_t *gpu_src = NULL;
+  NvU64 pages;
+  bool is_replayable_fault;
+  bool is_non_replayable_fault;
+  bool is_access_counter;
+
+  UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
+
+  if (UVM_ID_IS_GPU(event_data->migration.dst))
+    gpu_dst = uvm_gpu_get(event_data->migration.dst);
+
+  if (UVM_ID_IS_GPU(event_data->migration.src))
+    gpu_src = uvm_gpu_get(event_data->migration.src);
+
+  if (!gpu_dst && !gpu_src)
+    return;
+
+  // Page prefetching is also triggered by faults
+  is_replayable_fault =
+      event_data->migration.cause == UVM_MAKE_RESIDENT_CAUSE_REPLAYABLE_FAULT;
+  is_non_replayable_fault = event_data->migration.cause ==
+                            UVM_MAKE_RESIDENT_CAUSE_NON_REPLAYABLE_FAULT;
+  is_access_counter =
+      event_data->migration.cause == UVM_MAKE_RESIDENT_CAUSE_ACCESS_COUNTER;
+
+  pages = event_data->migration.bytes / PAGE_SIZE;
+  UVM_ASSERT(event_data->migration.bytes % PAGE_SIZE == 0);
+  UVM_ASSERT(pages > 0);
+
+  if (gpu_dst) {
+    atomic64_add(pages, &gpu_dst->parent->stats.num_pages_in);
+    if (is_replayable_fault) {
+      atomic64_add(
+          pages, &gpu_dst->parent->fault_buffer.replayable.stats.num_pages_in);
+    } else if (is_non_replayable_fault) {
+      atomic64_add(
+          pages,
+          &gpu_dst->parent->fault_buffer.non_replayable.stats.num_pages_in);
+    } else if (is_access_counter) {
+      NvU32 index = event_data->migration.access_counters_buffer_index;
+      atomic64_add(
+          pages,
+          &gpu_dst->parent->access_counters.buffer[index].stats.num_pages_in);
+    }
+  }
+  if (gpu_src) {
+    atomic64_add(pages, &gpu_src->parent->stats.num_pages_out);
+    if (is_replayable_fault) {
+      atomic64_add(
+          pages, &gpu_src->parent->fault_buffer.replayable.stats.num_pages_out);
+    } else if (is_non_replayable_fault) {
+      atomic64_add(
+          pages,
+          &gpu_src->parent->fault_buffer.non_replayable.stats.num_pages_out);
+    } else if (is_access_counter) {
+      NvU32 index = event_data->migration.access_counters_buffer_index;
+      atomic64_add(
+          pages,
+          &gpu_src->parent->access_counters.buffer[index].stats.num_pages_out);
+    }
+  }
 }
 
 // Override the UVM driver and GPU settings from the module loader
-static void uvm_param_conf(void)
-{
-    // uvm_peer_copy: Valid entries are "phys" and "virt" for Ampere+ GPUs.
-    // No effect in pre-Ampere GPUs
-    if (strcmp(uvm_peer_copy, UVM_PARAM_PEER_COPY_VIRTUAL) == 0) {
-        g_uvm_global.peer_copy_mode = UVM_GPU_PEER_COPY_MODE_VIRTUAL;
-    }
-    else {
-        if (strcmp(uvm_peer_copy, UVM_PARAM_PEER_COPY_PHYSICAL) != 0) {
-            UVM_INFO_PRINT("Invalid value for uvm_peer_copy = %s, using %s instead.\n",
-                           uvm_peer_copy,
-                           UVM_PARAM_PEER_COPY_PHYSICAL);
-        }
-
-        g_uvm_global.peer_copy_mode = UVM_GPU_PEER_COPY_MODE_PHYSICAL;
+static void uvm_param_conf(void) {
+  // uvm_peer_copy: Valid entries are "phys" and "virt" for Ampere+ GPUs.
+  // No effect in pre-Ampere GPUs
+  if (strcmp(uvm_peer_copy, UVM_PARAM_PEER_COPY_VIRTUAL) == 0) {
+    g_uvm_global.peer_copy_mode = UVM_GPU_PEER_COPY_MODE_VIRTUAL;
+  } else {
+    if (strcmp(uvm_peer_copy, UVM_PARAM_PEER_COPY_PHYSICAL) != 0) {
+      UVM_INFO_PRINT(
+          "Invalid value for uvm_peer_copy = %s, using %s instead.\n",
+          uvm_peer_copy, UVM_PARAM_PEER_COPY_PHYSICAL);
     }
+
+    g_uvm_global.peer_copy_mode = UVM_GPU_PEER_COPY_MODE_PHYSICAL;
+  }
 }
 
-NV_STATUS uvm_gpu_init(void)
-{
-    NV_STATUS status;
+NV_STATUS uvm_gpu_init(void) {
+  NV_STATUS status;
 
-    uvm_param_conf();
+  uvm_param_conf();
 
-    status = uvm_hal_init_table();
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("uvm_hal_init_table() failed: %s\n", nvstatusToString(status));
-        return status;
-    }
+  status = uvm_hal_init_table();
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("uvm_hal_init_table() failed: %s\n",
+                  nvstatusToString(status));
+    return status;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-void uvm_gpu_exit(void)
-{
-    uvm_parent_gpu_t *parent_gpu;
+void uvm_gpu_exit(void) {
+  uvm_parent_gpu_t *parent_gpu;
 
-    for_each_parent_gpu(parent_gpu)
-        UVM_ASSERT_MSG(false, "GPU still present: %s\n", uvm_parent_gpu_name(parent_gpu));
+  for_each_parent_gpu(parent_gpu) UVM_ASSERT_MSG(
+      false, "GPU still present: %s\n", uvm_parent_gpu_name(parent_gpu));
 
-    // CPU should never be in the retained GPUs mask
-    UVM_ASSERT(!uvm_processor_mask_test(&g_uvm_global.retained_gpus, UVM_ID_CPU));
+  // CPU should never be in the retained GPUs mask
+  UVM_ASSERT(!uvm_processor_mask_test(&g_uvm_global.retained_gpus, UVM_ID_CPU));
 }
 
-NV_STATUS uvm_gpu_init_va_space(uvm_va_space_t *va_space)
-{
-    NV_STATUS status;
+NV_STATUS uvm_gpu_init_va_space(uvm_va_space_t *va_space) {
+  NV_STATUS status;
 
-    if (uvm_procfs_is_debug_enabled()) {
-        status = uvm_perf_register_event_callback(&va_space->perf_events,
-                                                  UVM_PERF_EVENT_FAULT,
-                                                  update_stats_fault_cb);
-        if (status != NV_OK)
-            return status;
+  if (uvm_procfs_is_debug_enabled()) {
+    status = uvm_perf_register_event_callback(
+        &va_space->perf_events, UVM_PERF_EVENT_FAULT, update_stats_fault_cb);
+    if (status != NV_OK)
+      return status;
 
-        status = uvm_perf_register_event_callback(&va_space->perf_events,
-                                                  UVM_PERF_EVENT_MIGRATION,
-                                                  update_stats_migration_cb);
-        if (status != NV_OK)
-            return status;
-    }
+    status = uvm_perf_register_event_callback(&va_space->perf_events,
+                                              UVM_PERF_EVENT_MIGRATION,
+                                              update_stats_migration_cb);
+    if (status != NV_OK)
+      return status;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-uvm_parent_gpu_t *uvm_parent_gpu_get_by_uuid_locked(const NvProcessorUuid *gpu_uuid)
-{
-    uvm_parent_gpu_t *parent_gpu;
+uvm_parent_gpu_t *
+uvm_parent_gpu_get_by_uuid_locked(const NvProcessorUuid *gpu_uuid) {
+  uvm_parent_gpu_t *parent_gpu;
 
-    for_each_parent_gpu(parent_gpu) {
-        if (uvm_uuid_eq(&parent_gpu->uuid, gpu_uuid))
-            return parent_gpu;
-    }
+  for_each_parent_gpu(parent_gpu) {
+    if (uvm_uuid_eq(&parent_gpu->uuid, gpu_uuid))
+      return parent_gpu;
+  }
 
-    return NULL;
+  return NULL;
 }
 
-uvm_parent_gpu_t *uvm_parent_gpu_get_by_uuid(const NvProcessorUuid *gpu_uuid)
-{
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+uvm_parent_gpu_t *uvm_parent_gpu_get_by_uuid(const NvProcessorUuid *gpu_uuid) {
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    return uvm_parent_gpu_get_by_uuid_locked(gpu_uuid);
+  return uvm_parent_gpu_get_by_uuid_locked(gpu_uuid);
 }
 
-uvm_gpu_t *uvm_gpu_get_by_uuid(const NvProcessorUuid *gpu_uuid)
-{
-    uvm_gpu_id_t gpu_id;
+uvm_gpu_t *uvm_gpu_get_by_uuid(const NvProcessorUuid *gpu_uuid) {
+  uvm_gpu_id_t gpu_id;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    for_each_gpu_id(gpu_id) {
-        uvm_gpu_t *gpu = uvm_gpu_get(gpu_id);
+  for_each_gpu_id(gpu_id) {
+    uvm_gpu_t *gpu = uvm_gpu_get(gpu_id);
 
-        if (gpu) {
-            if (uvm_uuid_eq(&gpu->uuid, gpu_uuid))
-                return gpu;
-        }
+    if (gpu) {
+      if (uvm_uuid_eq(&gpu->uuid, gpu_uuid))
+        return gpu;
     }
+  }
 
-    return NULL;
+  return NULL;
 }
 
-static uvm_gpu_t *uvm_gpu_get_by_parent_and_swizz_id(uvm_parent_gpu_t *parent_gpu, NvU32 swizz_id)
-{
-    uvm_gpu_t *gpu;
+static uvm_gpu_t *
+uvm_gpu_get_by_parent_and_swizz_id(uvm_parent_gpu_t *parent_gpu,
+                                   NvU32 swizz_id) {
+  uvm_gpu_t *gpu;
 
-    UVM_ASSERT(parent_gpu);
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  UVM_ASSERT(parent_gpu);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    for_each_gpu_in_parent(gpu, parent_gpu) {
-        if (gpu->smc.swizz_id == swizz_id)
-            return gpu;
-    }
+  for_each_gpu_in_parent(gpu, parent_gpu) {
+    if (gpu->smc.swizz_id == swizz_id)
+      return gpu;
+  }
 
-    return NULL;
+  return NULL;
 }
 
-void uvm_gpu_retain(uvm_gpu_t *gpu)
-{
-    UVM_ASSERT(uvm_gpu_retained_count(gpu) > 0);
-    atomic64_inc(&gpu->retained_count);
+void uvm_gpu_retain(uvm_gpu_t *gpu) {
+  UVM_ASSERT(uvm_gpu_retained_count(gpu) > 0);
+  atomic64_inc(&gpu->retained_count);
 }
 
-bool uvm_parent_gpus_are_nvswitch_connected(const uvm_parent_gpu_t *parent_gpu0, const uvm_parent_gpu_t *parent_gpu1)
-{
-    if (parent_gpu0 != parent_gpu1 &&
-        parent_gpu0->nvswitch_info.is_nvswitch_connected &&
-        parent_gpu1->nvswitch_info.is_nvswitch_connected) {
-        UVM_ASSERT(parent_gpu_peer_caps(parent_gpu0, parent_gpu1)->link_type >= UVM_GPU_LINK_NVLINK_2);
-        return true;
-    }
+bool uvm_parent_gpus_are_nvswitch_connected(
+    const uvm_parent_gpu_t *parent_gpu0, const uvm_parent_gpu_t *parent_gpu1) {
+  if (parent_gpu0 != parent_gpu1 &&
+      parent_gpu0->nvswitch_info.is_nvswitch_connected &&
+      parent_gpu1->nvswitch_info.is_nvswitch_connected) {
+    UVM_ASSERT(parent_gpu_peer_caps(parent_gpu0, parent_gpu1)->link_type >=
+               UVM_GPU_LINK_NVLINK_2);
+    return true;
+  }
 
-    return false;
+  return false;
 }
 
-bool uvm_parent_gpus_are_bar1_peers(const uvm_parent_gpu_t *parent_gpu0, const uvm_parent_gpu_t *parent_gpu1)
-{
-    if (parent_gpu0 != parent_gpu1)
-        return parent_gpu_peer_caps(parent_gpu0, parent_gpu1)->link_type == UVM_GPU_LINK_PCIE_BAR1;
+bool uvm_parent_gpus_are_bar1_peers(const uvm_parent_gpu_t *parent_gpu0,
+                                    const uvm_parent_gpu_t *parent_gpu1) {
+  if (parent_gpu0 != parent_gpu1)
+    return parent_gpu_peer_caps(parent_gpu0, parent_gpu1)->link_type ==
+           UVM_GPU_LINK_PCIE_BAR1;
 
-    return false;
+  return false;
 }
 
-bool uvm_parent_gpus_are_nvlink_direct_connected(const uvm_parent_gpu_t *parent_gpu0, const uvm_parent_gpu_t *parent_gpu1)
-{
-    if (parent_gpu0 != parent_gpu1 &&
-        parent_gpu0->peer_address_info.is_nvlink_direct_connected &&
-        parent_gpu1->peer_address_info.is_nvlink_direct_connected)
-        return true;
+bool uvm_parent_gpus_are_nvlink_direct_connected(
+    const uvm_parent_gpu_t *parent_gpu0, const uvm_parent_gpu_t *parent_gpu1) {
+  if (parent_gpu0 != parent_gpu1 &&
+      parent_gpu0->peer_address_info.is_nvlink_direct_connected &&
+      parent_gpu1->peer_address_info.is_nvlink_direct_connected)
+    return true;
 
-    return false;
+  return false;
 }
 
-NV_STATUS uvm_gpu_check_ecc_error_no_rm(uvm_gpu_t *gpu)
-{
-    // We may need to call service_interrupts() which cannot be done in the top
-    // half interrupt handler so assert here as well to catch improper use as
-    // early as possible.
-    UVM_ASSERT(!in_interrupt());
-
-    if (!gpu->ecc.enabled)
-        return NV_OK;
-
-    // Early out If a global ECC error is already set to not spam the logs with
-    // the same error.
-    if (uvm_global_get_status() == NV_ERR_ECC_ERROR)
-        return NV_ERR_ECC_ERROR;
+NV_STATUS uvm_gpu_check_ecc_error_no_rm(uvm_gpu_t *gpu) {
+  // We may need to call service_interrupts() which cannot be done in the top
+  // half interrupt handler so assert here as well to catch improper use as
+  // early as possible.
+  UVM_ASSERT(!in_interrupt());
 
-    if (*gpu->ecc.error_notifier) {
-        UVM_ERR_PRINT("ECC error encountered, GPU %s\n", uvm_gpu_name(gpu));
-        uvm_global_set_fatal_error(NV_ERR_ECC_ERROR);
-        return NV_ERR_ECC_ERROR;
-    }
+  if (!gpu->ecc.enabled)
+    return NV_OK;
 
-    // RM hasn't seen an ECC error yet, check whether there is a pending
-    // interrupt that might indicate one. We might get false positives because
-    // the interrupt bits we read are not ECC-specific. They're just the
-    // top-level bits for any interrupt on all engines which support ECC. On
-    // Pascal for example, RM returns us a mask with the bits for GR, L2, and
-    // FB, because any of those might raise an ECC interrupt. So if they're set
-    // we have to ask RM to check whether it was really an ECC error (and a
-    // double-bit ECC error at that), in which case it sets the notifier.
-    if ((*gpu->ecc.hw_interrupt_tree_location & gpu->ecc.mask) == 0) {
-        // No pending interrupts.
-        return NV_OK;
-    }
+  // Early out If a global ECC error is already set to not spam the logs with
+  // the same error.
+  if (uvm_global_get_status() == NV_ERR_ECC_ERROR)
+    return NV_ERR_ECC_ERROR;
+
+  if (*gpu->ecc.error_notifier) {
+    UVM_ERR_PRINT("ECC error encountered, GPU %s\n", uvm_gpu_name(gpu));
+    uvm_global_set_fatal_error(NV_ERR_ECC_ERROR);
+    return NV_ERR_ECC_ERROR;
+  }
+
+  // RM hasn't seen an ECC error yet, check whether there is a pending
+  // interrupt that might indicate one. We might get false positives because
+  // the interrupt bits we read are not ECC-specific. They're just the
+  // top-level bits for any interrupt on all engines which support ECC. On
+  // Pascal for example, RM returns us a mask with the bits for GR, L2, and
+  // FB, because any of those might raise an ECC interrupt. So if they're set
+  // we have to ask RM to check whether it was really an ECC error (and a
+  // double-bit ECC error at that), in which case it sets the notifier.
+  if ((*gpu->ecc.hw_interrupt_tree_location & gpu->ecc.mask) == 0) {
+    // No pending interrupts.
+    return NV_OK;
+  }
 
-    // An interrupt that might mean an ECC error needs to be serviced, signal
-    // that to the caller.
-    return NV_WARN_MORE_PROCESSING_REQUIRED;
+  // An interrupt that might mean an ECC error needs to be serviced, signal
+  // that to the caller.
+  return NV_WARN_MORE_PROCESSING_REQUIRED;
 }
 
-NV_STATUS uvm_gpu_get_injected_nvlink_error(uvm_gpu_t *gpu)
-{
-    if (uvm_enable_builtin_tests) {
-        UVM_TEST_NVLINK_ERROR_TYPE error_type = atomic_read(&gpu->nvlink_status.injected_error);
+NV_STATUS uvm_gpu_get_injected_nvlink_error(uvm_gpu_t *gpu) {
+  if (uvm_enable_builtin_tests) {
+    UVM_TEST_NVLINK_ERROR_TYPE error_type =
+        atomic_read(&gpu->nvlink_status.injected_error);
 
-        if (error_type == UVM_TEST_NVLINK_ERROR_UNRESOLVED)
-            return NV_WARN_MORE_PROCESSING_REQUIRED;
+    if (error_type == UVM_TEST_NVLINK_ERROR_UNRESOLVED)
+      return NV_WARN_MORE_PROCESSING_REQUIRED;
 
-        if (error_type == UVM_TEST_NVLINK_ERROR_RESOLVED)
-            return NV_ERR_MEMORY_ERROR;
-    }
+    if (error_type == UVM_TEST_NVLINK_ERROR_RESOLVED)
+      return NV_ERR_MEMORY_ERROR;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-NV_STATUS uvm_gpu_check_nvlink_error_no_rm(uvm_gpu_t *gpu)
-{
-    NV_STATUS status;
-
-    // We may need to call service_interrupts() which cannot be done in the top
-    // half interrupt handler so assert here as well to catch improper use as
-    // early as possible.
-    UVM_ASSERT(!in_interrupt());
+NV_STATUS uvm_gpu_check_nvlink_error_no_rm(uvm_gpu_t *gpu) {
+  NV_STATUS status;
 
-    status = uvm_gpu_get_injected_nvlink_error(gpu);
-    if (status != NV_OK)
-        return status;
+  // We may need to call service_interrupts() which cannot be done in the top
+  // half interrupt handler so assert here as well to catch improper use as
+  // early as possible.
+  UVM_ASSERT(!in_interrupt());
 
-    if (!gpu->nvlink_status.enabled)
-        return NV_OK;
+  status = uvm_gpu_get_injected_nvlink_error(gpu);
+  if (status != NV_OK)
+    return status;
 
-    if (*gpu->nvlink_status.error_notifier) {
-        UVM_ERR_PRINT("NVLINK error encountered, GPU %s\n", uvm_gpu_name(gpu));
-        return NV_ERR_MEMORY_ERROR;
-    }
+  if (!gpu->nvlink_status.enabled)
+    return NV_OK;
 
-    // RM hasn't seen an NVLINK error yet, check whether there is a pending
-    // interrupt that might indicate one. We might get false positives because
-    // the interrupt bits we read are not NVLINK-specific. They're just the
-    // top-level bits for any interrupt on all engines which support NVLINK
-    // errors.
-    if ((*gpu->nvlink_status.hw_interrupt_tree_location & gpu->nvlink_status.mask) == 0) {
-        // No pending interrupts.
-        return NV_OK;
-    }
+  if (*gpu->nvlink_status.error_notifier) {
+    UVM_ERR_PRINT("NVLINK error encountered, GPU %s\n", uvm_gpu_name(gpu));
+    return NV_ERR_MEMORY_ERROR;
+  }
+
+  // RM hasn't seen an NVLINK error yet, check whether there is a pending
+  // interrupt that might indicate one. We might get false positives because
+  // the interrupt bits we read are not NVLINK-specific. They're just the
+  // top-level bits for any interrupt on all engines which support NVLINK
+  // errors.
+  if ((*gpu->nvlink_status.hw_interrupt_tree_location &
+       gpu->nvlink_status.mask) == 0) {
+    // No pending interrupts.
+    return NV_OK;
+  }
 
-    // An interrupt that might mean an NVLINK error needs to be serviced, signal
-    // that to the caller.
-    return NV_WARN_MORE_PROCESSING_REQUIRED;
+  // An interrupt that might mean an NVLINK error needs to be serviced, signal
+  // that to the caller.
+  return NV_WARN_MORE_PROCESSING_REQUIRED;
 }
 static NV_STATUS get_parent_p2p_caps(uvm_parent_gpu_t *parent_gpu0,
                                      uvm_parent_gpu_t *parent_gpu1,
-                                     UvmGpuP2PCapsParams *p2p_caps_params)
-{
-    NV_STATUS status;
-    uvmGpuDeviceHandle rm_device0, rm_device1;
-
-    if (uvm_parent_id_value(parent_gpu0->id) < uvm_parent_id_value(parent_gpu1->id)) {
-        rm_device0 = parent_gpu0->rm_device;
-        rm_device1 = parent_gpu1->rm_device;
-    }
-    else {
-        rm_device0 = parent_gpu1->rm_device;
-        rm_device1 = parent_gpu0->rm_device;
-    }
-
-    memset(p2p_caps_params, 0, sizeof(*p2p_caps_params));
-    status = uvm_rm_locked_call(nvUvmInterfaceGetP2PCaps(rm_device0, rm_device1, p2p_caps_params));
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("nvUvmInterfaceGetP2PCaps() failed with error: %s, for GPU0:%s and GPU1:%s\n",
-                       nvstatusToString(status),
-                       uvm_parent_gpu_name(parent_gpu0),
-                       uvm_parent_gpu_name(parent_gpu1));
-        return status;
-    }
+                                     UvmGpuP2PCapsParams *p2p_caps_params) {
+  NV_STATUS status;
+  uvmGpuDeviceHandle rm_device0, rm_device1;
+
+  if (uvm_parent_id_value(parent_gpu0->id) <
+      uvm_parent_id_value(parent_gpu1->id)) {
+    rm_device0 = parent_gpu0->rm_device;
+    rm_device1 = parent_gpu1->rm_device;
+  } else {
+    rm_device0 = parent_gpu1->rm_device;
+    rm_device1 = parent_gpu0->rm_device;
+  }
+
+  memset(p2p_caps_params, 0, sizeof(*p2p_caps_params));
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetP2PCaps(rm_device0, rm_device1, p2p_caps_params));
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("nvUvmInterfaceGetP2PCaps() failed with error: %s, for "
+                  "GPU0:%s and GPU1:%s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu0),
+                  uvm_parent_gpu_name(parent_gpu1));
+    return status;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
 static NV_STATUS create_parent_p2p_object(uvm_parent_gpu_t *parent_gpu0,
                                           uvm_parent_gpu_t *parent_gpu1,
-                                          NvHandle *p2p_handle)
-{
-    NV_STATUS status;
-    uvmGpuDeviceHandle rm_device0, rm_device1;
-
-    if (uvm_parent_id_value(parent_gpu0->id) < uvm_parent_id_value(parent_gpu1->id)) {
-        rm_device0 = parent_gpu0->rm_device;
-        rm_device1 = parent_gpu1->rm_device;
-    }
-    else {
-        rm_device0 = parent_gpu1->rm_device;
-        rm_device1 = parent_gpu0->rm_device;
-    }
-
-    *p2p_handle = 0;
-
-    status = uvm_rm_locked_call(nvUvmInterfaceP2pObjectCreate(rm_device0, rm_device1, p2p_handle));
-    if (status != NV_OK) {
-        UVM_ERR_PRINT("nvUvmInterfaceP2pObjectCreate() failed with error: %s, for GPU0:%s and GPU1:%s\n",
-                       nvstatusToString(status),
-                       uvm_parent_gpu_name(parent_gpu0),
-                       uvm_parent_gpu_name(parent_gpu1));
-        return status;
-    }
+                                          NvHandle *p2p_handle) {
+  NV_STATUS status;
+  uvmGpuDeviceHandle rm_device0, rm_device1;
+
+  if (uvm_parent_id_value(parent_gpu0->id) <
+      uvm_parent_id_value(parent_gpu1->id)) {
+    rm_device0 = parent_gpu0->rm_device;
+    rm_device1 = parent_gpu1->rm_device;
+  } else {
+    rm_device0 = parent_gpu1->rm_device;
+    rm_device1 = parent_gpu0->rm_device;
+  }
+
+  *p2p_handle = 0;
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceP2pObjectCreate(rm_device0, rm_device1, p2p_handle));
+  if (status != NV_OK) {
+    UVM_ERR_PRINT("nvUvmInterfaceP2pObjectCreate() failed with error: %s, for "
+                  "GPU0:%s and GPU1:%s\n",
+                  nvstatusToString(status), uvm_parent_gpu_name(parent_gpu0),
+                  uvm_parent_gpu_name(parent_gpu1));
+    return status;
+  }
 
-    UVM_ASSERT(*p2p_handle);
-    return NV_OK;
+  UVM_ASSERT(*p2p_handle);
+  return NV_OK;
 }
 
-static void set_optimal_p2p_write_ces(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps = parent_gpu_peer_caps(gpu0->parent, gpu1->parent);
-    bool sorted;
-    NvU32 ce0, ce1;
+static void set_optimal_p2p_write_ces(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1) {
+  uvm_parent_gpu_peer_t *parent_peer_caps =
+      parent_gpu_peer_caps(gpu0->parent, gpu1->parent);
+  bool sorted;
+  NvU32 ce0, ce1;
 
-    UVM_ASSERT(parent_peer_caps->ref_count);
-    UVM_ASSERT(gpu0->parent->peer_copy_mode == gpu1->parent->peer_copy_mode);
+  UVM_ASSERT(parent_peer_caps->ref_count);
+  UVM_ASSERT(gpu0->parent->peer_copy_mode == gpu1->parent->peer_copy_mode);
 
-    if (gpu0->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_UNSUPPORTED)
-        return;
+  if (gpu0->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_UNSUPPORTED)
+    return;
 
-    if (parent_peer_caps->link_type < UVM_GPU_LINK_NVLINK_1)
-        return;
+  if (parent_peer_caps->link_type < UVM_GPU_LINK_NVLINK_1)
+    return;
 
-    sorted = uvm_id_value(gpu0->id) < uvm_id_value(gpu1->id);
-    ce0 = parent_peer_caps->optimalNvlinkWriteCEs[sorted ? 0 : 1];
-    ce1 = parent_peer_caps->optimalNvlinkWriteCEs[sorted ? 1 : 0];
+  sorted = uvm_id_value(gpu0->id) < uvm_id_value(gpu1->id);
+  ce0 = parent_peer_caps->optimalNvlinkWriteCEs[sorted ? 0 : 1];
+  ce1 = parent_peer_caps->optimalNvlinkWriteCEs[sorted ? 1 : 0];
 
-    uvm_channel_manager_set_p2p_ce(gpu0->channel_manager, gpu1, ce0);
-    uvm_channel_manager_set_p2p_ce(gpu1->channel_manager, gpu0, ce1);
+  uvm_channel_manager_set_p2p_ce(gpu0->channel_manager, gpu1, ce0);
+  uvm_channel_manager_set_p2p_ce(gpu1->channel_manager, gpu0, ce1);
 }
 
-static int nv_procfs_read_parent_gpu_peer_caps(struct seq_file *s, void *v)
-{
-    if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
-        return -EAGAIN;
+static int nv_procfs_read_parent_gpu_peer_caps(struct seq_file *s, void *v) {
+  if (!uvm_down_read_trylock(&g_uvm_global.pm.lock))
+    return -EAGAIN;
 
-    parent_gpu_peer_caps_print((uvm_parent_gpu_t **)s->private, s);
+  parent_gpu_peer_caps_print((uvm_parent_gpu_t **)s->private, s);
 
-    uvm_up_read(&g_uvm_global.pm.lock);
+  uvm_up_read(&g_uvm_global.pm.lock);
 
-    return 0;
+  return 0;
 }
 
-static int nv_procfs_read_parent_gpu_peer_caps_entry(struct seq_file *s, void *v)
-{
-    UVM_ENTRY_RET(nv_procfs_read_parent_gpu_peer_caps(s, v));
+static int nv_procfs_read_parent_gpu_peer_caps_entry(struct seq_file *s,
+                                                     void *v) {
+  UVM_ENTRY_RET(nv_procfs_read_parent_gpu_peer_caps(s, v));
 }
 
 UVM_DEFINE_SINGLE_PROCFS_FILE(parent_gpu_peer_caps_entry);
 
-static NV_STATUS init_procfs_parent_peer_cap_files(uvm_parent_gpu_t *local, uvm_parent_gpu_t *remote, size_t local_idx)
-{
-    // This needs to hold a uvm_parent_gpu_id_t in decimal
-    char gpu_dir_name[16];
+static NV_STATUS init_procfs_parent_peer_cap_files(uvm_parent_gpu_t *local,
+                                                   uvm_parent_gpu_t *remote,
+                                                   size_t local_idx) {
+  // This needs to hold a uvm_parent_gpu_id_t in decimal
+  char gpu_dir_name[16];
 
-    // This needs to hold a GPU UUID
-    char symlink_name[UVM_GPU_UUID_STRING_LENGTH];
-    uvm_parent_gpu_peer_t *parent_peer_caps;
-
-    UVM_ASSERT(uvm_procfs_is_debug_enabled());
-
-    parent_peer_caps = parent_gpu_peer_caps(local, remote);
-    parent_peer_caps->procfs.pairs[local_idx][0] = local;
-    parent_peer_caps->procfs.pairs[local_idx][1] = remote;
-
-    // Create gpus/gpuA/peers/gpuB
-    snprintf(gpu_dir_name, sizeof(gpu_dir_name), "%u", uvm_parent_id_value(remote->id));
-    parent_peer_caps->procfs.peer_file[local_idx] = NV_CREATE_PROC_FILE(gpu_dir_name,
-                                                                        local->procfs.dir_peers,
-                                                                        parent_gpu_peer_caps_entry,
-                                                                        &parent_peer_caps->procfs.pairs[local_idx]);
-
-    if (parent_peer_caps->procfs.peer_file[local_idx] == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
-
-    // Create a symlink from UVM GPU UUID (GPU-...) to the UVM GPU ID gpuB
-    uvm_parent_gpu_uuid_string(symlink_name, &remote->uuid);
-    parent_peer_caps->procfs.peer_symlink_file[local_idx] = proc_symlink(symlink_name,
-                                                                         local->procfs.dir_peers,
-                                                                         gpu_dir_name);
-    if (parent_peer_caps->procfs.peer_symlink_file[local_idx] == NULL)
-        return NV_ERR_OPERATING_SYSTEM;
-
-    return NV_OK;
-}
-
-static NV_STATUS init_procfs_parent_peer_files(uvm_parent_gpu_t *parent_gpu0, uvm_parent_gpu_t *parent_gpu1)
-{
-    NV_STATUS status;
-
-    if (!uvm_procfs_is_debug_enabled())
-        return NV_OK;
-
-    status = init_procfs_parent_peer_cap_files(parent_gpu0, parent_gpu1, 0);
-    if (status != NV_OK)
-        return status;
-
-    status = init_procfs_parent_peer_cap_files(parent_gpu1, parent_gpu0, 1);
-    if (status != NV_OK)
-        return status;
-
-    return NV_OK;
-}
-
-static NV_STATUS parent_peers_init(uvm_parent_gpu_t *parent_gpu0,
-                                   uvm_parent_gpu_t *parent_gpu1,
-                                   uvm_parent_gpu_peer_t *parent_peer_caps)
-{
-    UvmGpuP2PCapsParams p2p_caps_params;
-    uvm_gpu_link_type_t link_type;
-    NvHandle p2p_handle;
-    NV_STATUS status;
-
-    UVM_ASSERT(parent_peer_caps->ref_count == 0);
-
-    status = create_parent_p2p_object(parent_gpu0, parent_gpu1, &p2p_handle);
-    if (status != NV_OK)
-        return status;
-
-    status = get_parent_p2p_caps(parent_gpu0, parent_gpu1, &p2p_caps_params);
-    if (status != NV_OK)
-        goto cleanup;
-
-    // check for peer-to-peer compatibility (PCI-E or NvLink).
-    link_type = get_gpu_link_type(p2p_caps_params.p2pLink);
-    if (link_type == UVM_GPU_LINK_INVALID || link_type == UVM_GPU_LINK_C2C) {
-        status = NV_ERR_NOT_SUPPORTED;
-        goto cleanup;
-    }
-
-    status = init_procfs_parent_peer_files(parent_gpu0, parent_gpu1);
-    if (status != NV_OK)
-        goto cleanup;
+  // This needs to hold a GPU UUID
+  char symlink_name[UVM_GPU_UUID_STRING_LENGTH];
+  uvm_parent_gpu_peer_t *parent_peer_caps;
 
-    parent_peer_caps->ref_count = 1;
-    parent_peer_caps->p2p_handle = p2p_handle;
-    parent_peer_caps->link_type = link_type;
-    parent_peer_caps->total_link_line_rate_mbyte_per_s = p2p_caps_params.totalLinkLineRateMBps;
+  UVM_ASSERT(uvm_procfs_is_debug_enabled());
 
-    if (parent_gpu0->egm.enabled || parent_gpu1->egm.enabled)
-        UVM_ASSERT(parent_peer_caps->link_type >= UVM_GPU_LINK_NVLINK_2);
+  parent_peer_caps = parent_gpu_peer_caps(local, remote);
+  parent_peer_caps->procfs.pairs[local_idx][0] = local;
+  parent_peer_caps->procfs.pairs[local_idx][1] = remote;
 
-    // Initialize peer ids and establish peer mappings
-    // Peer id from min(gpu_id0, gpu_id1) -> max(gpu_id0, gpu_id1)
-    parent_peer_caps->peer_ids[0] = p2p_caps_params.peerIds[0];
-    parent_peer_caps->egm_peer_ids[0] = p2p_caps_params.egmPeerIds[0];
+  // Create gpus/gpuA/peers/gpuB
+  snprintf(gpu_dir_name, sizeof(gpu_dir_name), "%u",
+           uvm_parent_id_value(remote->id));
+  parent_peer_caps->procfs.peer_file[local_idx] = NV_CREATE_PROC_FILE(
+      gpu_dir_name, local->procfs.dir_peers, parent_gpu_peer_caps_entry,
+      &parent_peer_caps->procfs.pairs[local_idx]);
 
-    // Peer id from max(gpu_id0, gpu_id1) -> min(gpu_id0, gpu_id1)
-    parent_peer_caps->peer_ids[1] = p2p_caps_params.peerIds[1];
-    parent_peer_caps->egm_peer_ids[1] = p2p_caps_params.egmPeerIds[1];
+  if (parent_peer_caps->procfs.peer_file[local_idx] == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    parent_peer_caps->optimalNvlinkWriteCEs[0] = p2p_caps_params.optimalNvlinkWriteCEs[0];
-    parent_peer_caps->optimalNvlinkWriteCEs[1] = p2p_caps_params.optimalNvlinkWriteCEs[1];
+  // Create a symlink from UVM GPU UUID (GPU-...) to the UVM GPU ID gpuB
+  uvm_parent_gpu_uuid_string(symlink_name, &remote->uuid);
+  parent_peer_caps->procfs.peer_symlink_file[local_idx] =
+      proc_symlink(symlink_name, local->procfs.dir_peers, gpu_dir_name);
+  if (parent_peer_caps->procfs.peer_symlink_file[local_idx] == NULL)
+    return NV_ERR_OPERATING_SYSTEM;
 
-    // Set IOMMU/DMA mappings for bar1 p2p
-    parent_peer_caps->bar1_p2p_dma_base_address[0] = p2p_caps_params.bar1DmaAddress[0];
-    parent_peer_caps->bar1_p2p_dma_base_address[1] = p2p_caps_params.bar1DmaAddress[1];
-    parent_peer_caps->bar1_p2p_dma_size[0] = p2p_caps_params.bar1DmaSize[0];
-    parent_peer_caps->bar1_p2p_dma_size[1] = p2p_caps_params.bar1DmaSize[1];
-    parent_peer_caps->bar1_p2p_pcie_atomics_enabled[0] = p2p_caps_params.bar1PcieAtomics[0];
-    parent_peer_caps->bar1_p2p_pcie_atomics_enabled[1] = p2p_caps_params.bar1PcieAtomics[1];
+  return NV_OK;
+}
 
-    if (parent_peer_caps->bar1_p2p_dma_size[0] || parent_peer_caps->bar1_p2p_dma_size[1])
-        UVM_ASSERT(link_type == UVM_GPU_LINK_PCIE_BAR1);
+static NV_STATUS init_procfs_parent_peer_files(uvm_parent_gpu_t *parent_gpu0,
+                                               uvm_parent_gpu_t *parent_gpu1) {
+  NV_STATUS status;
 
+  if (!uvm_procfs_is_debug_enabled())
     return NV_OK;
 
-cleanup:
-    uvm_rm_locked_call_void(nvUvmInterfaceP2pObjectDestroy(uvm_global_session_handle(), p2p_handle));
+  status = init_procfs_parent_peer_cap_files(parent_gpu0, parent_gpu1, 0);
+  if (status != NV_OK)
+    return status;
 
+  status = init_procfs_parent_peer_cap_files(parent_gpu1, parent_gpu0, 1);
+  if (status != NV_OK)
     return status;
+
+  return NV_OK;
 }
 
-static NV_STATUS parent_peers_retain(uvm_parent_gpu_t *parent_gpu0, uvm_parent_gpu_t *parent_gpu1)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps = parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
-    NV_STATUS status = NV_OK;
+static NV_STATUS parent_peers_init(uvm_parent_gpu_t *parent_gpu0,
+                                   uvm_parent_gpu_t *parent_gpu1,
+                                   uvm_parent_gpu_peer_t *parent_peer_caps) {
+  UvmGpuP2PCapsParams p2p_caps_params;
+  uvm_gpu_link_type_t link_type;
+  NvHandle p2p_handle;
+  NV_STATUS status;
 
-    if (parent_peer_caps->ref_count == 0)
-        status = parent_peers_init(parent_gpu0, parent_gpu1, parent_peer_caps);
-    else
-        parent_peer_caps->ref_count++;
+  UVM_ASSERT(parent_peer_caps->ref_count == 0);
 
+  status = create_parent_p2p_object(parent_gpu0, parent_gpu1, &p2p_handle);
+  if (status != NV_OK)
     return status;
+
+  status = get_parent_p2p_caps(parent_gpu0, parent_gpu1, &p2p_caps_params);
+  if (status != NV_OK)
+    goto cleanup;
+
+  // check for peer-to-peer compatibility (PCI-E or NvLink).
+  link_type = get_gpu_link_type(p2p_caps_params.p2pLink);
+  if (link_type == UVM_GPU_LINK_INVALID || link_type == UVM_GPU_LINK_C2C) {
+    status = NV_ERR_NOT_SUPPORTED;
+    goto cleanup;
+  }
+
+  status = init_procfs_parent_peer_files(parent_gpu0, parent_gpu1);
+  if (status != NV_OK)
+    goto cleanup;
+
+  parent_peer_caps->ref_count = 1;
+  parent_peer_caps->p2p_handle = p2p_handle;
+  parent_peer_caps->link_type = link_type;
+  parent_peer_caps->total_link_line_rate_mbyte_per_s =
+      p2p_caps_params.totalLinkLineRateMBps;
+
+  if (parent_gpu0->egm.enabled || parent_gpu1->egm.enabled)
+    UVM_ASSERT(parent_peer_caps->link_type >= UVM_GPU_LINK_NVLINK_2);
+
+  // Initialize peer ids and establish peer mappings
+  // Peer id from min(gpu_id0, gpu_id1) -> max(gpu_id0, gpu_id1)
+  parent_peer_caps->peer_ids[0] = p2p_caps_params.peerIds[0];
+  parent_peer_caps->egm_peer_ids[0] = p2p_caps_params.egmPeerIds[0];
+
+  // Peer id from max(gpu_id0, gpu_id1) -> min(gpu_id0, gpu_id1)
+  parent_peer_caps->peer_ids[1] = p2p_caps_params.peerIds[1];
+  parent_peer_caps->egm_peer_ids[1] = p2p_caps_params.egmPeerIds[1];
+
+  parent_peer_caps->optimalNvlinkWriteCEs[0] =
+      p2p_caps_params.optimalNvlinkWriteCEs[0];
+  parent_peer_caps->optimalNvlinkWriteCEs[1] =
+      p2p_caps_params.optimalNvlinkWriteCEs[1];
+
+  // Set IOMMU/DMA mappings for bar1 p2p
+  parent_peer_caps->bar1_p2p_dma_base_address[0] =
+      p2p_caps_params.bar1DmaAddress[0];
+  parent_peer_caps->bar1_p2p_dma_base_address[1] =
+      p2p_caps_params.bar1DmaAddress[1];
+  parent_peer_caps->bar1_p2p_dma_size[0] = p2p_caps_params.bar1DmaSize[0];
+  parent_peer_caps->bar1_p2p_dma_size[1] = p2p_caps_params.bar1DmaSize[1];
+  parent_peer_caps->bar1_p2p_pcie_atomics_enabled[0] =
+      p2p_caps_params.bar1PcieAtomics[0];
+  parent_peer_caps->bar1_p2p_pcie_atomics_enabled[1] =
+      p2p_caps_params.bar1PcieAtomics[1];
+
+  if (parent_peer_caps->bar1_p2p_dma_size[0] ||
+      parent_peer_caps->bar1_p2p_dma_size[1])
+    UVM_ASSERT(link_type == UVM_GPU_LINK_PCIE_BAR1);
+
+  return NV_OK;
+
+cleanup:
+  uvm_rm_locked_call_void(
+      nvUvmInterfaceP2pObjectDestroy(uvm_global_session_handle(), p2p_handle));
+
+  return status;
+}
+
+static NV_STATUS parent_peers_retain(uvm_parent_gpu_t *parent_gpu0,
+                                     uvm_parent_gpu_t *parent_gpu1) {
+  uvm_parent_gpu_peer_t *parent_peer_caps =
+      parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
+  NV_STATUS status = NV_OK;
+
+  if (parent_peer_caps->ref_count == 0)
+    status = parent_peers_init(parent_gpu0, parent_gpu1, parent_peer_caps);
+  else
+    parent_peer_caps->ref_count++;
+
+  return status;
 }
 
 static void parent_peers_destroy(uvm_parent_gpu_t *parent_gpu0,
                                  uvm_parent_gpu_t *parent_gpu1,
-                                 uvm_parent_gpu_peer_t *parent_peer_caps)
-{
-    UVM_ASSERT(parent_peer_caps->p2p_handle);
+                                 uvm_parent_gpu_peer_t *parent_peer_caps) {
+  UVM_ASSERT(parent_peer_caps->p2p_handle);
 
-    deinit_procfs_parent_peer_cap_files(parent_peer_caps);
+  deinit_procfs_parent_peer_cap_files(parent_peer_caps);
 
-    uvm_rm_locked_call_void(nvUvmInterfaceP2pObjectDestroy(uvm_global_session_handle(), parent_peer_caps->p2p_handle));
+  uvm_rm_locked_call_void(nvUvmInterfaceP2pObjectDestroy(
+      uvm_global_session_handle(), parent_peer_caps->p2p_handle));
 
-    memset(parent_peer_caps, 0, sizeof(*parent_peer_caps));
+  memset(parent_peer_caps, 0, sizeof(*parent_peer_caps));
 }
 
-static void parent_peers_release(uvm_parent_gpu_t *parent_gpu0, uvm_parent_gpu_t *parent_gpu1)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps = parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
+static void parent_peers_release(uvm_parent_gpu_t *parent_gpu0,
+                                 uvm_parent_gpu_t *parent_gpu1) {
+  uvm_parent_gpu_peer_t *parent_peer_caps =
+      parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
 
-    UVM_ASSERT(parent_peer_caps->ref_count);
+  UVM_ASSERT(parent_peer_caps->ref_count);
 
-    if (--parent_peer_caps->ref_count == 0)
-        parent_peers_destroy(parent_gpu0, parent_gpu1, parent_peer_caps);
+  if (--parent_peer_caps->ref_count == 0)
+    parent_peers_destroy(parent_gpu0, parent_gpu1, parent_peer_caps);
 }
 
-static NV_STATUS peers_init(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1, uvm_gpu_peer_t *peer_caps)
-{
-    NV_STATUS status;
+static NV_STATUS peers_init(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1,
+                            uvm_gpu_peer_t *peer_caps) {
+  NV_STATUS status;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-    UVM_ASSERT(peer_caps->ref_count == 0);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  UVM_ASSERT(peer_caps->ref_count == 0);
 
-    status = parent_peers_retain(gpu0->parent, gpu1->parent);
-    if (status != NV_OK)
-        return status;
+  status = parent_peers_retain(gpu0->parent, gpu1->parent);
+  if (status != NV_OK)
+    return status;
 
-    // Establish peer mappings from each GPU to the other.
-    status = uvm_mmu_create_peer_identity_mappings(gpu0, gpu1);
-    if (status != NV_OK)
-        goto cleanup_parent;
+  // Establish peer mappings from each GPU to the other.
+  status = uvm_mmu_create_peer_identity_mappings(gpu0, gpu1);
+  if (status != NV_OK)
+    goto cleanup_parent;
 
-    status = uvm_mmu_create_peer_identity_mappings(gpu1, gpu0);
-    if (status != NV_OK)
-        goto cleanup_mappings;
+  status = uvm_mmu_create_peer_identity_mappings(gpu1, gpu0);
+  if (status != NV_OK)
+    goto cleanup_mappings;
 
-    peer_caps->ref_count = 1;
+  peer_caps->ref_count = 1;
 
-    set_optimal_p2p_write_ces(gpu0, gpu1);
+  set_optimal_p2p_write_ces(gpu0, gpu1);
 
-    UVM_ASSERT(uvm_gpu_get(gpu0->id) == gpu0);
-    UVM_ASSERT(uvm_gpu_get(gpu1->id) == gpu1);
+  UVM_ASSERT(uvm_gpu_get(gpu0->id) == gpu0);
+  UVM_ASSERT(uvm_gpu_get(gpu1->id) == gpu1);
 
-    uvm_spin_lock(&gpu0->peer_info.peer_gpu_lock);
-    uvm_processor_mask_set(&gpu0->peer_info.peer_gpu_mask, gpu1->id);
-    uvm_spin_unlock(&gpu0->peer_info.peer_gpu_lock);
+  uvm_spin_lock(&gpu0->peer_info.peer_gpu_lock);
+  uvm_processor_mask_set(&gpu0->peer_info.peer_gpu_mask, gpu1->id);
+  uvm_spin_unlock(&gpu0->peer_info.peer_gpu_lock);
 
-    uvm_spin_lock(&gpu1->peer_info.peer_gpu_lock);
-    uvm_processor_mask_set(&gpu1->peer_info.peer_gpu_mask, gpu0->id);
-    uvm_spin_unlock(&gpu1->peer_info.peer_gpu_lock);
+  uvm_spin_lock(&gpu1->peer_info.peer_gpu_lock);
+  uvm_processor_mask_set(&gpu1->peer_info.peer_gpu_mask, gpu0->id);
+  uvm_spin_unlock(&gpu1->peer_info.peer_gpu_lock);
 
-    return NV_OK;
+  return NV_OK;
 
 cleanup_mappings:
-    uvm_mmu_destroy_peer_identity_mappings(gpu0, gpu1);
+  uvm_mmu_destroy_peer_identity_mappings(gpu0, gpu1);
 
 cleanup_parent:
-    parent_peers_release(gpu0->parent, gpu1->parent);
+  parent_peers_release(gpu0->parent, gpu1->parent);
 
-    return status;
+  return status;
 }
 
-static NV_STATUS peers_retain(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
-{
-    uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu0, gpu1);
-    NV_STATUS status = NV_OK;
+static NV_STATUS peers_retain(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1) {
+  uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu0, gpu1);
+  NV_STATUS status = NV_OK;
 
-    if (peer_caps->ref_count == 0)
-        status = peers_init(gpu0, gpu1, peer_caps);
-    else
-        peer_caps->ref_count++;
+  if (peer_caps->ref_count == 0)
+    status = peers_init(gpu0, gpu1, peer_caps);
+  else
+    peer_caps->ref_count++;
 
-    return status;
+  return status;
 }
 
-static void peers_destroy(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1, uvm_gpu_peer_t *peer_caps)
-{
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+static void peers_destroy(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1,
+                          uvm_gpu_peer_t *peer_caps) {
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    uvm_mmu_destroy_peer_identity_mappings(gpu0, gpu1);
-    uvm_mmu_destroy_peer_identity_mappings(gpu1, gpu0);
+  uvm_mmu_destroy_peer_identity_mappings(gpu0, gpu1);
+  uvm_mmu_destroy_peer_identity_mappings(gpu1, gpu0);
 
-    uvm_spin_lock(&gpu0->peer_info.peer_gpu_lock);
-    uvm_processor_mask_clear(&gpu0->peer_info.peer_gpu_mask, gpu1->id);
-    uvm_spin_unlock(&gpu0->peer_info.peer_gpu_lock);
+  uvm_spin_lock(&gpu0->peer_info.peer_gpu_lock);
+  uvm_processor_mask_clear(&gpu0->peer_info.peer_gpu_mask, gpu1->id);
+  uvm_spin_unlock(&gpu0->peer_info.peer_gpu_lock);
 
-    uvm_spin_lock(&gpu1->peer_info.peer_gpu_lock);
-    uvm_processor_mask_clear(&gpu1->peer_info.peer_gpu_mask, gpu0->id);
-    uvm_spin_unlock(&gpu1->peer_info.peer_gpu_lock);
+  uvm_spin_lock(&gpu1->peer_info.peer_gpu_lock);
+  uvm_processor_mask_clear(&gpu1->peer_info.peer_gpu_mask, gpu0->id);
+  uvm_spin_unlock(&gpu1->peer_info.peer_gpu_lock);
 
-    // Flush the access counter buffer to avoid getting stale notifications for
-    // accesses to GPUs to which peer access is being disabled. This is also
-    // needed in the case of disabling automatic (NVLINK) peers on GPU
-    // unregister, because access counter processing might still be using GPU
-    // IDs queried from the peer table above which are about to be removed from
-    // the global table.
-    if (gpu0->parent->access_counters_supported)
-        uvm_parent_gpu_access_counter_buffer_flush(gpu0->parent);
-    if (gpu1->parent->access_counters_supported)
-        uvm_parent_gpu_access_counter_buffer_flush(gpu1->parent);
+  // Flush the access counter buffer to avoid getting stale notifications for
+  // accesses to GPUs to which peer access is being disabled. This is also
+  // needed in the case of disabling automatic (NVLINK) peers on GPU
+  // unregister, because access counter processing might still be using GPU
+  // IDs queried from the peer table above which are about to be removed from
+  // the global table.
+  if (gpu0->parent->access_counters_supported)
+    uvm_parent_gpu_access_counter_buffer_flush(gpu0->parent);
+  if (gpu1->parent->access_counters_supported)
+    uvm_parent_gpu_access_counter_buffer_flush(gpu1->parent);
 
-    parent_peers_release(gpu0->parent, gpu1->parent);
+  parent_peers_release(gpu0->parent, gpu1->parent);
 
-    memset(peer_caps, 0, sizeof(*peer_caps));
+  memset(peer_caps, 0, sizeof(*peer_caps));
 }
 
-static void peers_release(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
-{
-    uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu0, gpu1);
+static void peers_release(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1) {
+  uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu0, gpu1);
 
-    UVM_ASSERT(peer_caps->ref_count);
+  UVM_ASSERT(peer_caps->ref_count);
 
-    if (--peer_caps->ref_count == 0)
-        peers_destroy(gpu0, gpu1, peer_caps);
+  if (--peer_caps->ref_count == 0)
+    peers_destroy(gpu0, gpu1, peer_caps);
 }
 
-static void parent_peers_destroy_static_link(uvm_parent_gpu_t *parent_gpu)
-{
-    uvm_parent_gpu_t *other_parent_gpu;
+static void parent_peers_destroy_static_link(uvm_parent_gpu_t *parent_gpu) {
+  uvm_parent_gpu_t *other_parent_gpu;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    for_each_parent_gpu(other_parent_gpu) {
-        uvm_parent_gpu_peer_t *parent_peer_caps;
+  for_each_parent_gpu(other_parent_gpu) {
+    uvm_parent_gpu_peer_t *parent_peer_caps;
 
-        if (other_parent_gpu == parent_gpu)
-            continue;
+    if (other_parent_gpu == parent_gpu)
+      continue;
 
-        parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
+    parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
 
-        // PCIe peers need to be explicitly destroyed via UvmDisablePeerAccess
-        if (parent_peer_caps->ref_count == 0 || parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
-            continue;
+    // PCIe peers need to be explicitly destroyed via UvmDisablePeerAccess
+    if (parent_peer_caps->ref_count == 0 ||
+        parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
+      continue;
 
-        parent_peers_release(parent_gpu, other_parent_gpu);
-    }
+    parent_peers_release(parent_gpu, other_parent_gpu);
+  }
 }
 
-static NV_STATUS parent_peers_discover_static_link(uvm_parent_gpu_t *parent_gpu)
-{
-    uvm_parent_gpu_t *other_parent_gpu;
-    NV_STATUS status;
+static NV_STATUS
+parent_peers_discover_static_link(uvm_parent_gpu_t *parent_gpu) {
+  uvm_parent_gpu_t *other_parent_gpu;
+  NV_STATUS status;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    for_each_parent_gpu(other_parent_gpu) {
-        uvm_parent_gpu_peer_t *parent_peer_caps;
-        UvmGpuP2PCapsParams p2p_caps_params;
+  for_each_parent_gpu(other_parent_gpu) {
+    uvm_parent_gpu_peer_t *parent_peer_caps;
+    UvmGpuP2PCapsParams p2p_caps_params;
 
-        if (other_parent_gpu == parent_gpu)
-            continue;
+    if (other_parent_gpu == parent_gpu)
+      continue;
 
-        status = get_parent_p2p_caps(parent_gpu, other_parent_gpu, &p2p_caps_params);
-        if (status != NV_OK)
-            goto cleanup;
+    status =
+        get_parent_p2p_caps(parent_gpu, other_parent_gpu, &p2p_caps_params);
+    if (status != NV_OK)
+      goto cleanup;
 
-        // PCIe peers need to be explicitly enabled via UvmEnablePeerAccess
-        if (p2p_caps_params.p2pLink == UVM_LINK_TYPE_NONE || p2p_caps_params.p2pLink == UVM_LINK_TYPE_PCIE)
-            continue;
+    // PCIe peers need to be explicitly enabled via UvmEnablePeerAccess
+    if (p2p_caps_params.p2pLink == UVM_LINK_TYPE_NONE ||
+        p2p_caps_params.p2pLink == UVM_LINK_TYPE_PCIE)
+      continue;
 
-        parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
-        UVM_ASSERT(parent_peer_caps->ref_count == 0);
-        status = parent_peers_init(parent_gpu, other_parent_gpu, parent_peer_caps);
-        if (status != NV_OK)
-            goto cleanup;
-    }
+    parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
+    UVM_ASSERT(parent_peer_caps->ref_count == 0);
+    status = parent_peers_init(parent_gpu, other_parent_gpu, parent_peer_caps);
+    if (status != NV_OK)
+      goto cleanup;
+  }
 
-    return NV_OK;
+  return NV_OK;
 
 cleanup:
-    parent_peers_destroy_static_link(parent_gpu);
+  parent_peers_destroy_static_link(parent_gpu);
 
-    return status;
+  return status;
 }
 
-static void peers_destroy_static_link(uvm_gpu_t *gpu)
-{
-    uvm_parent_gpu_t *other_parent_gpu;
-    uvm_parent_gpu_t *parent_gpu;
+static void peers_destroy_static_link(uvm_gpu_t *gpu) {
+  uvm_parent_gpu_t *other_parent_gpu;
+  uvm_parent_gpu_t *parent_gpu;
 
-    UVM_ASSERT(gpu);
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  UVM_ASSERT(gpu);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    parent_gpu = gpu->parent;
+  parent_gpu = gpu->parent;
 
-    for_each_parent_gpu(other_parent_gpu) {
-        uvm_parent_gpu_peer_t *parent_peer_caps;
-        uvm_gpu_t *other_gpu;
+  for_each_parent_gpu(other_parent_gpu) {
+    uvm_parent_gpu_peer_t *parent_peer_caps;
+    uvm_gpu_t *other_gpu;
 
-        if (other_parent_gpu == parent_gpu)
-            continue;
+    if (other_parent_gpu == parent_gpu)
+      continue;
 
-        parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
+    parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
 
-        // PCIe peers need to be explicitly destroyed via UvmDisablePeerAccess
-        if (parent_peer_caps->ref_count == 0 || parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
-            continue;
+    // PCIe peers need to be explicitly destroyed via UvmDisablePeerAccess
+    if (parent_peer_caps->ref_count == 0 ||
+        parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
+      continue;
 
-        for_each_gpu_in_parent(other_gpu, other_parent_gpu) {
-            uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu, other_gpu);
+    for_each_gpu_in_parent(other_gpu, other_parent_gpu) {
+      uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu, other_gpu);
 
-            if (peer_caps->ref_count == 0)
-                continue;
+      if (peer_caps->ref_count == 0)
+        continue;
 
-            peers_release(gpu, other_gpu);
-        }
+      peers_release(gpu, other_gpu);
     }
+  }
 }
 
-static NV_STATUS peers_discover_static_link(uvm_gpu_t *gpu)
-{
-    uvm_parent_gpu_t *parent_gpu = gpu->parent;
-    uvm_parent_gpu_t *other_parent_gpu;
-    uvm_gpu_t *other_gpu;
-    NV_STATUS status;
+static NV_STATUS peers_discover_static_link(uvm_gpu_t *gpu) {
+  uvm_parent_gpu_t *parent_gpu = gpu->parent;
+  uvm_parent_gpu_t *other_parent_gpu;
+  uvm_gpu_t *other_gpu;
+  NV_STATUS status;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    for_each_parent_gpu(other_parent_gpu) {
-        uvm_parent_gpu_peer_t *parent_peer_caps;
+  for_each_parent_gpu(other_parent_gpu) {
+    uvm_parent_gpu_peer_t *parent_peer_caps;
 
-        if (other_parent_gpu == parent_gpu)
-            continue;
+    if (other_parent_gpu == parent_gpu)
+      continue;
 
-        parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
-        if (parent_peer_caps->ref_count == 0 || parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
-            continue;
+    parent_peer_caps = parent_gpu_peer_caps(parent_gpu, other_parent_gpu);
+    if (parent_peer_caps->ref_count == 0 ||
+        parent_peer_caps->link_type == UVM_GPU_LINK_PCIE)
+      continue;
 
-        for_each_gpu_in_parent(other_gpu, other_parent_gpu) {
-            uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu, other_gpu);
+    for_each_gpu_in_parent(other_gpu, other_parent_gpu) {
+      uvm_gpu_peer_t *peer_caps = gpu_peer_caps(gpu, other_gpu);
 
-            UVM_ASSERT(peer_caps->ref_count == 0);
-            status = peers_init(gpu, other_gpu, peer_caps);
-            if (status != NV_OK)
-                goto cleanup;
-        }
+      UVM_ASSERT(peer_caps->ref_count == 0);
+      status = peers_init(gpu, other_gpu, peer_caps);
+      if (status != NV_OK)
+        goto cleanup;
     }
+  }
 
-    return NV_OK;
+  return NV_OK;
 
 cleanup:
-    peers_destroy_static_link(gpu);
+  peers_destroy_static_link(gpu);
 
-    return status;
+  return status;
 }
 
-static NV_STATUS uvm_gpu_init_access_bits(uvm_parent_gpu_t *parent_gpu)
-{
-    return uvm_rm_locked_call(nvUvmInterfaceAccessBitsBufAlloc(parent_gpu->rm_device, &parent_gpu->vab_info));
+static NV_STATUS uvm_gpu_init_access_bits(uvm_parent_gpu_t *parent_gpu) {
+  return uvm_rm_locked_call(nvUvmInterfaceAccessBitsBufAlloc(
+      parent_gpu->rm_device, &parent_gpu->vab_info));
 }
 
-static NV_STATUS uvm_gpu_update_access_bits(uvm_parent_gpu_t *parent_gpu, UVM_ACCESS_BITS_DUMP_MODE mode) 
-{
-    return nvUvmInterfaceAccessBitsDump(parent_gpu->rm_device, &parent_gpu->vab_info, mode);
+static NV_STATUS uvm_gpu_update_access_bits(uvm_parent_gpu_t *parent_gpu,
+                                            UVM_ACCESS_BITS_DUMP_MODE mode) {
+  return nvUvmInterfaceAccessBitsDump(parent_gpu->rm_device,
+                                      &parent_gpu->vab_info, mode);
 }
 
-static NV_STATUS uvm_gpu_deinit_access_bits(uvm_parent_gpu_t *parent_gpu)
-{
-    return uvm_rm_locked_call(nvUvmInterfaceAccessBitsBufFree(parent_gpu->rm_device, &parent_gpu->vab_info));
+static NV_STATUS uvm_gpu_deinit_access_bits(uvm_parent_gpu_t *parent_gpu) {
+  return uvm_rm_locked_call(nvUvmInterfaceAccessBitsBufFree(
+      parent_gpu->rm_device, &parent_gpu->vab_info));
 }
 
 // Remove a gpu and unregister it from RM
 // Note that this is also used in most error paths in add_gpu()
-static void remove_gpu(uvm_gpu_t *gpu)
-{
-    NvU32 sub_processor_index;
-    uvm_parent_gpu_t *parent_gpu;
-    bool free_parent;
-    NV_STATUS status;
+static void remove_gpu(uvm_gpu_t *gpu) {
+  NvU32 sub_processor_index;
+  uvm_parent_gpu_t *parent_gpu;
+  bool free_parent;
+  NV_STATUS status;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-    sub_processor_index = uvm_id_sub_processor_index(gpu->id);
-    parent_gpu = gpu->parent;
+  sub_processor_index = uvm_id_sub_processor_index(gpu->id);
+  parent_gpu = gpu->parent;
 
-    UVM_ASSERT_MSG(uvm_gpu_retained_count(gpu) == 0,
-                   "gpu_id %u retained_count %llu\n",
-                   uvm_id_value(gpu->id),
-                   uvm_gpu_retained_count(gpu));
+  UVM_ASSERT_MSG(uvm_gpu_retained_count(gpu) == 0,
+                 "gpu_id %u retained_count %llu\n", uvm_id_value(gpu->id),
+                 uvm_gpu_retained_count(gpu));
 
-    UVM_ASSERT(parent_gpu->num_retained_gpus > 0);
-    parent_gpu->num_retained_gpus--;
+  UVM_ASSERT(parent_gpu->num_retained_gpus > 0);
+  parent_gpu->num_retained_gpus--;
 
-    free_parent = (parent_gpu->num_retained_gpus == 0);
+  free_parent = (parent_gpu->num_retained_gpus == 0);
 
-    if (free_parent && parent_gpu->access_bits_supported) {
-        status = uvm_gpu_deinit_access_bits(parent_gpu);
-        UVM_ASSERT(status == NV_OK);
-    }
+  if (free_parent && parent_gpu->access_bits_supported) {
+    status = uvm_gpu_deinit_access_bits(parent_gpu);
+    UVM_ASSERT(status == NV_OK);
+  }
 
-    // NVLINK peers must be removed and the relevant access counter buffers must
-    // be flushed before removing this GPU from the global table.
-    peers_destroy_static_link(gpu);
+  // NVLINK peers must be removed and the relevant access counter buffers must
+  // be flushed before removing this GPU from the global table.
+  peers_destroy_static_link(gpu);
 
-    if (free_parent)
-        parent_peers_destroy_static_link(parent_gpu);
+  if (free_parent)
+    parent_peers_destroy_static_link(parent_gpu);
 
-    // uvm_mem_free and other uvm_mem APIs invoked by the Confidential Compute
-    // deinitialization must be called before the GPU is removed from the global
-    // table.
-    //
-    // TODO: Bug 2008200: Add and remove the GPU in a more reasonable spot.
-    uvm_conf_computing_gpu_deinit(gpu);
+  // uvm_mem_free and other uvm_mem APIs invoked by the Confidential Compute
+  // deinitialization must be called before the GPU is removed from the global
+  // table.
+  //
+  // TODO: Bug 2008200: Add and remove the GPU in a more reasonable spot.
+  uvm_conf_computing_gpu_deinit(gpu);
 
-    // If the parent is not being freed, the following gpu_table_lock is only
-    // needed to protect concurrent uvm_parent_gpu_find_first_valid_gpu() in BH
-    // from the __clear_bit here.
-    // In the free_parent case, gpu_table_lock protects the top half from the
-    // uvm_global_remove_parent_gpu()
-    uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
+  // If the parent is not being freed, the following gpu_table_lock is only
+  // needed to protect concurrent uvm_parent_gpu_find_first_valid_gpu() in BH
+  // from the __clear_bit here.
+  // In the free_parent case, gpu_table_lock protects the top half from the
+  // uvm_global_remove_parent_gpu()
+  uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
 
-    // Mark the GPU as invalid in the parent GPU's GPU table.
-    __clear_bit(sub_processor_index, parent_gpu->valid_gpus);
+  // Mark the GPU as invalid in the parent GPU's GPU table.
+  __clear_bit(sub_processor_index, parent_gpu->valid_gpus);
 
-    // Remove the GPU from the table.
-    if (free_parent)
-        uvm_global_remove_parent_gpu(parent_gpu);
+  // Remove the GPU from the table.
+  if (free_parent)
+    uvm_global_remove_parent_gpu(parent_gpu);
 
-    uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
+  uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
 
-    uvm_processor_mask_clear(&g_uvm_global.retained_gpus, gpu->id);
+  uvm_processor_mask_clear(&g_uvm_global.retained_gpus, gpu->id);
 
-    // If the parent is being freed, stop scheduling new bottom halves and
-    // update relevant software state. Else flush any pending bottom halves
-    // before continuing.
-    if (free_parent)
-        uvm_parent_gpu_disable_isr(parent_gpu);
-    else
-        uvm_parent_gpu_flush_bottom_halves(parent_gpu);
+  // If the parent is being freed, stop scheduling new bottom halves and
+  // update relevant software state. Else flush any pending bottom halves
+  // before continuing.
+  if (free_parent)
+    uvm_parent_gpu_disable_isr(parent_gpu);
+  else
+    uvm_parent_gpu_flush_bottom_halves(parent_gpu);
 
-    deinit_gpu(gpu);
+  deinit_gpu(gpu);
 
-    UVM_ASSERT(parent_gpu->gpus[sub_processor_index] == gpu);
-    parent_gpu->gpus[sub_processor_index] = NULL;
-    uvm_kvfree(gpu);
+  UVM_ASSERT(parent_gpu->gpus[sub_processor_index] == gpu);
+  parent_gpu->gpus[sub_processor_index] = NULL;
+  uvm_kvfree(gpu);
 
-    if (free_parent)
-        deinit_parent_gpu(parent_gpu);
+  if (free_parent)
+    deinit_parent_gpu(parent_gpu);
 }
 
 // Add a new gpu and register it with RM
-static NV_STATUS add_gpu(const NvProcessorUuid *gpu_uuid,
-                         const uvm_gpu_id_t gpu_id,
-                         const UvmGpuInfo *gpu_info,
-                         const UvmGpuPlatformInfo *gpu_platform_info,
-                         uvm_parent_gpu_t *parent_gpu,
-                         const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
-                         uvm_gpu_t **gpu_out)
-{
-    NV_STATUS status;
-    bool alloc_parent = (parent_gpu == NULL);
-    uvm_gpu_t *gpu = NULL;
-
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-
-    if (alloc_parent) {
-        status = alloc_parent_gpu(gpu_uuid, uvm_parent_gpu_id_from_gpu_id(gpu_id), &parent_gpu);
-        if (status != NV_OK)
-            return status;
-
-        if (uvm_enable_builtin_tests)
-            parent_gpu->test = *parent_gpu_error;
-    }
+static NV_STATUS
+add_gpu(const NvProcessorUuid *gpu_uuid, const uvm_gpu_id_t gpu_id,
+        const UvmGpuInfo *gpu_info, const UvmGpuPlatformInfo *gpu_platform_info,
+        uvm_parent_gpu_t *parent_gpu,
+        const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
+        uvm_gpu_t **gpu_out) {
+  NV_STATUS status;
+  bool alloc_parent = (parent_gpu == NULL);
+  uvm_gpu_t *gpu = NULL;
+
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+
+  if (alloc_parent) {
+    status = alloc_parent_gpu(gpu_uuid, uvm_parent_gpu_id_from_gpu_id(gpu_id),
+                              &parent_gpu);
+    if (status != NV_OK)
+      return status;
 
-    gpu = alloc_gpu(parent_gpu, gpu_id);
-    if (!gpu) {
-        if (alloc_parent)
-            uvm_parent_gpu_kref_put(parent_gpu);
+    if (uvm_enable_builtin_tests)
+      parent_gpu->test = *parent_gpu_error;
+  }
 
-        return NV_ERR_NO_MEMORY;
-    }
+  gpu = alloc_gpu(parent_gpu, gpu_id);
+  if (!gpu) {
+    if (alloc_parent)
+      uvm_parent_gpu_kref_put(parent_gpu);
 
-    parent_gpu->num_retained_gpus++;
+    return NV_ERR_NO_MEMORY;
+  }
 
-    if (alloc_parent)
-        fill_parent_gpu_info(parent_gpu, gpu_info);
+  parent_gpu->num_retained_gpus++;
 
-    // After this point all error clean up should be handled by remove_gpu()
+  if (alloc_parent)
+    fill_parent_gpu_info(parent_gpu, gpu_info);
 
-    if (!gpu_supports_uvm(parent_gpu)) {
-        UVM_DBG_PRINT("Registration of non-UVM-capable GPU attempted: GPU %s\n", uvm_gpu_name(gpu));
-        status = NV_ERR_NOT_SUPPORTED;
-        goto error;
-    }
+  // After this point all error clean up should be handled by remove_gpu()
 
-    if (alloc_parent) {
-        status = init_parent_gpu(parent_gpu, gpu_uuid, gpu_info, gpu_platform_info);
-        if (status != NV_OK)
-            goto error;
-    }
+  if (!gpu_supports_uvm(parent_gpu)) {
+    UVM_DBG_PRINT("Registration of non-UVM-capable GPU attempted: GPU %s\n",
+                  uvm_gpu_name(gpu));
+    status = NV_ERR_NOT_SUPPORTED;
+    goto error;
+  }
 
-    status = init_gpu(gpu, gpu_info);
+  if (alloc_parent) {
+    status = init_parent_gpu(parent_gpu, gpu_uuid, gpu_info, gpu_platform_info);
     if (status != NV_OK)
-        goto error;
-
-    status = uvm_gpu_check_ecc_error(gpu);
+      goto error;
+  }
+
+  status = init_gpu(gpu, gpu_info);
+  if (status != NV_OK)
+    goto error;
+
+  status = uvm_gpu_check_ecc_error(gpu);
+  if (status != NV_OK)
+    goto error;
+
+  atomic64_set(&gpu->retained_count, 1);
+  uvm_processor_mask_set(&g_uvm_global.retained_gpus, gpu->id);
+
+  uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
+
+  if (alloc_parent)
+    uvm_global_add_parent_gpu(parent_gpu);
+
+  // Mark the GPU as valid in the parent GPU's GPU table.
+  UVM_ASSERT(
+      !test_bit(uvm_id_sub_processor_index(gpu->id), parent_gpu->valid_gpus));
+  __set_bit(uvm_id_sub_processor_index(gpu->id), parent_gpu->valid_gpus);
+
+  // Although locking correctness does not, at this early point (before the
+  // GPU is visible in the table) strictly require holding the gpu_table_lock
+  // in order to read gpu->isr.replayable_faults.handling, nor to enable page
+  // fault interrupts (this could have been done earlier), it is best to do it
+  // here, in order to avoid an interrupt storm. That way, we take advantage
+  // of the spinlock_irqsave side effect of turning off local CPU interrupts,
+  // part of holding the gpu_table_lock. That means that the local CPU won't
+  // receive any of these interrupts, until the GPU is safely added to the
+  // table (where the top half ISR can find it).
+  //
+  // As usual with spinlock_irqsave behavior, *other* CPUs can still handle
+  // these interrupts, but the local CPU will not be slowed down (interrupted)
+  // by such handling, and can quickly release the gpu_table_lock, thus
+  // unblocking any other CPU's top half (which waits for the gpu_table_lock).
+  if (alloc_parent && parent_gpu->isr.replayable_faults.handling) {
+    parent_gpu->fault_buffer_hal->enable_replayable_faults(parent_gpu);
+
+    // Clear the interrupt bit and force the re-evaluation of the interrupt
+    // condition to ensure that we don't miss any pending interrupt
+    parent_gpu->fault_buffer_hal->clear_replayable_faults(
+        parent_gpu, parent_gpu->fault_buffer.replayable.cached_get);
+  }
+
+  // Access counters are enabled on demand
+
+  uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
+
+  if (alloc_parent) {
+    status = parent_peers_discover_static_link(parent_gpu);
     if (status != NV_OK)
-        goto error;
-
-    atomic64_set(&gpu->retained_count, 1);
-    uvm_processor_mask_set(&g_uvm_global.retained_gpus, gpu->id);
-
-    uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
-
-    if (alloc_parent)
-        uvm_global_add_parent_gpu(parent_gpu);
-
-    // Mark the GPU as valid in the parent GPU's GPU table.
-    UVM_ASSERT(!test_bit(uvm_id_sub_processor_index(gpu->id), parent_gpu->valid_gpus));
-    __set_bit(uvm_id_sub_processor_index(gpu->id), parent_gpu->valid_gpus);
-
-    // Although locking correctness does not, at this early point (before the
-    // GPU is visible in the table) strictly require holding the gpu_table_lock
-    // in order to read gpu->isr.replayable_faults.handling, nor to enable page
-    // fault interrupts (this could have been done earlier), it is best to do it
-    // here, in order to avoid an interrupt storm. That way, we take advantage
-    // of the spinlock_irqsave side effect of turning off local CPU interrupts,
-    // part of holding the gpu_table_lock. That means that the local CPU won't
-    // receive any of these interrupts, until the GPU is safely added to the
-    // table (where the top half ISR can find it).
-    //
-    // As usual with spinlock_irqsave behavior, *other* CPUs can still handle
-    // these interrupts, but the local CPU will not be slowed down (interrupted)
-    // by such handling, and can quickly release the gpu_table_lock, thus
-    // unblocking any other CPU's top half (which waits for the gpu_table_lock).
-    if (alloc_parent && parent_gpu->isr.replayable_faults.handling) {
-        parent_gpu->fault_buffer_hal->enable_replayable_faults(parent_gpu);
-
-        // Clear the interrupt bit and force the re-evaluation of the interrupt
-        // condition to ensure that we don't miss any pending interrupt
-        parent_gpu->fault_buffer_hal->clear_replayable_faults(parent_gpu,
-                                                              parent_gpu->fault_buffer.replayable.cached_get);
-    }
+      goto error_retained;
+  }
 
-    // Access counters are enabled on demand
+  status = peers_discover_static_link(gpu);
+  if (status != NV_OK)
+    goto error_retained;
 
-    uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
+  *gpu_out = gpu;
 
-    if (alloc_parent) {
-        status = parent_peers_discover_static_link(parent_gpu);
-        if (status != NV_OK)
-            goto error_retained;
-    }
-
-    status = peers_discover_static_link(gpu);
+  if (alloc_parent && parent_gpu->access_bits_supported) {
+    status = uvm_gpu_init_access_bits(parent_gpu);
     if (status != NV_OK)
-        goto error_retained;
-
-    *gpu_out = gpu;
+      goto error_retained;
+  }
 
-    if (alloc_parent && parent_gpu->access_bits_supported) {
-        status = uvm_gpu_init_access_bits(parent_gpu);
-        if (status != NV_OK)
-            goto error_retained;
-    }
-
-    return NV_OK;
+  return NV_OK;
 
 error_retained:
-    UVM_ERR_PRINT("Failed to discover NVLINK/BAR1 peers: %s, GPU %s\n", nvstatusToString(status), uvm_gpu_name(gpu));
+  UVM_ERR_PRINT("Failed to discover NVLINK/BAR1 peers: %s, GPU %s\n",
+                nvstatusToString(status), uvm_gpu_name(gpu));
 
-    // Nobody can have retained the GPU yet, since we still hold the
-    // global lock.
-    UVM_ASSERT(uvm_gpu_retained_count(gpu) == 1);
-    atomic64_set(&gpu->retained_count, 0);
+  // Nobody can have retained the GPU yet, since we still hold the
+  // global lock.
+  UVM_ASSERT(uvm_gpu_retained_count(gpu) == 1);
+  atomic64_set(&gpu->retained_count, 0);
 error:
-    remove_gpu(gpu);
+  remove_gpu(gpu);
 
-    return status;
+  return status;
 }
 
 // Increment the refcount for the GPU with the given UUID. If this is the first
@@ -2998,937 +3048,959 @@ error:
 // obtain information about the partition. nvUvmInterfaceGetGpuInfo returns, in
 // gpu_info, whether SMC is enabled, the swizzId, and GI UUID corresponding to
 // the partition.
-static NV_STATUS gpu_retain_by_uuid_locked(const NvProcessorUuid *gpu_uuid,
-                                           const uvm_rm_user_object_t *user_rm_device,
-                                           const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
-                                           uvm_gpu_t **gpu_out)
-{
-    NV_STATUS status = NV_OK;
-    uvm_gpu_t *gpu = NULL;
-    uvm_parent_gpu_t *parent_gpu;
-    UvmGpuInfo *gpu_info = NULL;
-    UvmGpuClientInfo client_info = {0};
-    UvmGpuPlatformInfo gpu_platform_info = {0};
-    uvm_gpu_id_t gpu_id;
-
-    client_info.hClient = user_rm_device->user_client;
-    client_info.hSmcPartRef = user_rm_device->user_object;
-
-    gpu_info = uvm_kvmalloc_zero(sizeof(*gpu_info));
-    if (!gpu_info)
-        return NV_ERR_NO_MEMORY;
-
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-
-    parent_gpu = uvm_parent_gpu_get_by_uuid_locked(gpu_uuid);
-
-    if (parent_gpu == NULL) {
-        // If this is the first time the UUID is seen, register it on RM
-        status = uvm_rm_locked_call(nvUvmInterfaceRegisterGpu(gpu_uuid, &gpu_platform_info));
-        if (status != NV_OK)
-            goto error_free_gpu_info;
-    }
-
-    status = uvm_rm_locked_call(nvUvmInterfaceGetGpuInfo(gpu_uuid, &client_info, gpu_info));
+static NV_STATUS gpu_retain_by_uuid_locked(
+    const NvProcessorUuid *gpu_uuid, const uvm_rm_user_object_t *user_rm_device,
+    const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
+    uvm_gpu_t **gpu_out) {
+  NV_STATUS status = NV_OK;
+  uvm_gpu_t *gpu = NULL;
+  uvm_parent_gpu_t *parent_gpu;
+  UvmGpuInfo *gpu_info = NULL;
+  UvmGpuClientInfo client_info = {0};
+  UvmGpuPlatformInfo gpu_platform_info = {0};
+  uvm_gpu_id_t gpu_id;
+
+  client_info.hClient = user_rm_device->user_client;
+  client_info.hSmcPartRef = user_rm_device->user_object;
+
+  gpu_info = uvm_kvmalloc_zero(sizeof(*gpu_info));
+  if (!gpu_info)
+    return NV_ERR_NO_MEMORY;
+
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+
+  parent_gpu = uvm_parent_gpu_get_by_uuid_locked(gpu_uuid);
+
+  if (parent_gpu == NULL) {
+    // If this is the first time the UUID is seen, register it on RM
+    status = uvm_rm_locked_call(
+        nvUvmInterfaceRegisterGpu(gpu_uuid, &gpu_platform_info));
     if (status != NV_OK)
-        goto error_unregister;
-
-    if (parent_gpu != NULL) {
-        // If the UUID has been seen before, and if SMC is enabled, then check
-        // if this specific partition has been seen previously. The UUID-based
-        // look-up above may have succeeded for a different partition with the
-        // same parent GPU.
-        if (gpu_info->smcEnabled) {
-            gpu = uvm_gpu_get_by_parent_and_swizz_id(parent_gpu, gpu_info->smcSwizzId);
-        }
-        else {
-            gpu = parent_gpu->gpus[0];
-            UVM_ASSERT(gpu != NULL);
-        }
-    }
-
-    if (gpu == NULL) {
-        status = find_unused_gpu_id(parent_gpu, &gpu_id);
-        if (status != NV_OK)
-            goto error_unregister;
+      goto error_free_gpu_info;
+  }
+
+  status = uvm_rm_locked_call(
+      nvUvmInterfaceGetGpuInfo(gpu_uuid, &client_info, gpu_info));
+  if (status != NV_OK)
+    goto error_unregister;
+
+  if (parent_gpu != NULL) {
+    // If the UUID has been seen before, and if SMC is enabled, then check
+    // if this specific partition has been seen previously. The UUID-based
+    // look-up above may have succeeded for a different partition with the
+    // same parent GPU.
+    if (gpu_info->smcEnabled) {
+      gpu =
+          uvm_gpu_get_by_parent_and_swizz_id(parent_gpu, gpu_info->smcSwizzId);
+    } else {
+      gpu = parent_gpu->gpus[0];
+      UVM_ASSERT(gpu != NULL);
+    }
+  }
+
+  if (gpu == NULL) {
+    status = find_unused_gpu_id(parent_gpu, &gpu_id);
+    if (status != NV_OK)
+      goto error_unregister;
 
-        status = add_gpu(gpu_uuid, gpu_id, gpu_info, &gpu_platform_info, parent_gpu, parent_gpu_error, &gpu);
-        if (status != NV_OK)
-            goto error_unregister;
-    }
-    else {
-        atomic64_inc(&gpu->retained_count);
-    }
+    status = add_gpu(gpu_uuid, gpu_id, gpu_info, &gpu_platform_info, parent_gpu,
+                     parent_gpu_error, &gpu);
+    if (status != NV_OK)
+      goto error_unregister;
+  } else {
+    atomic64_inc(&gpu->retained_count);
+  }
 
-    *gpu_out = gpu;
+  *gpu_out = gpu;
 
-    uvm_kvfree(gpu_info);
+  uvm_kvfree(gpu_info);
 
-    return status;
+  return status;
 
 error_unregister:
-    if (parent_gpu == NULL)
-        uvm_rm_locked_call_void(nvUvmInterfaceUnregisterGpu(gpu_uuid));
+  if (parent_gpu == NULL)
+    uvm_rm_locked_call_void(nvUvmInterfaceUnregisterGpu(gpu_uuid));
 error_free_gpu_info:
-    uvm_kvfree(gpu_info);
+  uvm_kvfree(gpu_info);
 
-    return status;
+  return status;
 }
 
-NV_STATUS uvm_gpu_retain_by_uuid(const NvProcessorUuid *gpu_uuid,
-                                 const uvm_rm_user_object_t *user_rm_device,
-                                 const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
-                                 uvm_gpu_t **gpu_out)
-{
-    NV_STATUS status;
-    uvm_mutex_lock(&g_uvm_global.global_lock);
-    status = gpu_retain_by_uuid_locked(gpu_uuid, user_rm_device, parent_gpu_error, gpu_out);
-    uvm_mutex_unlock(&g_uvm_global.global_lock);
-    return status;
+NV_STATUS uvm_gpu_retain_by_uuid(
+    const NvProcessorUuid *gpu_uuid, const uvm_rm_user_object_t *user_rm_device,
+    const uvm_test_parent_gpu_inject_error_t *parent_gpu_error,
+    uvm_gpu_t **gpu_out) {
+  NV_STATUS status;
+  uvm_mutex_lock(&g_uvm_global.global_lock);
+  status = gpu_retain_by_uuid_locked(gpu_uuid, user_rm_device, parent_gpu_error,
+                                     gpu_out);
+  uvm_mutex_unlock(&g_uvm_global.global_lock);
+  return status;
 }
 
-void uvm_gpu_release_locked(uvm_gpu_t *gpu)
-{
-    uvm_parent_gpu_t *parent_gpu = gpu->parent;
+void uvm_gpu_release_locked(uvm_gpu_t *gpu) {
+  uvm_parent_gpu_t *parent_gpu = gpu->parent;
 
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-    UVM_ASSERT(uvm_gpu_retained_count(gpu) > 0);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  UVM_ASSERT(uvm_gpu_retained_count(gpu) > 0);
 
-    if (atomic64_dec_and_test(&gpu->retained_count)) {
-        nv_kref_get(&parent_gpu->gpu_kref);
-        remove_gpu(gpu);
-        if (parent_gpu->num_retained_gpus == 0)
-            uvm_rm_locked_call_void(nvUvmInterfaceUnregisterGpu(&parent_gpu->uuid));
-        uvm_parent_gpu_kref_put(parent_gpu);
-    }
+  if (atomic64_dec_and_test(&gpu->retained_count)) {
+    nv_kref_get(&parent_gpu->gpu_kref);
+    remove_gpu(gpu);
+    if (parent_gpu->num_retained_gpus == 0)
+      uvm_rm_locked_call_void(nvUvmInterfaceUnregisterGpu(&parent_gpu->uuid));
+    uvm_parent_gpu_kref_put(parent_gpu);
+  }
 }
 
-void uvm_gpu_release(uvm_gpu_t *gpu)
-{
-    uvm_mutex_lock(&g_uvm_global.global_lock);
-    uvm_gpu_release_locked(gpu);
-    uvm_mutex_unlock(&g_uvm_global.global_lock);
+void uvm_gpu_release(uvm_gpu_t *gpu) {
+  uvm_mutex_lock(&g_uvm_global.global_lock);
+  uvm_gpu_release_locked(gpu);
+  uvm_mutex_unlock(&g_uvm_global.global_lock);
 }
 
-NV_STATUS uvm_gpu_retain_pcie_peer_access(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
-{
-    NV_STATUS status;
-
-    UVM_ASSERT(gpu0);
-    UVM_ASSERT(gpu1);
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-
-    status = peers_retain(gpu0, gpu1);
-    if (status != NV_OK)
-        return status;
-
-    if (uvm_parent_gpu_peer_link_type(gpu0->parent, gpu1->parent) != UVM_GPU_LINK_PCIE) {
-        peers_release(gpu0, gpu1);
-        return NV_ERR_INVALID_DEVICE;
-    }
-
-    // GPUs can't be destroyed until their peer pairings have also been
-    // destroyed.
-    uvm_gpu_retain(gpu0);
-    uvm_gpu_retain(gpu1);
+NV_STATUS uvm_gpu_retain_pcie_peer_access(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1) {
+  NV_STATUS status;
 
-    return NV_OK;
-}
+  UVM_ASSERT(gpu0);
+  UVM_ASSERT(gpu1);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
 
-void uvm_gpu_release_pcie_peer_access(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
-{
-    UVM_ASSERT(gpu0);
-    UVM_ASSERT(gpu1);
-    uvm_assert_mutex_locked(&g_uvm_global.global_lock);
-    UVM_ASSERT(uvm_parent_gpu_peer_link_type(gpu0->parent, gpu1->parent) == UVM_GPU_LINK_PCIE);
+  status = peers_retain(gpu0, gpu1);
+  if (status != NV_OK)
+    return status;
 
+  if (uvm_parent_gpu_peer_link_type(gpu0->parent, gpu1->parent) !=
+      UVM_GPU_LINK_PCIE) {
     peers_release(gpu0, gpu1);
+    return NV_ERR_INVALID_DEVICE;
+  }
 
-    uvm_gpu_release_locked(gpu0);
-    uvm_gpu_release_locked(gpu1);
-}
-
-uvm_gpu_link_type_t uvm_parent_gpu_peer_link_type(uvm_parent_gpu_t *parent_gpu0, uvm_parent_gpu_t *parent_gpu1)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps;
-
-    if (parent_gpu0 == parent_gpu1)
-        return UVM_GPU_LINK_INVALID;
-
-    parent_peer_caps = parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
-    if (parent_peer_caps->ref_count == 0)
-        return UVM_GPU_LINK_INVALID;
-
-    return parent_peer_caps->link_type;
-}
-
-uvm_aperture_t uvm_gpu_peer_aperture(uvm_gpu_t *local_gpu, uvm_gpu_t *remote_gpu)
-{
-    uvm_parent_gpu_peer_t *parent_peer_caps;
-
-    // MIG instances in the same physical GPU have vidmem addresses
-    if (uvm_gpus_are_smc_peers(local_gpu, remote_gpu))
-        return UVM_APERTURE_VID;
+  // GPUs can't be destroyed until their peer pairings have also been
+  // destroyed.
+  uvm_gpu_retain(gpu0);
+  uvm_gpu_retain(gpu1);
 
-    parent_peer_caps = parent_gpu_peer_caps(local_gpu->parent, remote_gpu->parent);
-    return parent_gpu_peer_aperture(local_gpu->parent, remote_gpu->parent, parent_peer_caps);
+  return NV_OK;
 }
 
-uvm_gpu_phys_address_t uvm_gpu_peer_phys_address(uvm_gpu_t *owning_gpu, NvU64 address, uvm_gpu_t *accessing_gpu)
-{
-    uvm_aperture_t aperture = uvm_gpu_peer_aperture(accessing_gpu, owning_gpu);
+void uvm_gpu_release_pcie_peer_access(uvm_gpu_t *gpu0, uvm_gpu_t *gpu1) {
+  UVM_ASSERT(gpu0);
+  UVM_ASSERT(gpu1);
+  uvm_assert_mutex_locked(&g_uvm_global.global_lock);
+  UVM_ASSERT(uvm_parent_gpu_peer_link_type(gpu0->parent, gpu1->parent) ==
+             UVM_GPU_LINK_PCIE);
 
-    if (uvm_parent_gpus_are_nvlink_direct_connected(accessing_gpu->parent, owning_gpu->parent)) {
-        UVM_ASSERT(uvm_aperture_is_peer(aperture));
-        address += owning_gpu->parent->peer_address_info.peer_gpa_memory_window_start;
-    }
-    else if (uvm_parent_gpus_are_nvswitch_connected(accessing_gpu->parent, owning_gpu->parent)) {
-        UVM_ASSERT(uvm_aperture_is_peer(aperture));
-        address += owning_gpu->parent->nvswitch_info.fabric_memory_window_start;
-    }
-    else if (uvm_aperture_is_sys(aperture)) {
-        // BAR1 P2P can use either coherent or non-coherent sysmem,
-        // depending on atomic capabilities of the peer devices.
-        uvm_parent_gpu_peer_t *parent_peer_caps = parent_gpu_peer_caps(accessing_gpu->parent, owning_gpu->parent);
-        int peer_index = (uvm_id_cmp(accessing_gpu->id, owning_gpu->id) < 0) ? 0 : 1;
-        
-        UVM_ASSERT(parent_peer_caps->link_type == UVM_GPU_LINK_PCIE_BAR1);
-        UVM_ASSERT(parent_peer_caps->bar1_p2p_dma_size[peer_index] != 0);
-
-        address += parent_peer_caps->bar1_p2p_dma_base_address[peer_index];
-    }
+  peers_release(gpu0, gpu1);
 
-    return uvm_gpu_phys_address(aperture, address);
+  uvm_gpu_release_locked(gpu0);
+  uvm_gpu_release_locked(gpu1);
 }
 
-uvm_gpu_address_t uvm_gpu_peer_copy_address(uvm_gpu_t *owning_gpu, NvU64 address, uvm_gpu_t *accessing_gpu)
-{
-    uvm_gpu_identity_mapping_t *gpu_peer_mapping;
+uvm_gpu_link_type_t
+uvm_parent_gpu_peer_link_type(uvm_parent_gpu_t *parent_gpu0,
+                              uvm_parent_gpu_t *parent_gpu1) {
+  uvm_parent_gpu_peer_t *parent_peer_caps;
 
-    if (accessing_gpu->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_PHYSICAL)
-        return uvm_gpu_address_from_phys(uvm_gpu_peer_phys_address(owning_gpu, address, accessing_gpu));
+  if (parent_gpu0 == parent_gpu1)
+    return UVM_GPU_LINK_INVALID;
 
-    UVM_ASSERT(accessing_gpu->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_VIRTUAL);
-    gpu_peer_mapping = uvm_gpu_get_peer_mapping(accessing_gpu, owning_gpu->id);
+  parent_peer_caps = parent_gpu_peer_caps(parent_gpu0, parent_gpu1);
+  if (parent_peer_caps->ref_count == 0)
+    return UVM_GPU_LINK_INVALID;
 
-    return uvm_gpu_address_virtual(gpu_peer_mapping->base + address);
+  return parent_peer_caps->link_type;
 }
 
-uvm_aperture_t uvm_gpu_egm_peer_aperture(uvm_parent_gpu_t *local_gpu, uvm_parent_gpu_t *remote_gpu)
-{
-    uvm_parent_gpu_peer_t *peer_caps;
-    NvU8 peer_id;
-
-    if (local_gpu == remote_gpu) {
-        UVM_ASSERT(local_gpu->egm.enabled);
-        peer_id = local_gpu->egm.local_peer_id;
-    }
-    else {
-        UVM_ASSERT(remote_gpu->egm.enabled);
-
-        peer_caps = parent_gpu_peer_caps(local_gpu, remote_gpu);
-
-        // Comparison is based on min(gpu1, gpu2) logic for peer GPUs.
-        if (uvm_parent_id_cmp(local_gpu->id, remote_gpu->id) < 0)
-            peer_id = peer_caps->egm_peer_ids[0];
-        else
-            peer_id = peer_caps->egm_peer_ids[1];
-    }
-
-    return UVM_APERTURE_PEER(peer_id);
-}
+uvm_aperture_t uvm_gpu_peer_aperture(uvm_gpu_t *local_gpu,
+                                     uvm_gpu_t *remote_gpu) {
+  uvm_parent_gpu_peer_t *parent_peer_caps;
 
-NvU64 uvm_gpu_peer_ref_count(const uvm_gpu_t *gpu0, const uvm_gpu_t *gpu1)
-{
-    UVM_ASSERT(!uvm_gpus_are_smc_peers(gpu0, gpu1));
+  // MIG instances in the same physical GPU have vidmem addresses
+  if (uvm_gpus_are_smc_peers(local_gpu, remote_gpu))
+    return UVM_APERTURE_VID;
 
-    return gpu_peer_caps(gpu0, gpu1)->ref_count;
+  parent_peer_caps =
+      parent_gpu_peer_caps(local_gpu->parent, remote_gpu->parent);
+  return parent_gpu_peer_aperture(local_gpu->parent, remote_gpu->parent,
+                                  parent_peer_caps);
 }
 
-static bool gpu_address_is_coherent_peer(uvm_gpu_t *gpu, uvm_gpu_phys_address_t address)
-{
-    bool is_peer = false;
-    uvm_parent_gpu_t *parent_gpu;
-    phys_addr_t phys_addr;
-
-    if (address.aperture != UVM_APERTURE_SYS)
-        return false;
-
-    // GPU uses DMA addresses, which might be translated by IOMMU/SMMU,
-    // either inline, or via ATS.
-    phys_addr = dma_to_phys(&gpu->parent->pci_dev->dev, (dma_addr_t)address.address);
+uvm_gpu_phys_address_t uvm_gpu_peer_phys_address(uvm_gpu_t *owning_gpu,
+                                                 NvU64 address,
+                                                 uvm_gpu_t *accessing_gpu) {
+  uvm_aperture_t aperture = uvm_gpu_peer_aperture(accessing_gpu, owning_gpu);
 
-    // Exposed coherent vidmem can be accessed via sys aperture even without
-    // GPUs being explicit peers, so each parent GPU is a potential peer.
-    uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
-    for_each_parent_gpu(parent_gpu) {
+  if (uvm_parent_gpus_are_nvlink_direct_connected(accessing_gpu->parent,
+                                                  owning_gpu->parent)) {
+    UVM_ASSERT(uvm_aperture_is_peer(aperture));
+    address +=
+        owning_gpu->parent->peer_address_info.peer_gpa_memory_window_start;
+  } else if (uvm_parent_gpus_are_nvswitch_connected(accessing_gpu->parent,
+                                                    owning_gpu->parent)) {
+    UVM_ASSERT(uvm_aperture_is_peer(aperture));
+    address += owning_gpu->parent->nvswitch_info.fabric_memory_window_start;
+  } else if (uvm_aperture_is_sys(aperture)) {
+    // BAR1 P2P can use either coherent or non-coherent sysmem,
+    // depending on atomic capabilities of the peer devices.
+    uvm_parent_gpu_peer_t *parent_peer_caps =
+        parent_gpu_peer_caps(accessing_gpu->parent, owning_gpu->parent);
+    int peer_index =
+        (uvm_id_cmp(accessing_gpu->id, owning_gpu->id) < 0) ? 0 : 1;
 
-        if (parent_gpu == gpu->parent)
-            continue;
+    UVM_ASSERT(parent_peer_caps->link_type == UVM_GPU_LINK_PCIE_BAR1);
+    UVM_ASSERT(parent_peer_caps->bar1_p2p_dma_size[peer_index] != 0);
 
-        if (phys_addr >= parent_gpu->system_bus.memory_window_start &&
-            phys_addr <= parent_gpu->system_bus.memory_window_end) {
-            is_peer = true;
-            break;
-        }
-    }
-    uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
+    address += parent_peer_caps->bar1_p2p_dma_base_address[peer_index];
+  }
 
-    return is_peer;
+  return uvm_gpu_phys_address(aperture, address);
 }
 
-static bool gpu_phys_address_is_bar1p2p_peer(uvm_gpu_t *gpu, uvm_gpu_phys_address_t address)
-{
-    bool is_peer = false;
-    uvm_parent_processor_mask_t peer_parent_gpus;
-    uvm_parent_gpu_t *peer_parent_gpu;
-
-    // BAR1 P2P is accessed via sys aperture
-    if (!uvm_aperture_is_sys(address.aperture))
-        return false;
-
-    uvm_spin_lock(&gpu->peer_info.peer_gpu_lock);
-    uvm_parent_gpus_from_processor_mask(&peer_parent_gpus, &gpu->peer_info.peer_gpu_mask);
-    for_each_parent_gpu_in_mask(peer_parent_gpu, &peer_parent_gpus) {
-        const uvm_parent_gpu_peer_t *peer_caps = parent_gpu_peer_caps(gpu->parent, peer_parent_gpu);
-        const int peer_index = (uvm_parent_id_cmp(gpu->parent->id, peer_parent_gpu->id) < 0) ? 0 : 1;
-
-        UVM_ASSERT(peer_caps->ref_count > 0);
-        if (peer_caps->link_type != UVM_GPU_LINK_PCIE_BAR1)
-            continue;
-
-        if (address.address >= peer_caps->bar1_p2p_dma_base_address[peer_index] &&
-            address.address < (peer_caps->bar1_p2p_dma_base_address[peer_index] + peer_caps->bar1_p2p_dma_size[peer_index])) {
-            is_peer = true;
-            break;
-        }
-    }
-    uvm_spin_unlock(&gpu->peer_info.peer_gpu_lock);
+uvm_gpu_address_t uvm_gpu_peer_copy_address(uvm_gpu_t *owning_gpu,
+                                            NvU64 address,
+                                            uvm_gpu_t *accessing_gpu) {
+  uvm_gpu_identity_mapping_t *gpu_peer_mapping;
 
-    return is_peer;
-}
+  if (accessing_gpu->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_PHYSICAL)
+    return uvm_gpu_address_from_phys(
+        uvm_gpu_peer_phys_address(owning_gpu, address, accessing_gpu));
 
-bool uvm_gpu_address_is_peer(uvm_gpu_t *gpu, uvm_gpu_address_t address)
-{
-    if (address.is_virtual) {
-        // Virtual addresses can be used for peer copies only if
-        // peer_copy_mode == UVM_GPU_PEER_COPY_MODE_VIRTUAL
-        if (gpu->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_VIRTUAL) {
-            return (address.address >= gpu->parent->peer_va_base &&
-                    address.address < (gpu->parent->peer_va_base + gpu->parent->peer_va_size));
-        }
-    }
-    else {
-        if (uvm_aperture_is_peer(address.aperture)) {
-            uvm_parent_processor_mask_t peer_parent_gpus;
-            uvm_parent_gpu_t *peer_parent_gpu;
-
-            // Local EGM accesses don't go over NVLINK
-            if (gpu->parent->egm.enabled && address.aperture == gpu->parent->egm.local_peer_id)
-                return false;
-
-            uvm_spin_lock(&gpu->peer_info.peer_gpu_lock);
-            uvm_parent_gpus_from_processor_mask(&peer_parent_gpus, &gpu->peer_info.peer_gpu_mask);
-            for_each_parent_gpu_in_mask(peer_parent_gpu, &peer_parent_gpus) {
-                if (!peer_parent_gpu->egm.enabled)
-                    continue;
-
-                // EGM uses peer IDs but they are different from VIDMEM peer
-                // IDs.
-                // Check if the address aperture is an EGM aperture.
-                // We should not use remote EGM addresses internally until
-                // NVLINK STO handling is updated to handle EGM.
-                // TODO: Bug: 5068688 [UVM] Detect STO and prevent data leaks
-                //                    when accessing EGM memory
-                // TODO: Bug: 5007527 [UVM] Extend STO recovery to EGM enabled
-                //                    systems
-                UVM_ASSERT(address.aperture != uvm_gpu_egm_peer_aperture(gpu->parent, peer_parent_gpu));
-            }
-
-            uvm_spin_unlock(&gpu->peer_info.peer_gpu_lock);
-
-            return true;
-        }
-        else if (uvm_aperture_is_sys(address.aperture)) {
-            // SYS aperture is used for coherent peers or BAR1 P2P.
-            // SYS_NON_COHERNET aperture is used for BAR1 P2P.
-            uvm_gpu_phys_address_t phys_addr = uvm_gpu_phys_address(address.aperture, address.address);
-            return gpu_address_is_coherent_peer(gpu, phys_addr) || gpu_phys_address_is_bar1p2p_peer(gpu, phys_addr);
-        }
-
-        UVM_ASSERT(address.aperture == UVM_APERTURE_VID);
-    }
+  UVM_ASSERT(accessing_gpu->parent->peer_copy_mode ==
+             UVM_GPU_PEER_COPY_MODE_VIRTUAL);
+  gpu_peer_mapping = uvm_gpu_get_peer_mapping(accessing_gpu, owning_gpu->id);
 
-    return false;
+  return uvm_gpu_address_virtual(gpu_peer_mapping->base + address);
 }
 
-uvm_aperture_t uvm_get_page_tree_location(const uvm_gpu_t *gpu)
-{
-    // See comments in page_tree_set_location
-    if (uvm_parent_gpu_is_virt_mode_sriov_heavy(gpu->parent) || g_uvm_global.conf_computing_enabled)
-        return UVM_APERTURE_VID;
-
-    // Sysmem is represented by UVM_APERTURE_SYS
-    if (!gpu->mem_info.size)
-        return UVM_APERTURE_SYS;
-
-
-    return UVM_APERTURE_DEFAULT;
-}
+uvm_aperture_t uvm_gpu_egm_peer_aperture(uvm_parent_gpu_t *local_gpu,
+                                         uvm_parent_gpu_t *remote_gpu) {
+  uvm_parent_gpu_peer_t *peer_caps;
+  NvU8 peer_id;
 
-static NvU64 instance_ptr_to_key(uvm_gpu_phys_address_t instance_ptr)
-{
-    NvU64 key;
-    int is_sys = (instance_ptr.aperture == UVM_APERTURE_SYS);
+  if (local_gpu == remote_gpu) {
+    UVM_ASSERT(local_gpu->egm.enabled);
+    peer_id = local_gpu->egm.local_peer_id;
+  } else {
+    UVM_ASSERT(remote_gpu->egm.enabled);
 
-    // Instance pointers must be 4k aligned and they must have either VID or SYS
-    // apertures. Compress them as much as we can both to guarantee that the key
-    // fits within 64 bits, and to make the key space as small as possible.
-    UVM_ASSERT(IS_ALIGNED(instance_ptr.address, UVM_PAGE_SIZE_4K));
-    UVM_ASSERT(instance_ptr.aperture == UVM_APERTURE_VID || instance_ptr.aperture == UVM_APERTURE_SYS);
+    peer_caps = parent_gpu_peer_caps(local_gpu, remote_gpu);
 
-    key = (instance_ptr.address >> 11) | is_sys;
+    // Comparison is based on min(gpu1, gpu2) logic for peer GPUs.
+    if (uvm_parent_id_cmp(local_gpu->id, remote_gpu->id) < 0)
+      peer_id = peer_caps->egm_peer_ids[0];
+    else
+      peer_id = peer_caps->egm_peer_ids[1];
+  }
 
-    return key;
+  return UVM_APERTURE_PEER(peer_id);
 }
 
-static NV_STATUS parent_gpu_add_user_channel_subctx_info(uvm_parent_gpu_t *parent_gpu,
-                                                         uvm_user_channel_t *user_channel)
-{
-    uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
-    NV_STATUS status = NV_OK;
-    uvm_rb_tree_node_t *channel_tree_node;
-    NvU64 node_key;
-    uvm_user_channel_subctx_info_t *channel_subctx_info;
-    uvm_user_channel_subctx_info_t *new_channel_subctx_info = NULL;
-    uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
-
-    if (!user_channel->in_subctx)
-        return NV_OK;
-
-    // Pre-allocate a subcontext info descriptor out of the lock, in case we
-    // need to add a new entry to the tree
-    new_channel_subctx_info = uvm_kvmalloc_zero(sizeof(*new_channel_subctx_info));
-
-    // Don't check for the result of the allocation since it is only needed
-    // if the TSG has not been registered yet, and we do that under the lock
-    // below
-    if (new_channel_subctx_info) {
-        new_channel_subctx_info->subctxs =
-            uvm_kvmalloc_zero(sizeof(*new_channel_subctx_info->subctxs) * user_channel->tsg.max_subctx_count);
-    }
-
-    uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+NvU64 uvm_gpu_peer_ref_count(const uvm_gpu_t *gpu0, const uvm_gpu_t *gpu1) {
+  UVM_ASSERT(!uvm_gpus_are_smc_peers(gpu0, gpu1));
 
-    // Check if the subcontext information for the channel already exists
-    node_key = ((NvU64)user_channel->hw_runlist_id << 32) | user_channel->tsg.id;
-    channel_tree_node = uvm_rb_tree_find(&parent_gpu->tsg_table, node_key);
-
-    if (!channel_tree_node) {
-        // We could not allocate the descriptor before taking the lock. Exiting
-        if (!new_channel_subctx_info || !new_channel_subctx_info->subctxs) {
-            status = NV_ERR_NO_MEMORY;
-            goto exit_unlock;
-        }
-
-        // Insert the new subcontext information descriptor
-        new_channel_subctx_info->node.key = node_key;
-        status = uvm_rb_tree_insert(&parent_gpu->tsg_table, &new_channel_subctx_info->node);
-        UVM_ASSERT(status == NV_OK);
-
-        channel_subctx_info = new_channel_subctx_info;
-        channel_subctx_info->smc_engine_id = user_channel->smc_engine_id;
-    }
-    else {
-        channel_subctx_info = container_of(channel_tree_node, uvm_user_channel_subctx_info_t, node);
-        UVM_ASSERT(channel_subctx_info->smc_engine_id == user_channel->smc_engine_id);
-    }
-
-    user_channel->subctx_info = channel_subctx_info;
-
-    // Register the GPU VA space of the channel subcontext info descriptor, or
-    // check that the existing one matches the channel's
-    if (channel_subctx_info->subctxs[user_channel->subctx_id].refcount++ > 0) {
-        UVM_ASSERT_MSG(channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space == gpu_va_space,
-                       "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: expected GPU VA space 0x%llx but got 0x%llx instead\n",
-                       user_channel->hw_runlist_id,
-                       user_channel->hw_channel_id,
-                       instance_ptr.address,
-                       uvm_aperture_string(instance_ptr.aperture),
-                       user_channel->subctx_id,
-                       user_channel->tsg.id,
-                       (NvU64)gpu_va_space,
-                       (NvU64)channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space);
-        UVM_ASSERT_MSG(channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space != NULL,
-                       "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: GPU VA space is NULL\n",
-                       user_channel->hw_runlist_id,
-                       user_channel->hw_channel_id,
-                       instance_ptr.address,
-                       uvm_aperture_string(instance_ptr.aperture),
-                       user_channel->subctx_id,
-                       user_channel->tsg.id);
-        UVM_ASSERT_MSG(channel_subctx_info->total_refcount > 0,
-                       "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: TSG refcount is 0\n",
-                       user_channel->hw_runlist_id,
-                       user_channel->hw_channel_id,
-                       instance_ptr.address,
-                       uvm_aperture_string(instance_ptr.aperture),
-                       user_channel->subctx_id,
-                       user_channel->tsg.id);
-    }
-    else {
-        UVM_ASSERT_MSG(channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space == NULL,
-                       "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: expected GPU VA space NULL but got 0x%llx instead\n",
-                       user_channel->hw_runlist_id,
-                       user_channel->hw_channel_id,
-                       instance_ptr.address,
-                       uvm_aperture_string(instance_ptr.aperture),
-                       user_channel->subctx_id,
-                       user_channel->tsg.id,
-                       (NvU64)channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space);
-
-        channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space = gpu_va_space;
-    }
-
-    ++channel_subctx_info->total_refcount;
-
-exit_unlock:
-    uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
-
-    // Remove the pre-allocated per-TSG subctx information struct if there was
-    // some error or it was not used
-    if (status != NV_OK || user_channel->subctx_info != new_channel_subctx_info) {
-        if (new_channel_subctx_info)
-            uvm_kvfree(new_channel_subctx_info->subctxs);
-
-        uvm_kvfree(new_channel_subctx_info);
-    }
-
-    return status;
+  return gpu_peer_caps(gpu0, gpu1)->ref_count;
 }
 
-static void parent_gpu_remove_user_channel_subctx_info_locked(uvm_parent_gpu_t *parent_gpu,
-                                                              uvm_user_channel_t *user_channel)
-{
-    uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
-    uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
+static bool gpu_address_is_coherent_peer(uvm_gpu_t *gpu,
+                                         uvm_gpu_phys_address_t address) {
+  bool is_peer = false;
+  uvm_parent_gpu_t *parent_gpu;
+  phys_addr_t phys_addr;
 
-    uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
-
-    // Channel subcontext info descriptor may not have been registered in
-    // tsg_table since this function is called in some teardown paths during
-    // channel creation
-    if (!user_channel->subctx_info)
-        return;
+  if (address.aperture != UVM_APERTURE_SYS)
+    return false;
 
-    UVM_ASSERT_MSG(&user_channel->subctx_info->node ==
-                   uvm_rb_tree_find(&parent_gpu->tsg_table, user_channel->subctx_info->node.key),
-                   "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: SubCTX not found in TSG table\n",
-                   user_channel->hw_runlist_id,
-                   user_channel->hw_channel_id,
-                   instance_ptr.address,
-                   uvm_aperture_string(instance_ptr.aperture),
-                   user_channel->subctx_id,
-                   user_channel->tsg.id);
+  // GPU uses DMA addresses, which might be translated by IOMMU/SMMU,
+  // either inline, or via ATS.
+  phys_addr =
+      dma_to_phys(&gpu->parent->pci_dev->dev, (dma_addr_t)address.address);
 
-    UVM_ASSERT_MSG(user_channel->subctx_info->subctxs[user_channel->subctx_id].refcount > 0,
-                   "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: SubCTX refcount is 0\n",
-                   user_channel->hw_runlist_id,
-                   user_channel->hw_channel_id,
-                   instance_ptr.address,
-                   uvm_aperture_string(instance_ptr.aperture),
-                   user_channel->subctx_id,
-                   user_channel->tsg.id);
-
-    UVM_ASSERT_MSG(user_channel->subctx_info->subctxs[user_channel->subctx_id].gpu_va_space == gpu_va_space,
-                   "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: "
-                   "expected GPU VA space 0x%llx but got 0x%llx instead\n",
-                   user_channel->hw_runlist_id,
-                   user_channel->hw_channel_id,
-                   instance_ptr.address,
-                   uvm_aperture_string(instance_ptr.aperture),
-                   user_channel->subctx_id,
-                   user_channel->tsg.id,
-                   (NvU64)gpu_va_space,
-                   (NvU64)user_channel->subctx_info->subctxs[user_channel->subctx_id].gpu_va_space);
-
-    UVM_ASSERT_MSG(user_channel->subctx_info->total_refcount > 0,
-                   "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: TSG refcount is 0\n",
-                   user_channel->hw_runlist_id,
-                   user_channel->hw_channel_id,
-                   instance_ptr.address,
-                   uvm_aperture_string(instance_ptr.aperture),
-                   user_channel->subctx_id,
-                   user_channel->tsg.id);
+  // Exposed coherent vidmem can be accessed via sys aperture even without
+  // GPUs being explicit peers, so each parent GPU is a potential peer.
+  uvm_spin_lock_irqsave(&g_uvm_global.gpu_table_lock);
+  for_each_parent_gpu(parent_gpu) {
 
-    // Decrement VA space refcount. If it gets to zero, unregister the pointer
-    if (--user_channel->subctx_info->subctxs[user_channel->subctx_id].refcount == 0)
-        user_channel->subctx_info->subctxs[user_channel->subctx_id].gpu_va_space = NULL;
+    if (parent_gpu == gpu->parent)
+      continue;
 
-    if (--user_channel->subctx_info->total_refcount == 0) {
-        uvm_rb_tree_remove(&parent_gpu->tsg_table, &user_channel->subctx_info->node);
-        uvm_kvfree(user_channel->subctx_info->subctxs);
-        uvm_kvfree(user_channel->subctx_info);
+    if (phys_addr >= parent_gpu->system_bus.memory_window_start &&
+        phys_addr <= parent_gpu->system_bus.memory_window_end) {
+      is_peer = true;
+      break;
     }
+  }
+  uvm_spin_unlock_irqrestore(&g_uvm_global.gpu_table_lock);
 
-    user_channel->subctx_info = NULL;
+  return is_peer;
 }
 
-static void parent_gpu_add_user_channel_instance_ptr(uvm_parent_gpu_t *parent_gpu,
-                                                     uvm_user_channel_t *user_channel)
-{
-    uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
-    NvU64 instance_ptr_key = instance_ptr_to_key(instance_ptr);
-    NV_STATUS status;
+static bool gpu_phys_address_is_bar1p2p_peer(uvm_gpu_t *gpu,
+                                             uvm_gpu_phys_address_t address) {
+  bool is_peer = false;
+  uvm_parent_processor_mask_t peer_parent_gpus;
+  uvm_parent_gpu_t *peer_parent_gpu;
 
-    uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+  // BAR1 P2P is accessed via sys aperture
+  if (!uvm_aperture_is_sys(address.aperture))
+    return false;
 
-    // Insert the instance_ptr -> user_channel mapping
-    user_channel->instance_ptr.node.key = instance_ptr_key;
-    status = uvm_rb_tree_insert(&parent_gpu->instance_ptr_table, &user_channel->instance_ptr.node);
+  uvm_spin_lock(&gpu->peer_info.peer_gpu_lock);
+  uvm_parent_gpus_from_processor_mask(&peer_parent_gpus,
+                                      &gpu->peer_info.peer_gpu_mask);
+  for_each_parent_gpu_in_mask(peer_parent_gpu, &peer_parent_gpus) {
+    const uvm_parent_gpu_peer_t *peer_caps =
+        parent_gpu_peer_caps(gpu->parent, peer_parent_gpu);
+    const int peer_index =
+        (uvm_parent_id_cmp(gpu->parent->id, peer_parent_gpu->id) < 0) ? 0 : 1;
+
+    UVM_ASSERT(peer_caps->ref_count > 0);
+    if (peer_caps->link_type != UVM_GPU_LINK_PCIE_BAR1)
+      continue;
+
+    if (address.address >= peer_caps->bar1_p2p_dma_base_address[peer_index] &&
+        address.address < (peer_caps->bar1_p2p_dma_base_address[peer_index] +
+                           peer_caps->bar1_p2p_dma_size[peer_index])) {
+      is_peer = true;
+      break;
+    }
+  }
+  uvm_spin_unlock(&gpu->peer_info.peer_gpu_lock);
+
+  return is_peer;
+}
+
+bool uvm_gpu_address_is_peer(uvm_gpu_t *gpu, uvm_gpu_address_t address) {
+  if (address.is_virtual) {
+    // Virtual addresses can be used for peer copies only if
+    // peer_copy_mode == UVM_GPU_PEER_COPY_MODE_VIRTUAL
+    if (gpu->parent->peer_copy_mode == UVM_GPU_PEER_COPY_MODE_VIRTUAL) {
+      return (address.address >= gpu->parent->peer_va_base &&
+              address.address <
+                  (gpu->parent->peer_va_base + gpu->parent->peer_va_size));
+    }
+  } else {
+    if (uvm_aperture_is_peer(address.aperture)) {
+      uvm_parent_processor_mask_t peer_parent_gpus;
+      uvm_parent_gpu_t *peer_parent_gpu;
+
+      // Local EGM accesses don't go over NVLINK
+      if (gpu->parent->egm.enabled &&
+          address.aperture == gpu->parent->egm.local_peer_id)
+        return false;
 
-    uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
+      uvm_spin_lock(&gpu->peer_info.peer_gpu_lock);
+      uvm_parent_gpus_from_processor_mask(&peer_parent_gpus,
+                                          &gpu->peer_info.peer_gpu_mask);
+      for_each_parent_gpu_in_mask(peer_parent_gpu, &peer_parent_gpus) {
+        if (!peer_parent_gpu->egm.enabled)
+          continue;
+
+        // EGM uses peer IDs but they are different from VIDMEM peer
+        // IDs.
+        // Check if the address aperture is an EGM aperture.
+        // We should not use remote EGM addresses internally until
+        // NVLINK STO handling is updated to handle EGM.
+        // TODO: Bug: 5068688 [UVM] Detect STO and prevent data leaks
+        //                    when accessing EGM memory
+        // TODO: Bug: 5007527 [UVM] Extend STO recovery to EGM enabled
+        //                    systems
+        UVM_ASSERT(address.aperture !=
+                   uvm_gpu_egm_peer_aperture(gpu->parent, peer_parent_gpu));
+      }
+
+      uvm_spin_unlock(&gpu->peer_info.peer_gpu_lock);
+
+      return true;
+    } else if (uvm_aperture_is_sys(address.aperture)) {
+      // SYS aperture is used for coherent peers or BAR1 P2P.
+      // SYS_NON_COHERNET aperture is used for BAR1 P2P.
+      uvm_gpu_phys_address_t phys_addr =
+          uvm_gpu_phys_address(address.aperture, address.address);
+      return gpu_address_is_coherent_peer(gpu, phys_addr) ||
+             gpu_phys_address_is_bar1p2p_peer(gpu, phys_addr);
+    }
+
+    UVM_ASSERT(address.aperture == UVM_APERTURE_VID);
+  }
+
+  return false;
+}
+
+uvm_aperture_t uvm_get_page_tree_location(const uvm_gpu_t *gpu) {
+  // See comments in page_tree_set_location
+  if (uvm_parent_gpu_is_virt_mode_sriov_heavy(gpu->parent) ||
+      g_uvm_global.conf_computing_enabled)
+    return UVM_APERTURE_VID;
+
+  // Sysmem is represented by UVM_APERTURE_SYS
+  if (!gpu->mem_info.size)
+    return UVM_APERTURE_SYS;
+
+  return UVM_APERTURE_DEFAULT;
+}
+
+static NvU64 instance_ptr_to_key(uvm_gpu_phys_address_t instance_ptr) {
+  NvU64 key;
+  int is_sys = (instance_ptr.aperture == UVM_APERTURE_SYS);
+
+  // Instance pointers must be 4k aligned and they must have either VID or SYS
+  // apertures. Compress them as much as we can both to guarantee that the key
+  // fits within 64 bits, and to make the key space as small as possible.
+  UVM_ASSERT(IS_ALIGNED(instance_ptr.address, UVM_PAGE_SIZE_4K));
+  UVM_ASSERT(instance_ptr.aperture == UVM_APERTURE_VID ||
+             instance_ptr.aperture == UVM_APERTURE_SYS);
+
+  key = (instance_ptr.address >> 11) | is_sys;
+
+  return key;
+}
+
+static NV_STATUS
+parent_gpu_add_user_channel_subctx_info(uvm_parent_gpu_t *parent_gpu,
+                                        uvm_user_channel_t *user_channel) {
+  uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
+  NV_STATUS status = NV_OK;
+  uvm_rb_tree_node_t *channel_tree_node;
+  NvU64 node_key;
+  uvm_user_channel_subctx_info_t *channel_subctx_info;
+  uvm_user_channel_subctx_info_t *new_channel_subctx_info = NULL;
+  uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
+
+  if (!user_channel->in_subctx)
+    return NV_OK;
 
-    UVM_ASSERT_MSG(status == NV_OK, "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: error %s\n",
-                   user_channel->hw_runlist_id,
-                   user_channel->hw_channel_id,
+  // Pre-allocate a subcontext info descriptor out of the lock, in case we
+  // need to add a new entry to the tree
+  new_channel_subctx_info = uvm_kvmalloc_zero(sizeof(*new_channel_subctx_info));
+
+  // Don't check for the result of the allocation since it is only needed
+  // if the TSG has not been registered yet, and we do that under the lock
+  // below
+  if (new_channel_subctx_info) {
+    new_channel_subctx_info->subctxs =
+        uvm_kvmalloc_zero(sizeof(*new_channel_subctx_info->subctxs) *
+                          user_channel->tsg.max_subctx_count);
+  }
+
+  uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+
+  // Check if the subcontext information for the channel already exists
+  node_key = ((NvU64)user_channel->hw_runlist_id << 32) | user_channel->tsg.id;
+  channel_tree_node = uvm_rb_tree_find(&parent_gpu->tsg_table, node_key);
+
+  if (!channel_tree_node) {
+    // We could not allocate the descriptor before taking the lock. Exiting
+    if (!new_channel_subctx_info || !new_channel_subctx_info->subctxs) {
+      status = NV_ERR_NO_MEMORY;
+      goto exit_unlock;
+    }
+
+    // Insert the new subcontext information descriptor
+    new_channel_subctx_info->node.key = node_key;
+    status = uvm_rb_tree_insert(&parent_gpu->tsg_table,
+                                &new_channel_subctx_info->node);
+    UVM_ASSERT(status == NV_OK);
+
+    channel_subctx_info = new_channel_subctx_info;
+    channel_subctx_info->smc_engine_id = user_channel->smc_engine_id;
+  } else {
+    channel_subctx_info =
+        container_of(channel_tree_node, uvm_user_channel_subctx_info_t, node);
+    UVM_ASSERT(channel_subctx_info->smc_engine_id ==
+               user_channel->smc_engine_id);
+  }
+
+  user_channel->subctx_info = channel_subctx_info;
+
+  // Register the GPU VA space of the channel subcontext info descriptor, or
+  // check that the existing one matches the channel's
+  if (channel_subctx_info->subctxs[user_channel->subctx_id].refcount++ > 0) {
+    UVM_ASSERT_MSG(
+        channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space ==
+            gpu_va_space,
+        "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: expected GPU "
+        "VA space 0x%llx but got 0x%llx instead\n",
+        user_channel->hw_runlist_id, user_channel->hw_channel_id,
+        instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+        user_channel->subctx_id, user_channel->tsg.id, (NvU64)gpu_va_space,
+        (NvU64)channel_subctx_info->subctxs[user_channel->subctx_id]
+            .gpu_va_space);
+    UVM_ASSERT_MSG(
+        channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space !=
+            NULL,
+        "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: GPU VA space "
+        "is NULL\n",
+        user_channel->hw_runlist_id, user_channel->hw_channel_id,
+        instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+        user_channel->subctx_id, user_channel->tsg.id);
+    UVM_ASSERT_MSG(channel_subctx_info->total_refcount > 0,
+                   "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: TSG "
+                   "refcount is 0\n",
+                   user_channel->hw_runlist_id, user_channel->hw_channel_id,
                    instance_ptr.address,
                    uvm_aperture_string(instance_ptr.aperture),
-                   user_channel->subctx_id,
-                   user_channel->tsg.id,
-                   nvstatusToString(status));
-}
-
-static void parent_gpu_remove_user_channel_instance_ptr_locked(uvm_parent_gpu_t *parent_gpu,
-                                                               uvm_user_channel_t *user_channel)
-{
-    uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
+                   user_channel->subctx_id, user_channel->tsg.id);
+  } else {
+    UVM_ASSERT_MSG(
+        channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space ==
+            NULL,
+        "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: expected GPU "
+        "VA space NULL but got 0x%llx instead\n",
+        user_channel->hw_runlist_id, user_channel->hw_channel_id,
+        instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+        user_channel->subctx_id, user_channel->tsg.id,
+        (NvU64)channel_subctx_info->subctxs[user_channel->subctx_id]
+            .gpu_va_space);
+
+    channel_subctx_info->subctxs[user_channel->subctx_id].gpu_va_space =
+        gpu_va_space;
+  }
+
+  ++channel_subctx_info->total_refcount;
 
-    if (UVM_RB_TREE_EMPTY_NODE(&user_channel->instance_ptr.node))
-        return;
-
-    uvm_rb_tree_remove(&parent_gpu->instance_ptr_table, &user_channel->instance_ptr.node);
+exit_unlock:
+  uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
+
+  // Remove the pre-allocated per-TSG subctx information struct if there was
+  // some error or it was not used
+  if (status != NV_OK || user_channel->subctx_info != new_channel_subctx_info) {
+    if (new_channel_subctx_info)
+      uvm_kvfree(new_channel_subctx_info->subctxs);
+
+    uvm_kvfree(new_channel_subctx_info);
+  }
+
+  return status;
+}
+
+static void parent_gpu_remove_user_channel_subctx_info_locked(
+    uvm_parent_gpu_t *parent_gpu, uvm_user_channel_t *user_channel) {
+  uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
+  uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
+
+  uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
+
+  // Channel subcontext info descriptor may not have been registered in
+  // tsg_table since this function is called in some teardown paths during
+  // channel creation
+  if (!user_channel->subctx_info)
+    return;
+
+  UVM_ASSERT_MSG(&user_channel->subctx_info->node ==
+                     uvm_rb_tree_find(&parent_gpu->tsg_table,
+                                      user_channel->subctx_info->node.key),
+                 "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: "
+                 "SubCTX not found in TSG table\n",
+                 user_channel->hw_runlist_id, user_channel->hw_channel_id,
+                 instance_ptr.address,
+                 uvm_aperture_string(instance_ptr.aperture),
+                 user_channel->subctx_id, user_channel->tsg.id);
+
+  UVM_ASSERT_MSG(
+      user_channel->subctx_info->subctxs[user_channel->subctx_id].refcount > 0,
+      "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: SubCTX refcount "
+      "is 0\n",
+      user_channel->hw_runlist_id, user_channel->hw_channel_id,
+      instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+      user_channel->subctx_id, user_channel->tsg.id);
+
+  UVM_ASSERT_MSG(
+      user_channel->subctx_info->subctxs[user_channel->subctx_id]
+              .gpu_va_space == gpu_va_space,
+      "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: "
+      "expected GPU VA space 0x%llx but got 0x%llx instead\n",
+      user_channel->hw_runlist_id, user_channel->hw_channel_id,
+      instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+      user_channel->subctx_id, user_channel->tsg.id, (NvU64)gpu_va_space,
+      (NvU64)user_channel->subctx_info->subctxs[user_channel->subctx_id]
+          .gpu_va_space);
+
+  UVM_ASSERT_MSG(user_channel->subctx_info->total_refcount > 0,
+                 "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: TSG "
+                 "refcount is 0\n",
+                 user_channel->hw_runlist_id, user_channel->hw_channel_id,
+                 instance_ptr.address,
+                 uvm_aperture_string(instance_ptr.aperture),
+                 user_channel->subctx_id, user_channel->tsg.id);
+
+  // Decrement VA space refcount. If it gets to zero, unregister the pointer
+  if (--user_channel->subctx_info->subctxs[user_channel->subctx_id].refcount ==
+      0)
+    user_channel->subctx_info->subctxs[user_channel->subctx_id].gpu_va_space =
+        NULL;
+
+  if (--user_channel->subctx_info->total_refcount == 0) {
+    uvm_rb_tree_remove(&parent_gpu->tsg_table,
+                       &user_channel->subctx_info->node);
+    uvm_kvfree(user_channel->subctx_info->subctxs);
+    uvm_kvfree(user_channel->subctx_info);
+  }
+
+  user_channel->subctx_info = NULL;
 }
 
-NV_STATUS uvm_parent_gpu_add_user_channel(uvm_parent_gpu_t *parent_gpu, uvm_user_channel_t *user_channel)
-{
-    uvm_va_space_t *va_space;
-    uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
-    NV_STATUS status;
+static void
+parent_gpu_add_user_channel_instance_ptr(uvm_parent_gpu_t *parent_gpu,
+                                         uvm_user_channel_t *user_channel) {
+  uvm_gpu_phys_address_t instance_ptr = user_channel->instance_ptr.addr;
+  NvU64 instance_ptr_key = instance_ptr_to_key(instance_ptr);
+  NV_STATUS status;
 
-    UVM_ASSERT(user_channel->rm_retained_channel);
-    UVM_ASSERT(gpu_va_space);
-    UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) == UVM_GPU_VA_SPACE_STATE_ACTIVE);
-    va_space = gpu_va_space->va_space;
-    uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
 
-    status = parent_gpu_add_user_channel_subctx_info(parent_gpu, user_channel);
-    if (status != NV_OK)
-        return status;
+  // Insert the instance_ptr -> user_channel mapping
+  user_channel->instance_ptr.node.key = instance_ptr_key;
+  status = uvm_rb_tree_insert(&parent_gpu->instance_ptr_table,
+                              &user_channel->instance_ptr.node);
 
-    parent_gpu_add_user_channel_instance_ptr(parent_gpu, user_channel);
+  uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
 
-    return NV_OK;
+  UVM_ASSERT_MSG(
+      status == NV_OK,
+      "CH %u:%u instance_ptr {0x%llx:%s} SubCTX %u in TSG %u: error %s\n",
+      user_channel->hw_runlist_id, user_channel->hw_channel_id,
+      instance_ptr.address, uvm_aperture_string(instance_ptr.aperture),
+      user_channel->subctx_id, user_channel->tsg.id, nvstatusToString(status));
 }
 
-static uvm_user_channel_t *instance_ptr_to_user_channel(uvm_parent_gpu_t *parent_gpu,
-                                                        uvm_gpu_phys_address_t instance_ptr)
-{
-    NvU64 key = instance_ptr_to_key(instance_ptr);
-    uvm_rb_tree_node_t *instance_node;
-
-    uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
+static void parent_gpu_remove_user_channel_instance_ptr_locked(
+    uvm_parent_gpu_t *parent_gpu, uvm_user_channel_t *user_channel) {
+  uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
 
-    instance_node = uvm_rb_tree_find(&parent_gpu->instance_ptr_table, key);
-    if (!instance_node)
-        return NULL;
+  if (UVM_RB_TREE_EMPTY_NODE(&user_channel->instance_ptr.node))
+    return;
 
-    return get_user_channel(instance_node);
+  uvm_rb_tree_remove(&parent_gpu->instance_ptr_table,
+                     &user_channel->instance_ptr.node);
 }
 
-static uvm_gpu_va_space_t *user_channel_and_subctx_to_gpu_va_space(uvm_user_channel_t *user_channel, NvU32 subctx_id)
-{
-    uvm_user_channel_subctx_info_t *channel_subctx_info;
+NV_STATUS uvm_parent_gpu_add_user_channel(uvm_parent_gpu_t *parent_gpu,
+                                          uvm_user_channel_t *user_channel) {
+  uvm_va_space_t *va_space;
+  uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
+  NV_STATUS status;
 
-    UVM_ASSERT(user_channel);
-    UVM_ASSERT(user_channel->in_subctx);
-    UVM_ASSERT(user_channel->subctx_info);
+  UVM_ASSERT(user_channel->rm_retained_channel);
+  UVM_ASSERT(gpu_va_space);
+  UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) ==
+             UVM_GPU_VA_SPACE_STATE_ACTIVE);
+  va_space = gpu_va_space->va_space;
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    uvm_assert_spinlock_locked(&user_channel->gpu->parent->instance_ptr_table_lock);
-
-    channel_subctx_info = user_channel->subctx_info;
+  status = parent_gpu_add_user_channel_subctx_info(parent_gpu, user_channel);
+  if (status != NV_OK)
+    return status;
 
-    UVM_ASSERT_MSG(subctx_id < user_channel->tsg.max_subctx_count,
-                   "instance_ptr {0x%llx:%s} in TSG %u. Invalid SubCTX %u\n",
-                   user_channel->instance_ptr.addr.address,
-                   uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
-                   user_channel->tsg.id,
-                   subctx_id);
-    UVM_ASSERT_MSG(channel_subctx_info->total_refcount > 0,
-                   "instance_ptr {0x%llx:%s} in TSG %u: TSG refcount is 0\n",
-                   user_channel->instance_ptr.addr.address,
-                   uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
-                   user_channel->tsg.id);
-
-    // A subcontext's refcount can be zero if that subcontext is torn down
-    // uncleanly and work from that subcontext continues running with work from
-    // other subcontexts.
-    if (channel_subctx_info->subctxs[subctx_id].refcount == 0) {
-        UVM_ASSERT(channel_subctx_info->subctxs[subctx_id].gpu_va_space == NULL);
-    }
-    else {
-        UVM_ASSERT_MSG(channel_subctx_info->subctxs[subctx_id].gpu_va_space,
-                       "instance_ptr {0x%llx:%s} in TSG %u: no GPU VA space for SubCTX %u\n",
-                       user_channel->instance_ptr.addr.address,
-                       uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
-                       user_channel->tsg.id,
-                       subctx_id);
-    }
+  parent_gpu_add_user_channel_instance_ptr(parent_gpu, user_channel);
 
-    return channel_subctx_info->subctxs[subctx_id].gpu_va_space;
+  return NV_OK;
 }
 
-NV_STATUS uvm_parent_gpu_fault_entry_to_va_space(uvm_parent_gpu_t *parent_gpu,
-                                                 const uvm_fault_buffer_entry_t *fault,
-                                                 uvm_va_space_t **out_va_space,
-                                                 uvm_gpu_t **out_gpu)
-{
-    uvm_user_channel_t *user_channel;
-    uvm_gpu_va_space_t *gpu_va_space;
-    NV_STATUS status = NV_OK;
-
-    *out_va_space = NULL;
-    *out_gpu = NULL;
-
-    uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
-
-    user_channel = instance_ptr_to_user_channel(parent_gpu, fault->instance_ptr);
-    if (!user_channel) {
-        status = NV_ERR_INVALID_CHANNEL;
-        goto exit_unlock;
-    }
-
-    // Faults from HUB clients will always report VEID 0 even if the channel
-    // belongs a TSG with many subcontexts. Therefore, we cannot use the per-TSG
-    // subctx table and we need to directly return the channel's VA space
-    if (!user_channel->in_subctx || (fault->fault_source.client_type == UVM_FAULT_CLIENT_TYPE_HUB)) {
-        UVM_ASSERT_MSG(fault->fault_source.ve_id == 0,
-                       "Fault packet contains SubCTX %u for channel not in subctx\n",
-                       fault->fault_source.ve_id);
-
-        // We can safely access user_channel->gpu_va_space under the
-        // instance_ptr_table_lock since gpu_va_space is set to NULL after this
-        // function is called in uvm_user_channel_detach
-        gpu_va_space = user_channel->gpu_va_space;
-        UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) == UVM_GPU_VA_SPACE_STATE_ACTIVE);
-        *out_va_space = gpu_va_space->va_space;
-        *out_gpu = gpu_va_space->gpu;
-    }
-    else {
-        NvU32 ve_id = fault->fault_source.ve_id;
-
-        // Compute the SMC engine-local VEID
-        UVM_ASSERT(ve_id >= user_channel->smc_engine_ve_id_offset);
+static uvm_user_channel_t *
+instance_ptr_to_user_channel(uvm_parent_gpu_t *parent_gpu,
+                             uvm_gpu_phys_address_t instance_ptr) {
+  NvU64 key = instance_ptr_to_key(instance_ptr);
+  uvm_rb_tree_node_t *instance_node;
 
-        ve_id -= user_channel->smc_engine_ve_id_offset;
+  uvm_assert_spinlock_locked(&parent_gpu->instance_ptr_table_lock);
 
-        gpu_va_space = user_channel_and_subctx_to_gpu_va_space(user_channel, ve_id);
+  instance_node = uvm_rb_tree_find(&parent_gpu->instance_ptr_table, key);
+  if (!instance_node)
+    return NULL;
 
-        // Instance pointer is valid but the fault targets a non-existent
-        // subcontext.
-        if (gpu_va_space) {
-            *out_va_space = gpu_va_space->va_space;
-            *out_gpu = gpu_va_space->gpu;
-        }
-        else {
-            status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
-        }
-    }
+  return get_user_channel(instance_node);
+}
+
+static uvm_gpu_va_space_t *
+user_channel_and_subctx_to_gpu_va_space(uvm_user_channel_t *user_channel,
+                                        NvU32 subctx_id) {
+  uvm_user_channel_subctx_info_t *channel_subctx_info;
+
+  UVM_ASSERT(user_channel);
+  UVM_ASSERT(user_channel->in_subctx);
+  UVM_ASSERT(user_channel->subctx_info);
+
+  uvm_assert_spinlock_locked(
+      &user_channel->gpu->parent->instance_ptr_table_lock);
+
+  channel_subctx_info = user_channel->subctx_info;
+
+  UVM_ASSERT_MSG(subctx_id < user_channel->tsg.max_subctx_count,
+                 "instance_ptr {0x%llx:%s} in TSG %u. Invalid SubCTX %u\n",
+                 user_channel->instance_ptr.addr.address,
+                 uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
+                 user_channel->tsg.id, subctx_id);
+  UVM_ASSERT_MSG(channel_subctx_info->total_refcount > 0,
+                 "instance_ptr {0x%llx:%s} in TSG %u: TSG refcount is 0\n",
+                 user_channel->instance_ptr.addr.address,
+                 uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
+                 user_channel->tsg.id);
+
+  // A subcontext's refcount can be zero if that subcontext is torn down
+  // uncleanly and work from that subcontext continues running with work from
+  // other subcontexts.
+  if (channel_subctx_info->subctxs[subctx_id].refcount == 0) {
+    UVM_ASSERT(channel_subctx_info->subctxs[subctx_id].gpu_va_space == NULL);
+  } else {
+    UVM_ASSERT_MSG(
+        channel_subctx_info->subctxs[subctx_id].gpu_va_space,
+        "instance_ptr {0x%llx:%s} in TSG %u: no GPU VA space for SubCTX %u\n",
+        user_channel->instance_ptr.addr.address,
+        uvm_aperture_string(user_channel->instance_ptr.addr.aperture),
+        user_channel->tsg.id, subctx_id);
+  }
+
+  return channel_subctx_info->subctxs[subctx_id].gpu_va_space;
+}
+
+NV_STATUS uvm_parent_gpu_fault_entry_to_va_space(
+    uvm_parent_gpu_t *parent_gpu, const uvm_fault_buffer_entry_t *fault,
+    uvm_va_space_t **out_va_space, uvm_gpu_t **out_gpu) {
+  uvm_user_channel_t *user_channel;
+  uvm_gpu_va_space_t *gpu_va_space;
+  NV_STATUS status = NV_OK;
+
+  *out_va_space = NULL;
+  *out_gpu = NULL;
+
+  uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+
+  user_channel = instance_ptr_to_user_channel(parent_gpu, fault->instance_ptr);
+  if (!user_channel) {
+    status = NV_ERR_INVALID_CHANNEL;
+    goto exit_unlock;
+  }
+
+  // Faults from HUB clients will always report VEID 0 even if the channel
+  // belongs a TSG with many subcontexts. Therefore, we cannot use the per-TSG
+  // subctx table and we need to directly return the channel's VA space
+  if (!user_channel->in_subctx ||
+      (fault->fault_source.client_type == UVM_FAULT_CLIENT_TYPE_HUB)) {
+    UVM_ASSERT_MSG(
+        fault->fault_source.ve_id == 0,
+        "Fault packet contains SubCTX %u for channel not in subctx\n",
+        fault->fault_source.ve_id);
+
+    // We can safely access user_channel->gpu_va_space under the
+    // instance_ptr_table_lock since gpu_va_space is set to NULL after this
+    // function is called in uvm_user_channel_detach
+    gpu_va_space = user_channel->gpu_va_space;
+    UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) ==
+               UVM_GPU_VA_SPACE_STATE_ACTIVE);
+    *out_va_space = gpu_va_space->va_space;
+    *out_gpu = gpu_va_space->gpu;
+  } else {
+    NvU32 ve_id = fault->fault_source.ve_id;
+
+    // Compute the SMC engine-local VEID
+    UVM_ASSERT(ve_id >= user_channel->smc_engine_ve_id_offset);
+
+    ve_id -= user_channel->smc_engine_ve_id_offset;
+
+    gpu_va_space = user_channel_and_subctx_to_gpu_va_space(user_channel, ve_id);
+
+    // Instance pointer is valid but the fault targets a non-existent
+    // subcontext.
+    if (gpu_va_space) {
+      *out_va_space = gpu_va_space->va_space;
+      *out_gpu = gpu_va_space->gpu;
+    } else {
+      status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
+    }
+  }
 
 exit_unlock:
-    uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
-
-    return status;
-}
-
-NV_STATUS uvm_parent_gpu_access_counter_entry_to_va_space(uvm_parent_gpu_t *parent_gpu,
-                                                          const uvm_access_counter_buffer_entry_t *entry,
-                                                          uvm_va_space_t **out_va_space,
-                                                          uvm_gpu_t **out_gpu)
-{
-    uvm_user_channel_t *user_channel;
-    uvm_gpu_va_space_t *gpu_va_space;
-    NV_STATUS status = NV_OK;
-
-    *out_va_space = NULL;
-    *out_gpu = NULL;
-
-    uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
-
-    user_channel = instance_ptr_to_user_channel(parent_gpu, entry->instance_ptr);
-    if (!user_channel) {
-        status = NV_ERR_INVALID_CHANNEL;
-        goto exit_unlock;
-    }
-
-    if (!user_channel->in_subctx) {
-        UVM_ASSERT_MSG(entry->ve_id == 0,
-                       "Access counter packet contains SubCTX %u for channel not in subctx\n",
-                       entry->ve_id);
-
-        gpu_va_space = user_channel->gpu_va_space;
-        UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) == UVM_GPU_VA_SPACE_STATE_ACTIVE);
-        *out_va_space = gpu_va_space->va_space;
-        *out_gpu = gpu_va_space->gpu;
-    }
-    else {
-        gpu_va_space = user_channel_and_subctx_to_gpu_va_space(user_channel, entry->ve_id);
-        if (gpu_va_space) {
-            *out_va_space = gpu_va_space->va_space;
-            *out_gpu = gpu_va_space->gpu;
-        }
-        else {
-            status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
-        }
-    }
+  uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
+
+  return status;
+}
+
+NV_STATUS uvm_parent_gpu_access_counter_entry_to_va_space(
+    uvm_parent_gpu_t *parent_gpu,
+    const uvm_access_counter_buffer_entry_t *entry,
+    uvm_va_space_t **out_va_space, uvm_gpu_t **out_gpu) {
+  uvm_user_channel_t *user_channel;
+  uvm_gpu_va_space_t *gpu_va_space;
+  NV_STATUS status = NV_OK;
+
+  *out_va_space = NULL;
+  *out_gpu = NULL;
+
+  uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+
+  user_channel = instance_ptr_to_user_channel(parent_gpu, entry->instance_ptr);
+  if (!user_channel) {
+    status = NV_ERR_INVALID_CHANNEL;
+    goto exit_unlock;
+  }
+
+  if (!user_channel->in_subctx) {
+    UVM_ASSERT_MSG(
+        entry->ve_id == 0,
+        "Access counter packet contains SubCTX %u for channel not in subctx\n",
+        entry->ve_id);
+
+    gpu_va_space = user_channel->gpu_va_space;
+    UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) ==
+               UVM_GPU_VA_SPACE_STATE_ACTIVE);
+    *out_va_space = gpu_va_space->va_space;
+    *out_gpu = gpu_va_space->gpu;
+  } else {
+    gpu_va_space =
+        user_channel_and_subctx_to_gpu_va_space(user_channel, entry->ve_id);
+    if (gpu_va_space) {
+      *out_va_space = gpu_va_space->va_space;
+      *out_gpu = gpu_va_space->gpu;
+    } else {
+      status = NV_ERR_PAGE_TABLE_NOT_AVAIL;
+    }
+  }
 
 exit_unlock:
-    uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
+  uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
 
-    return status;
+  return status;
 }
 
-void uvm_parent_gpu_remove_user_channel(uvm_parent_gpu_t *parent_gpu, uvm_user_channel_t *user_channel)
-{
-    uvm_va_space_t *va_space;
-    uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
+void uvm_parent_gpu_remove_user_channel(uvm_parent_gpu_t *parent_gpu,
+                                        uvm_user_channel_t *user_channel) {
+  uvm_va_space_t *va_space;
+  uvm_gpu_va_space_t *gpu_va_space = user_channel->gpu_va_space;
 
-    UVM_ASSERT(user_channel->rm_retained_channel);
-    UVM_ASSERT(gpu_va_space);
-    UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) == UVM_GPU_VA_SPACE_STATE_ACTIVE);
-    va_space = gpu_va_space->va_space;
-    uvm_assert_rwsem_locked_write(&va_space->lock);
+  UVM_ASSERT(user_channel->rm_retained_channel);
+  UVM_ASSERT(gpu_va_space);
+  UVM_ASSERT(uvm_gpu_va_space_state(gpu_va_space) ==
+             UVM_GPU_VA_SPACE_STATE_ACTIVE);
+  va_space = gpu_va_space->va_space;
+  uvm_assert_rwsem_locked_write(&va_space->lock);
 
-    uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
-    parent_gpu_remove_user_channel_subctx_info_locked(parent_gpu, user_channel);
-    parent_gpu_remove_user_channel_instance_ptr_locked(parent_gpu, user_channel);
-    uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
+  uvm_spin_lock(&parent_gpu->instance_ptr_table_lock);
+  parent_gpu_remove_user_channel_subctx_info_locked(parent_gpu, user_channel);
+  parent_gpu_remove_user_channel_instance_ptr_locked(parent_gpu, user_channel);
+  uvm_spin_unlock(&parent_gpu->instance_ptr_table_lock);
 }
 
-static NvU64 gpu_addr_to_dma_addr(uvm_parent_gpu_t *parent_gpu, NvU64 gpu_addr)
-{
-    NvU64 dma_addr = gpu_addr;
-    UVM_ASSERT(dma_addr <= dma_addr + parent_gpu->dma_addressable_start);
+static NvU64 gpu_addr_to_dma_addr(uvm_parent_gpu_t *parent_gpu,
+                                  NvU64 gpu_addr) {
+  NvU64 dma_addr = gpu_addr;
+  UVM_ASSERT(dma_addr <= dma_addr + parent_gpu->dma_addressable_start);
 
-    dma_addr += parent_gpu->dma_addressable_start;
+  dma_addr += parent_gpu->dma_addressable_start;
 
-    return dma_addr;
+  return dma_addr;
 }
 
 // The GPU has its NV_PFB_XV_UPPER_ADDR register set by RM to
 // dma_addressable_start (in bifSetupDmaWindow_IMPL()) and hence when
 // referencing sysmem from the GPU, dma_addressable_start should be
 // subtracted from the DMA address we get from the OS.
-NvU64 uvm_parent_gpu_dma_addr_to_gpu_addr(uvm_parent_gpu_t *parent_gpu, NvU64 dma_addr)
-{
-    NvU64 gpu_addr = dma_addr - parent_gpu->dma_addressable_start;
-    UVM_ASSERT(dma_addr >= gpu_addr);
+NvU64 uvm_parent_gpu_dma_addr_to_gpu_addr(uvm_parent_gpu_t *parent_gpu,
+                                          NvU64 dma_addr) {
+  NvU64 gpu_addr = dma_addr - parent_gpu->dma_addressable_start;
+  UVM_ASSERT(dma_addr >= gpu_addr);
 
-    return gpu_addr;
+  return gpu_addr;
 }
 
-static void *parent_gpu_dma_alloc(NvU64 size, uvm_parent_gpu_t *parent_gpu, gfp_t gfp_flags, NvU64 *dma_address_out)
-{
-    NvU64 dma_addr;
-    void *cpu_addr;
-
-    cpu_addr = dma_alloc_coherent(&parent_gpu->pci_dev->dev, size, &dma_addr, gfp_flags);
-    if (!cpu_addr)
-        return cpu_addr;
+static void *parent_gpu_dma_alloc(NvU64 size, uvm_parent_gpu_t *parent_gpu,
+                                  gfp_t gfp_flags, NvU64 *dma_address_out) {
+  NvU64 dma_addr;
+  void *cpu_addr;
 
-    *dma_address_out = uvm_parent_gpu_dma_addr_to_gpu_addr(parent_gpu, dma_addr);
-    atomic64_add(size, &parent_gpu->mapped_cpu_pages_size);
+  cpu_addr =
+      dma_alloc_coherent(&parent_gpu->pci_dev->dev, size, &dma_addr, gfp_flags);
+  if (!cpu_addr)
     return cpu_addr;
+
+  *dma_address_out = uvm_parent_gpu_dma_addr_to_gpu_addr(parent_gpu, dma_addr);
+  atomic64_add(size, &parent_gpu->mapped_cpu_pages_size);
+  return cpu_addr;
 }
 
-NV_STATUS uvm_gpu_dma_alloc(NvU64 size, uvm_gpu_t *gpu, gfp_t gfp_flags, void **cpu_addr_out, NvU64 *dma_address_out)
-{
-    NV_STATUS status;
-    void *cpu_addr = parent_gpu_dma_alloc(size, gpu->parent, gfp_flags, dma_address_out);
-    if (!cpu_addr)
-        return NV_ERR_NO_MEMORY;
+NV_STATUS uvm_gpu_dma_alloc(NvU64 size, uvm_gpu_t *gpu, gfp_t gfp_flags,
+                            void **cpu_addr_out, NvU64 *dma_address_out) {
+  NV_STATUS status;
+  void *cpu_addr =
+      parent_gpu_dma_alloc(size, gpu->parent, gfp_flags, dma_address_out);
+  if (!cpu_addr)
+    return NV_ERR_NO_MEMORY;
 
-    // Some GPUs require TLB invalidation on IOMMU invalid -> valid transitions
-    status = uvm_mmu_tlb_invalidate_phys(gpu);
-    if (status != NV_OK) {
-        uvm_parent_gpu_dma_free(size, gpu->parent, cpu_addr, *dma_address_out);
-        return status;
-    }
+  // Some GPUs require TLB invalidation on IOMMU invalid -> valid transitions
+  status = uvm_mmu_tlb_invalidate_phys(gpu);
+  if (status != NV_OK) {
+    uvm_parent_gpu_dma_free(size, gpu->parent, cpu_addr, *dma_address_out);
+    return status;
+  }
 
-    *cpu_addr_out = cpu_addr;
-    return NV_OK;
+  *cpu_addr_out = cpu_addr;
+  return NV_OK;
 }
 
-void uvm_parent_gpu_dma_free(NvU64 size, uvm_parent_gpu_t *parent_gpu, void *cpu_addr, NvU64 dma_address)
-{
-    dma_address = gpu_addr_to_dma_addr(parent_gpu, dma_address);
-    dma_free_coherent(&parent_gpu->pci_dev->dev, size, cpu_addr, dma_address);
-    atomic64_sub(size, &parent_gpu->mapped_cpu_pages_size);
-}
-
-NV_STATUS uvm_gpu_map_cpu_pages_no_invalidate(uvm_gpu_t *gpu, struct page *page, size_t size, NvU64 *dma_address_out)
-{
-    // This function only actually needs the parent GPU, but it takes in the
-    // sub GPU for API symmetry with uvm_gpu_map_cpu_pages().
-    uvm_parent_gpu_t *parent_gpu = gpu->parent;
-    NvU64 dma_addr;
-
-    UVM_ASSERT(PAGE_ALIGNED(size));
-
-    dma_addr = dma_map_page(&parent_gpu->pci_dev->dev, page, 0, size, DMA_BIDIRECTIONAL);
-    if (dma_mapping_error(&parent_gpu->pci_dev->dev, dma_addr))
-        return NV_ERR_OPERATING_SYSTEM;
-
-    if (dma_addr < parent_gpu->dma_addressable_start ||
-        dma_addr + size - 1 > parent_gpu->dma_addressable_limit) {
-        dma_unmap_page(&parent_gpu->pci_dev->dev, dma_addr, size, DMA_BIDIRECTIONAL);
-        UVM_ERR_PRINT_RL("PCI mapped range [0x%llx, 0x%llx) not in the addressable range [0x%llx, 0x%llx), GPU %s\n",
-                         dma_addr,
-                         dma_addr + (NvU64)size,
-                         parent_gpu->dma_addressable_start,
-                         parent_gpu->dma_addressable_limit + 1,
-                         uvm_parent_gpu_name(parent_gpu));
-        return NV_ERR_INVALID_ADDRESS;
-    }
+void uvm_parent_gpu_dma_free(NvU64 size, uvm_parent_gpu_t *parent_gpu,
+                             void *cpu_addr, NvU64 dma_address) {
+  dma_address = gpu_addr_to_dma_addr(parent_gpu, dma_address);
+  dma_free_coherent(&parent_gpu->pci_dev->dev, size, cpu_addr, dma_address);
+  atomic64_sub(size, &parent_gpu->mapped_cpu_pages_size);
+}
 
-    atomic64_add(size, &parent_gpu->mapped_cpu_pages_size);
-    *dma_address_out = uvm_parent_gpu_dma_addr_to_gpu_addr(parent_gpu, dma_addr);
+NV_STATUS uvm_gpu_map_cpu_pages_no_invalidate(uvm_gpu_t *gpu, struct page *page,
+                                              size_t size,
+                                              NvU64 *dma_address_out) {
+  // This function only actually needs the parent GPU, but it takes in the
+  // sub GPU for API symmetry with uvm_gpu_map_cpu_pages().
+  uvm_parent_gpu_t *parent_gpu = gpu->parent;
+  NvU64 dma_addr;
 
-    return NV_OK;
-}
+  UVM_ASSERT(PAGE_ALIGNED(size));
 
-NV_STATUS uvm_gpu_map_cpu_pages(uvm_gpu_t *gpu, struct page *page, size_t size, NvU64 *dma_address_out)
-{
-    NV_STATUS status = uvm_gpu_map_cpu_pages_no_invalidate(gpu, page, size, dma_address_out);
-    if (status != NV_OK)
-        return status;
+  dma_addr =
+      dma_map_page(&parent_gpu->pci_dev->dev, page, 0, size, DMA_BIDIRECTIONAL);
+  if (dma_mapping_error(&parent_gpu->pci_dev->dev, dma_addr))
+    return NV_ERR_OPERATING_SYSTEM;
 
-    // Some GPUs require TLB invalidation on IOMMU invalid -> valid
-    // transitions.
-    status = uvm_mmu_tlb_invalidate_phys(gpu);
-    if (status != NV_OK)
-        uvm_parent_gpu_unmap_cpu_pages(gpu->parent, *dma_address_out, size);
+  if (dma_addr < parent_gpu->dma_addressable_start ||
+      dma_addr + size - 1 > parent_gpu->dma_addressable_limit) {
+    dma_unmap_page(&parent_gpu->pci_dev->dev, dma_addr, size,
+                   DMA_BIDIRECTIONAL);
+    UVM_ERR_PRINT_RL(
+        "PCI mapped range [0x%llx, 0x%llx) not in the addressable range "
+        "[0x%llx, 0x%llx), GPU %s\n",
+        dma_addr, dma_addr + (NvU64)size, parent_gpu->dma_addressable_start,
+        parent_gpu->dma_addressable_limit + 1, uvm_parent_gpu_name(parent_gpu));
+    return NV_ERR_INVALID_ADDRESS;
+  }
+
+  atomic64_add(size, &parent_gpu->mapped_cpu_pages_size);
+  *dma_address_out = uvm_parent_gpu_dma_addr_to_gpu_addr(parent_gpu, dma_addr);
+
+  return NV_OK;
+}
 
+NV_STATUS uvm_gpu_map_cpu_pages(uvm_gpu_t *gpu, struct page *page, size_t size,
+                                NvU64 *dma_address_out) {
+  NV_STATUS status =
+      uvm_gpu_map_cpu_pages_no_invalidate(gpu, page, size, dma_address_out);
+  if (status != NV_OK)
     return status;
+
+  // Some GPUs require TLB invalidation on IOMMU invalid -> valid
+  // transitions.
+  status = uvm_mmu_tlb_invalidate_phys(gpu);
+  if (status != NV_OK)
+    uvm_parent_gpu_unmap_cpu_pages(gpu->parent, *dma_address_out, size);
+
+  return status;
 }
 
-void uvm_parent_gpu_unmap_cpu_pages(uvm_parent_gpu_t *parent_gpu, NvU64 dma_address, size_t size)
-{
-    UVM_ASSERT(PAGE_ALIGNED(size));
+void uvm_parent_gpu_unmap_cpu_pages(uvm_parent_gpu_t *parent_gpu,
+                                    NvU64 dma_address, size_t size) {
+  UVM_ASSERT(PAGE_ALIGNED(size));
 
-    dma_address = gpu_addr_to_dma_addr(parent_gpu, dma_address);
-    dma_unmap_page(&parent_gpu->pci_dev->dev, dma_address, size, DMA_BIDIRECTIONAL);
-    atomic64_sub(size, &parent_gpu->mapped_cpu_pages_size);
+  dma_address = gpu_addr_to_dma_addr(parent_gpu, dma_address);
+  dma_unmap_page(&parent_gpu->pci_dev->dev, dma_address, size,
+                 DMA_BIDIRECTIONAL);
+  atomic64_sub(size, &parent_gpu->mapped_cpu_pages_size);
 }
 
 // This function implements the UvmRegisterGpu API call, as described in uvm.h.
@@ -3955,179 +4027,178 @@ void uvm_parent_gpu_unmap_cpu_pages(uvm_parent_gpu_t *parent_gpu, NvU64 dma_addr
 // is something different: there may be more than one
 // GPU VA space within a process, and therefore within a UVM VA space.
 //
-NV_STATUS uvm_api_register_gpu(UVM_REGISTER_GPU_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_rm_user_object_t user_rm_va_space = {
-        .rm_control_fd = params->rmCtrlFd,
-        .user_client   = params->hClient,
-        .user_object   = params->hSmcPartRef,
-    };
-    NvProcessorUuid gpu_instance_uuid;
-    NV_STATUS status;
-
-    status = uvm_va_space_register_gpu(va_space,
-                                       &params->gpu_uuid,
-                                       &user_rm_va_space,
-                                       &params->numaEnabled,
-                                       &params->numaNodeId,
-                                       &gpu_instance_uuid);
-    if (status == NV_OK)
-        uvm_uuid_copy(&params->gpu_uuid, &gpu_instance_uuid);
+NV_STATUS uvm_api_register_gpu(UVM_REGISTER_GPU_PARAMS *params,
+                               struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_rm_user_object_t user_rm_va_space = {
+      .rm_control_fd = params->rmCtrlFd,
+      .user_client = params->hClient,
+      .user_object = params->hSmcPartRef,
+  };
+  NvProcessorUuid gpu_instance_uuid;
+  NV_STATUS status;
 
-    return status;
+  status = uvm_va_space_register_gpu(va_space, &params->gpu_uuid,
+                                     &user_rm_va_space, &params->numaEnabled,
+                                     &params->numaNodeId, &gpu_instance_uuid);
+  if (status == NV_OK)
+    uvm_uuid_copy(&params->gpu_uuid, &gpu_instance_uuid);
+
+  return status;
 }
 
-NV_STATUS uvm_api_unregister_gpu(UVM_UNREGISTER_GPU_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+NV_STATUS uvm_api_unregister_gpu(UVM_UNREGISTER_GPU_PARAMS *params,
+                                 struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-    return uvm_va_space_unregister_gpu(va_space, &params->gpu_uuid);
+  return uvm_va_space_unregister_gpu(va_space, &params->gpu_uuid);
 }
 
-NV_STATUS uvm_api_register_gpu_va_space(UVM_REGISTER_GPU_VASPACE_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_rm_user_object_t user_rm_va_space = {
-        .rm_control_fd = params->rmCtrlFd,
-        .user_client   = params->hClient,
-        .user_object   = params->hVaSpace
-    };
-    return uvm_va_space_register_gpu_va_space(va_space, &user_rm_va_space, &params->gpuUuid);
+NV_STATUS uvm_api_register_gpu_va_space(UVM_REGISTER_GPU_VASPACE_PARAMS *params,
+                                        struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_rm_user_object_t user_rm_va_space = {.rm_control_fd = params->rmCtrlFd,
+                                           .user_client = params->hClient,
+                                           .user_object = params->hVaSpace};
+  return uvm_va_space_register_gpu_va_space(va_space, &user_rm_va_space,
+                                            &params->gpuUuid);
 }
 
-NV_STATUS uvm_api_unregister_gpu_va_space(UVM_UNREGISTER_GPU_VASPACE_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    return uvm_va_space_unregister_gpu_va_space(va_space, &params->gpuUuid);
+NV_STATUS
+uvm_api_unregister_gpu_va_space(UVM_UNREGISTER_GPU_VASPACE_PARAMS *params,
+                                struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  return uvm_va_space_unregister_gpu_va_space(va_space, &params->gpuUuid);
 }
 
-NV_STATUS uvm_api_pageable_mem_access_on_gpu(UVM_PAGEABLE_MEM_ACCESS_ON_GPU_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_gpu_t *gpu;
+NV_STATUS uvm_api_pageable_mem_access_on_gpu(
+    UVM_PAGEABLE_MEM_ACCESS_ON_GPU_PARAMS *params, struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_gpu_t *gpu;
 
-    uvm_va_space_down_read(va_space);
-    gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
-
-    if (!gpu) {
-        uvm_va_space_up_read(va_space);
-        return NV_ERR_INVALID_DEVICE;
-    }
-
-    if (uvm_va_space_pageable_mem_access_enabled(va_space) && gpu->parent->replayable_faults_supported)
-        params->pageableMemAccess = NV_TRUE;
-    else
-        params->pageableMemAccess = NV_FALSE;
+  uvm_va_space_down_read(va_space);
+  gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
 
+  if (!gpu) {
     uvm_va_space_up_read(va_space);
-    return NV_OK;
-}
-
-NV_STATUS uvm_test_set_prefetch_filtering(UVM_TEST_SET_PREFETCH_FILTERING_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_gpu_t *gpu = NULL;
-    NV_STATUS status = NV_OK;
-
-    uvm_mutex_lock(&g_uvm_global.global_lock);
-
-    uvm_va_space_down_read(va_space);
-
-    gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
-
-    if (!gpu) {
-        status = NV_ERR_INVALID_DEVICE;
-        goto done;
-    }
-
-    if (!gpu->parent->isr.replayable_faults.handling || !gpu->parent->prefetch_fault_supported) {
-        status = NV_ERR_INVALID_DEVICE;
-        goto done;
-    }
-
-    switch (params->filtering_mode) {
-        case UVM_TEST_PREFETCH_FILTERING_MODE_FILTER_ALL:
-            uvm_parent_gpu_disable_prefetch_faults(gpu->parent);
-            break;
-        case UVM_TEST_PREFETCH_FILTERING_MODE_FILTER_NONE:
-            uvm_parent_gpu_enable_prefetch_faults(gpu->parent);
-            break;
-        default:
-            status = NV_ERR_INVALID_ARGUMENT;
-            break;
-    }
+    return NV_ERR_INVALID_DEVICE;
+  }
+
+  if (uvm_va_space_pageable_mem_access_enabled(va_space) &&
+      gpu->parent->replayable_faults_supported)
+    params->pageableMemAccess = NV_TRUE;
+  else
+    params->pageableMemAccess = NV_FALSE;
+
+  uvm_va_space_up_read(va_space);
+  return NV_OK;
+}
+
+NV_STATUS
+uvm_test_set_prefetch_filtering(UVM_TEST_SET_PREFETCH_FILTERING_PARAMS *params,
+                                struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_gpu_t *gpu = NULL;
+  NV_STATUS status = NV_OK;
+
+  uvm_mutex_lock(&g_uvm_global.global_lock);
+
+  uvm_va_space_down_read(va_space);
+
+  gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
+
+  if (!gpu) {
+    status = NV_ERR_INVALID_DEVICE;
+    goto done;
+  }
+
+  if (!gpu->parent->isr.replayable_faults.handling ||
+      !gpu->parent->prefetch_fault_supported) {
+    status = NV_ERR_INVALID_DEVICE;
+    goto done;
+  }
+
+  switch (params->filtering_mode) {
+  case UVM_TEST_PREFETCH_FILTERING_MODE_FILTER_ALL:
+    uvm_parent_gpu_disable_prefetch_faults(gpu->parent);
+    break;
+  case UVM_TEST_PREFETCH_FILTERING_MODE_FILTER_NONE:
+    uvm_parent_gpu_enable_prefetch_faults(gpu->parent);
+    break;
+  default:
+    status = NV_ERR_INVALID_ARGUMENT;
+    break;
+  }
 
 done:
-    uvm_va_space_up_read(va_space);
+  uvm_va_space_up_read(va_space);
 
-    uvm_mutex_unlock(&g_uvm_global.global_lock);
-    return status;
+  uvm_mutex_unlock(&g_uvm_global.global_lock);
+  return status;
 }
 
-NV_STATUS uvm_test_get_gpu_time(UVM_TEST_GET_GPU_TIME_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_gpu_t *gpu = NULL;
-    NV_STATUS status = NV_OK;
+NV_STATUS uvm_test_get_gpu_time(UVM_TEST_GET_GPU_TIME_PARAMS *params,
+                                struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_gpu_t *gpu = NULL;
+  NV_STATUS status = NV_OK;
 
-    uvm_va_space_down_read(va_space);
+  uvm_va_space_down_read(va_space);
 
-    gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
+  gpu = uvm_va_space_get_gpu_by_uuid(va_space, &params->gpu_uuid);
 
-    if (gpu)
-        params->timestamp_ns = gpu->parent->host_hal->get_time(gpu);
-    else
-        status = NV_ERR_INVALID_DEVICE;
+  if (gpu)
+    params->timestamp_ns = gpu->parent->host_hal->get_time(gpu);
+  else
+    status = NV_ERR_INVALID_DEVICE;
 
-    uvm_va_space_up_read(va_space);
+  uvm_va_space_up_read(va_space);
 
-    return status;
+  return status;
 }
 
-NV_STATUS uvm_test_dump_access_bits(UVM_TEST_DUMP_ACCESS_BITS_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
-    uvm_gpu_t *gpu = NULL;
-    NV_STATUS status = NV_OK;
-    NvU64 granularity_size_kb = 0;
+NV_STATUS uvm_test_dump_access_bits(UVM_TEST_DUMP_ACCESS_BITS_PARAMS *params,
+                                    struct file *filp) {
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
+  uvm_gpu_t *gpu = NULL;
+  NV_STATUS status = NV_OK;
+  NvU64 granularity_size_kb = 0;
 
-    gpu = uvm_va_space_retain_gpu_by_uuid(va_space, &params->gpu_uuid);
-    if (!gpu || !gpu->parent->access_bits_supported) {
-        status = NV_ERR_INVALID_DEVICE;
-        goto done;
-    }
+  gpu = uvm_va_space_retain_gpu_by_uuid(va_space, &params->gpu_uuid);
+  if (!gpu || !gpu->parent->access_bits_supported) {
+    status = NV_ERR_INVALID_DEVICE;
+    goto done;
+  }
 
-    if (!gpu->parent->vab_info.accessBitsBufferHandle) {
-        status = NV_ERR_INVALID_STATE;
-        goto done;
-    }
+  if (!gpu->parent->vab_info.accessBitsBufferHandle) {
+    status = NV_ERR_INVALID_STATE;
+    goto done;
+  }
 
-    // See resman/interface/rmapi/finn/ctrl/ctrlc763.finn for 'granularity' enum values
-    granularity_size_kb = (NvU64)(64) << gpu->parent->vab_info.granularity;
-    params->granularity_size_kb = granularity_size_kb;
+  // See resman/interface/rmapi/finn/ctrl/ctrlc763.finn for 'granularity' enum
+  // values
+  granularity_size_kb = (NvU64)(64) << gpu->parent->vab_info.granularity;
+  params->granularity_size_kb = granularity_size_kb;
 
-    status = uvm_gpu_update_access_bits(gpu->parent, params->mode);
-    if (status != NV_OK)
-        goto done;
+  status = uvm_gpu_update_access_bits(gpu->parent, params->mode);
+  if (status != NV_OK)
+    goto done;
 
-    // If this is a length query, we are done after we set the length
-    if (params->current_bits_length == 0) {
-        params->current_bits_length = ARRAY_SIZE(gpu->parent->vab_info.currentBits);
-        goto done;
-    }
+  // If this is a length query, we are done after we set the length
+  if (params->current_bits_length == 0) {
+    params->current_bits_length = ARRAY_SIZE(gpu->parent->vab_info.currentBits);
+    goto done;
+  }
 
-    // Copy the bits to user space
-    if (copy_to_user(params->current_bits, 
-                     gpu->parent->vab_info.currentBits,
-                     sizeof(NvU64) * params->current_bits_length)) {
-        status = NV_ERR_INVALID_ADDRESS;
-        goto done;
-    }
+  // Copy the bits to user space
+  if (copy_to_user(params->current_bits, gpu->parent->vab_info.currentBits,
+                   sizeof(NvU64) * params->current_bits_length)) {
+    status = NV_ERR_INVALID_ADDRESS;
+    goto done;
+  }
 
 done:
-    if (gpu)
-        uvm_gpu_release(gpu);
-    return status;
+  if (gpu)
+    uvm_gpu_release(gpu);
+  return status;
 }
-
diff --git a/kernel-open/nvidia-uvm/uvm_ioctl.h b/kernel-open/nvidia-uvm/uvm_ioctl.h
index d612daef..648cf01c 100644
--- a/kernel-open/nvidia-uvm/uvm_ioctl.h
+++ b/kernel-open/nvidia-uvm/uvm_ioctl.h
@@ -35,87 +35,82 @@ extern "C" {
 //
 
 #if defined(WIN32) || defined(WIN64)
-#   define UVM_IOCTL_BASE(i)       CTL_CODE(FILE_DEVICE_UNKNOWN, 0x800+i, METHOD_BUFFERED, FILE_READ_DATA | FILE_WRITE_DATA)
+#define UVM_IOCTL_BASE(i)                                                      \
+  CTL_CODE(FILE_DEVICE_UNKNOWN, 0x800 + i, METHOD_BUFFERED,                    \
+           FILE_READ_DATA | FILE_WRITE_DATA)
 #else
-#   define UVM_IOCTL_BASE(i) i
+#define UVM_IOCTL_BASE(i) i
 #endif
 
 //
 // UvmReserveVa
 //
-#define UVM_RESERVE_VA                                                UVM_IOCTL_BASE(1)
+#define UVM_RESERVE_VA UVM_IOCTL_BASE(1)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_RESERVE_VA_PARAMS;
 
 //
 // UvmReleaseVa
 //
-#define UVM_RELEASE_VA                                                UVM_IOCTL_BASE(2)
+#define UVM_RELEASE_VA UVM_IOCTL_BASE(2)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_RELEASE_VA_PARAMS;
 
 //
 // UvmRegionCommit
 //
-#define UVM_REGION_COMMIT                                             UVM_IOCTL_BASE(3)
+#define UVM_REGION_COMMIT UVM_IOCTL_BASE(3)
 
-typedef struct
-{
-    NvU64           requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64           length        NV_ALIGN_BYTES(8); // IN
-    UvmStream       streamId      NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid gpuUuid;                         // IN
-    NV_STATUS       rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  UvmStream streamId NV_ALIGN_BYTES(8);  // IN
+  NvProcessorUuid gpuUuid;               // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_REGION_COMMIT_PARAMS;
 
 //
 // UvmRegionDecommit
 //
-#define UVM_REGION_DECOMMIT                                           UVM_IOCTL_BASE(4)
+#define UVM_REGION_DECOMMIT UVM_IOCTL_BASE(4)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_REGION_DECOMMIT_PARAMS;
 
 //
 // UvmRegionSetStream
 //
-#define UVM_REGION_SET_STREAM                                         UVM_IOCTL_BASE(5)
+#define UVM_REGION_SET_STREAM UVM_IOCTL_BASE(5)
 
-typedef struct
-{
-    NvU64           requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64           length        NV_ALIGN_BYTES(8); // IN
-    UvmStream       newStreamId   NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid gpuUuid;                         // IN
-    NV_STATUS       rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8);          // IN
+  UvmStream newStreamId NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid gpuUuid;                 // IN
+  NV_STATUS rmStatus;                      // OUT
 } UVM_REGION_SET_STREAM_PARAMS;
 
 //
 // UvmSetStreamRunning
 //
-#define UVM_SET_STREAM_RUNNING                                        UVM_IOCTL_BASE(6)
+#define UVM_SET_STREAM_RUNNING UVM_IOCTL_BASE(6)
 
-typedef struct
-{
-    UvmStream  streamId NV_ALIGN_BYTES(8);  // IN
-    NV_STATUS  rmStatus;                    // OUT
+typedef struct {
+  UvmStream streamId NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_SET_STREAM_RUNNING_PARAMS;
 
-
 //
 // Due to limitations in how much we want to send per ioctl call, the nStreams
 // member must be less than or equal to about 250. That's an upper limit.
@@ -138,24 +133,23 @@ typedef struct
 //
 // UvmSetStreamStopped
 //
-#define UVM_SET_STREAM_STOPPED                                        UVM_IOCTL_BASE(7)
+#define UVM_SET_STREAM_STOPPED UVM_IOCTL_BASE(7)
 
-typedef struct
-{
-    UvmStream streamIdArray[UVM_MAX_STREAMS_PER_IOCTL_CALL] NV_ALIGN_BYTES(8); // IN
-    NvU64     nStreams                                      NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                                        // OUT
+typedef struct {
+  UvmStream
+      streamIdArray[UVM_MAX_STREAMS_PER_IOCTL_CALL] NV_ALIGN_BYTES(8); // IN
+  NvU64 nStreams NV_ALIGN_BYTES(8);                                    // IN
+  NV_STATUS rmStatus;                                                  // OUT
 } UVM_SET_STREAM_STOPPED_PARAMS;
 
 //
 // UvmAddSession
 //
-#define UVM_ADD_SESSION                                               UVM_IOCTL_BASE(10)
+#define UVM_ADD_SESSION UVM_IOCTL_BASE(10)
 
-typedef struct
-{
-    NvU32        pidTarget;                             // IN
-    NV_STATUS    rmStatus;                              // OUT
+typedef struct {
+  NvU32 pidTarget;    // IN
+  NV_STATUS rmStatus; // OUT
 } UVM_ADD_SESSION_PARAMS;
 
 #define UVM_MAX_COUNTERS_PER_IOCTL_CALL 32
@@ -163,314 +157,291 @@ typedef struct
 //
 // UvmEnableCounters
 //
-#define UVM_ENABLE_COUNTERS                                           UVM_IOCTL_BASE(12)
+#define UVM_ENABLE_COUNTERS UVM_IOCTL_BASE(12)
 
-typedef struct
-{
-    UvmCounterConfig config[UVM_MAX_COUNTERS_PER_IOCTL_CALL]; // IN
-    NvU32            count;                                   // IN
-    NV_STATUS        rmStatus;                                // OUT
+typedef struct {
+  UvmCounterConfig config[UVM_MAX_COUNTERS_PER_IOCTL_CALL]; // IN
+  NvU32 count;                                              // IN
+  NV_STATUS rmStatus;                                       // OUT
 } UVM_ENABLE_COUNTERS_PARAMS;
 
 //
 // UvmMapCounter
 //
-#define UVM_MAP_COUNTER                                               UVM_IOCTL_BASE(13)
+#define UVM_MAP_COUNTER UVM_IOCTL_BASE(13)
 
-typedef struct
-{
-    NvU32           scope;                          // IN (UvmCounterScope)
-    NvU32           counterName;                    // IN (UvmCounterName)
-    NvProcessorUuid gpuUuid;                        // IN
-    NvP64           addr         NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS       rmStatus;                       // OUT
+typedef struct {
+  NvU32 scope;                  // IN (UvmCounterScope)
+  NvU32 counterName;            // IN (UvmCounterName)
+  NvProcessorUuid gpuUuid;      // IN
+  NvP64 addr NV_ALIGN_BYTES(8); // OUT
+  NV_STATUS rmStatus;           // OUT
 } UVM_MAP_COUNTER_PARAMS;
 
 //
 // UvmCreateEventQueue
 //
-#define UVM_CREATE_EVENT_QUEUE                                        UVM_IOCTL_BASE(14)
+#define UVM_CREATE_EVENT_QUEUE UVM_IOCTL_BASE(14)
 
-typedef struct
-{
-    NvU32                 eventQueueIndex;                      // OUT
-    NvU64                 queueSize          NV_ALIGN_BYTES(8); // IN
-    NvU64                 notificationCount  NV_ALIGN_BYTES(8); // IN
-    NvU64                 notificationHandle NV_ALIGN_BYTES(8); // IN
-    NvU32                 timeStampType;                        // IN (UvmEventTimeStampType)
-    NV_STATUS             rmStatus;                             // OUT
+typedef struct {
+  NvU32 eventQueueIndex;                      // OUT
+  NvU64 queueSize NV_ALIGN_BYTES(8);          // IN
+  NvU64 notificationCount NV_ALIGN_BYTES(8);  // IN
+  NvU64 notificationHandle NV_ALIGN_BYTES(8); // IN
+  NvU32 timeStampType;                        // IN (UvmEventTimeStampType)
+  NV_STATUS rmStatus;                         // OUT
 } UVM_CREATE_EVENT_QUEUE_PARAMS;
 
 //
 // UvmRemoveEventQueue
 //
-#define UVM_REMOVE_EVENT_QUEUE                                        UVM_IOCTL_BASE(15)
+#define UVM_REMOVE_EVENT_QUEUE UVM_IOCTL_BASE(15)
 
-typedef struct
-{
-    NvU32         eventQueueIndex;    // IN
-    NV_STATUS     rmStatus;           // OUT
+typedef struct {
+  NvU32 eventQueueIndex; // IN
+  NV_STATUS rmStatus;    // OUT
 } UVM_REMOVE_EVENT_QUEUE_PARAMS;
 
 //
 // UvmMapEventQueue
 //
-#define UVM_MAP_EVENT_QUEUE                                           UVM_IOCTL_BASE(16)
-
-typedef struct
-{
-    NvU32         eventQueueIndex;                    // IN
-    NvP64         userRODataAddr   NV_ALIGN_BYTES(8); // IN
-    NvP64         userRWDataAddr   NV_ALIGN_BYTES(8); // IN
-    NvP64         readIndexAddr    NV_ALIGN_BYTES(8); // OUT
-    NvP64         writeIndexAddr   NV_ALIGN_BYTES(8); // OUT
-    NvP64         queueBufferAddr  NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS     rmStatus;                           // OUT
+#define UVM_MAP_EVENT_QUEUE UVM_IOCTL_BASE(16)
+
+typedef struct {
+  NvU32 eventQueueIndex;                   // IN
+  NvP64 userRODataAddr NV_ALIGN_BYTES(8);  // IN
+  NvP64 userRWDataAddr NV_ALIGN_BYTES(8);  // IN
+  NvP64 readIndexAddr NV_ALIGN_BYTES(8);   // OUT
+  NvP64 writeIndexAddr NV_ALIGN_BYTES(8);  // OUT
+  NvP64 queueBufferAddr NV_ALIGN_BYTES(8); // OUT
+  NV_STATUS rmStatus;                      // OUT
 } UVM_MAP_EVENT_QUEUE_PARAMS;
 
 //
 // UvmEnableEvent
 //
-#define UVM_EVENT_CTRL                                                UVM_IOCTL_BASE(17)
+#define UVM_EVENT_CTRL UVM_IOCTL_BASE(17)
 
-typedef struct
-{
-    NvU32        eventQueueIndex;   // IN
-    NvS32        eventType;         // IN
-    NvU32        enable;            // IN
-    NV_STATUS    rmStatus;          // OUT
+typedef struct {
+  NvU32 eventQueueIndex; // IN
+  NvS32 eventType;       // IN
+  NvU32 enable;          // IN
+  NV_STATUS rmStatus;    // OUT
 } UVM_EVENT_CTRL_PARAMS;
 
 //
 // UvmEventGetGpuUuidTable
 //
-#define UVM_GET_GPU_UUID_TABLE                                        UVM_IOCTL_BASE(20)
+#define UVM_GET_GPU_UUID_TABLE UVM_IOCTL_BASE(20)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuidArray[UVM_MAX_GPUS_V1]; // OUT
-    NvU32           validCount;                    // OUT
-    NV_STATUS       rmStatus;                      // OUT
+typedef struct {
+  NvProcessorUuid gpuUuidArray[UVM_MAX_GPUS_V1]; // OUT
+  NvU32 validCount;                              // OUT
+  NV_STATUS rmStatus;                            // OUT
 } UVM_GET_GPU_UUID_TABLE_PARAMS;
 
 #if defined(WIN32) || defined(WIN64)
 //
 // UvmRegionSetBacking
 //
-#define UVM_REGION_SET_BACKING                                        UVM_IOCTL_BASE(21)
+#define UVM_REGION_SET_BACKING UVM_IOCTL_BASE(21)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuid;                        // IN
-    NvU32           hAllocation;                    // IN
-    NvP64           vaAddr       NV_ALIGN_BYTES(8); // IN
-    NvU64           regionLength NV_ALIGN_BYTES(8); // IN
-    NV_STATUS       rmStatus;                       // OUT
+typedef struct {
+  NvProcessorUuid gpuUuid;              // IN
+  NvU32 hAllocation;                    // IN
+  NvP64 vaAddr NV_ALIGN_BYTES(8);       // IN
+  NvU64 regionLength NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_REGION_SET_BACKING_PARAMS;
 
 //
 // UvmRegionUnsetBacking
 //
-#define UVM_REGION_UNSET_BACKING                                      UVM_IOCTL_BASE(22)
+#define UVM_REGION_UNSET_BACKING UVM_IOCTL_BASE(22)
 
-typedef struct
-{
-    NvP64     vaAddr       NV_ALIGN_BYTES(8); // IN
-    NvU64     regionLength NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                       // OUT
+typedef struct {
+  NvP64 vaAddr NV_ALIGN_BYTES(8);       // IN
+  NvU64 regionLength NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_REGION_UNSET_BACKING_PARAMS;
 
 #endif
 
-#define UVM_CREATE_RANGE_GROUP                                        UVM_IOCTL_BASE(23)
+#define UVM_CREATE_RANGE_GROUP UVM_IOCTL_BASE(23)
 
-typedef struct
-{
-    NvU64     rangeGroupId NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS rmStatus;                       // OUT
+typedef struct {
+  NvU64 rangeGroupId NV_ALIGN_BYTES(8); // OUT
+  NV_STATUS rmStatus;                   // OUT
 } UVM_CREATE_RANGE_GROUP_PARAMS;
 
-#define UVM_DESTROY_RANGE_GROUP                                       UVM_IOCTL_BASE(24)
+#define UVM_DESTROY_RANGE_GROUP UVM_IOCTL_BASE(24)
 
-typedef struct
-{
-    NvU64     rangeGroupId NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                      // OUT
+typedef struct {
+  NvU64 rangeGroupId NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_DESTROY_RANGE_GROUP_PARAMS;
 
 //
 // UvmRegisterGpuVaSpace
 //
-#define UVM_REGISTER_GPU_VASPACE                                      UVM_IOCTL_BASE(25)
+#define UVM_REGISTER_GPU_VASPACE UVM_IOCTL_BASE(25)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuid;  // IN
-    NvS32           rmCtrlFd; // IN
-    NvHandle        hClient;  // IN
-    NvHandle        hVaSpace; // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpuUuid; // IN
+  NvS32 rmCtrlFd;          // IN
+  NvHandle hClient;        // IN
+  NvHandle hVaSpace;       // IN
+  NV_STATUS rmStatus;      // OUT
 } UVM_REGISTER_GPU_VASPACE_PARAMS;
 
 //
 // UvmUnregisterGpuVaSpace
 //
-#define UVM_UNREGISTER_GPU_VASPACE                                    UVM_IOCTL_BASE(26)
+#define UVM_UNREGISTER_GPU_VASPACE UVM_IOCTL_BASE(26)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuid;  // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpuUuid; // IN
+  NV_STATUS rmStatus;      // OUT
 } UVM_UNREGISTER_GPU_VASPACE_PARAMS;
 
 //
 // UvmRegisterChannel
 //
-#define UVM_REGISTER_CHANNEL                                          UVM_IOCTL_BASE(27)
-
-typedef struct
-{
-    NvProcessorUuid gpuUuid;                     // IN
-    NvS32           rmCtrlFd;                    // IN
-    NvHandle        hClient;                     // IN
-    NvHandle        hChannel;                    // IN
-    NvU64           base      NV_ALIGN_BYTES(8); // IN
-    NvU64           length    NV_ALIGN_BYTES(8); // IN
-    NV_STATUS       rmStatus;                    // OUT
+#define UVM_REGISTER_CHANNEL UVM_IOCTL_BASE(27)
+
+typedef struct {
+  NvProcessorUuid gpuUuid;        // IN
+  NvS32 rmCtrlFd;                 // IN
+  NvHandle hClient;               // IN
+  NvHandle hChannel;              // IN
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_REGISTER_CHANNEL_PARAMS;
 
 //
 // UvmUnregisterChannel
 //
-#define UVM_UNREGISTER_CHANNEL                                        UVM_IOCTL_BASE(28)
+#define UVM_UNREGISTER_CHANNEL UVM_IOCTL_BASE(28)
 
-typedef struct
-{
-    NvHandle        hClient;  // IN
-    NvHandle        hChannel; // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvHandle hClient;   // IN
+  NvHandle hChannel;  // IN
+  NV_STATUS rmStatus; // OUT
 } UVM_UNREGISTER_CHANNEL_PARAMS;
 
 //
 // UvmEnablePeerAccess
 //
-#define UVM_ENABLE_PEER_ACCESS                                       UVM_IOCTL_BASE(29)
+#define UVM_ENABLE_PEER_ACCESS UVM_IOCTL_BASE(29)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuidA; // IN
-    NvProcessorUuid gpuUuidB; // IN
-    NV_STATUS  rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpuUuidA; // IN
+  NvProcessorUuid gpuUuidB; // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_ENABLE_PEER_ACCESS_PARAMS;
 
 //
 // UvmDisablePeerAccess
 //
-#define UVM_DISABLE_PEER_ACCESS                                      UVM_IOCTL_BASE(30)
+#define UVM_DISABLE_PEER_ACCESS UVM_IOCTL_BASE(30)
 
-typedef struct
-{
-    NvProcessorUuid gpuUuidA; // IN
-    NvProcessorUuid gpuUuidB; // IN
-    NV_STATUS  rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpuUuidA; // IN
+  NvProcessorUuid gpuUuidB; // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_DISABLE_PEER_ACCESS_PARAMS;
 
 //
 // UvmSetRangeGroup
 //
-#define UVM_SET_RANGE_GROUP                                           UVM_IOCTL_BASE(31)
+#define UVM_SET_RANGE_GROUP UVM_IOCTL_BASE(31)
 
-typedef struct
-{
-    NvU64     rangeGroupId  NV_ALIGN_BYTES(8); // IN
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 rangeGroupId NV_ALIGN_BYTES(8);  // IN
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_SET_RANGE_GROUP_PARAMS;
 
 //
 // UvmMapExternalAllocation
 //
-#define UVM_MAP_EXTERNAL_ALLOCATION                                   UVM_IOCTL_BASE(33)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    NvU64                   offset                          NV_ALIGN_BYTES(8); // IN
-    UvmGpuMappingAttributes perGpuAttributes[UVM_MAX_GPUS];                    // IN
-    NvU64                   gpuAttributesCount              NV_ALIGN_BYTES(8); // IN
-    NvS32                   rmCtrlFd;                                          // IN
-    NvU32                   hClient;                                           // IN
-    NvU32                   hMemory;                                           // IN
-
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_MAP_EXTERNAL_ALLOCATION UVM_IOCTL_BASE(33)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);                           // IN
+  NvU64 length NV_ALIGN_BYTES(8);                         // IN
+  NvU64 offset NV_ALIGN_BYTES(8);                         // IN
+  UvmGpuMappingAttributes perGpuAttributes[UVM_MAX_GPUS]; // IN
+  NvU64 gpuAttributesCount NV_ALIGN_BYTES(8);             // IN
+  NvS32 rmCtrlFd;                                         // IN
+  NvU32 hClient;                                          // IN
+  NvU32 hMemory;                                          // IN
+
+  NV_STATUS rmStatus; // OUT
 } UVM_MAP_EXTERNAL_ALLOCATION_PARAMS;
 
 //
 // UvmFree
 //
-#define UVM_FREE                                                      UVM_IOCTL_BASE(34)
-typedef struct
-{
-    NvU64     base      NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                    // OUT
+#define UVM_FREE UVM_IOCTL_BASE(34)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;           // OUT
 } UVM_FREE_PARAMS;
 
 //
 // UvmMemMap
 //
-#define UVM_MEM_MAP                                                   UVM_IOCTL_BASE(35)
+#define UVM_MEM_MAP UVM_IOCTL_BASE(35)
 
-typedef struct
-{
-    NvP64     regionBase   NV_ALIGN_BYTES(8); // IN
-    NvU64     regionLength NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                       // OUT
+typedef struct {
+  NvP64 regionBase NV_ALIGN_BYTES(8);   // IN
+  NvU64 regionLength NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_MEM_MAP_PARAMS;
 
 //
 // UvmRegisterGpu
 //
-#define UVM_REGISTER_GPU                                              UVM_IOCTL_BASE(37)
-
-typedef struct
-{
-    NvProcessorUuid gpu_uuid;    // IN/OUT
-    NvBool          numaEnabled; // OUT
-    NvS32           numaNodeId;  // OUT
-    NvS32           rmCtrlFd;    // IN
-    NvHandle        hClient;     // IN
-    NvHandle        hSmcPartRef; // IN
-    NV_STATUS       rmStatus;    // OUT
+#define UVM_REGISTER_GPU UVM_IOCTL_BASE(37)
+
+typedef struct {
+  NvProcessorUuid gpu_uuid; // IN/OUT
+  NvBool numaEnabled;       // OUT
+  NvS32 numaNodeId;         // OUT
+  NvS32 rmCtrlFd;           // IN
+  NvHandle hClient;         // IN
+  NvHandle hSmcPartRef;     // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_REGISTER_GPU_PARAMS;
 
 //
 // UvmUnregisterGpu
 //
-#define UVM_UNREGISTER_GPU                                            UVM_IOCTL_BASE(38)
+#define UVM_UNREGISTER_GPU UVM_IOCTL_BASE(38)
 
-typedef struct
-{
-    NvProcessorUuid gpu_uuid; // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpu_uuid; // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_UNREGISTER_GPU_PARAMS;
 
-#define UVM_PAGEABLE_MEM_ACCESS                                       UVM_IOCTL_BASE(39)
+#define UVM_PAGEABLE_MEM_ACCESS UVM_IOCTL_BASE(39)
 
-typedef struct
-{
-    NvBool    pageableMemAccess; // OUT
-    NV_STATUS rmStatus;          // OUT
+typedef struct {
+  NvBool pageableMemAccess; // OUT
+  NV_STATUS rmStatus;       // OUT
 } UVM_PAGEABLE_MEM_ACCESS_PARAMS;
 
 //
-// Due to limitations in how much we want to send per ioctl call, the numGroupIds
-// member must be less than or equal to about 250. That's an upper limit.
+// Due to limitations in how much we want to send per ioctl call, the
+// numGroupIds member must be less than or equal to about 250. That's an upper
+// limit.
 //
 // However, from a typical user-space driver's point of view (for example, the
 // CUDA driver), a vast majority of the time, we expect there to be only one
-// range group passed in. The second most common case is something like atmost 32
-// range groups being passed in. The cases where there are more than 32 range
+// range group passed in. The second most common case is something like atmost
+// 32 range groups being passed in. The cases where there are more than 32 range
 // groups are the most rare. So we might want to optimize the ioctls accordingly
 // so that we don't always copy a 250 * sizeof(NvU64) sized array when there's
 // only one or a few range groups.
@@ -485,101 +456,95 @@ typedef struct
 //
 // UvmPreventMigrationRangeGroups
 //
-#define UVM_PREVENT_MIGRATION_RANGE_GROUPS                            UVM_IOCTL_BASE(40)
+#define UVM_PREVENT_MIGRATION_RANGE_GROUPS UVM_IOCTL_BASE(40)
 
-typedef struct
-{
-    NvU64     rangeGroupIds[UVM_MAX_RANGE_GROUPS_PER_IOCTL_CALL] NV_ALIGN_BYTES(8); // IN
-    NvU64     numGroupIds                                        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                                             // OUT
+typedef struct {
+  NvU64 rangeGroupIds[UVM_MAX_RANGE_GROUPS_PER_IOCTL_CALL] NV_ALIGN_BYTES(
+      8);                              // IN
+  NvU64 numGroupIds NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                  // OUT
 } UVM_PREVENT_MIGRATION_RANGE_GROUPS_PARAMS;
 
 //
 // UvmAllowMigrationRangeGroups
 //
-#define UVM_ALLOW_MIGRATION_RANGE_GROUPS                              UVM_IOCTL_BASE(41)
+#define UVM_ALLOW_MIGRATION_RANGE_GROUPS UVM_IOCTL_BASE(41)
 
-typedef struct
-{
-    NvU64     rangeGroupIds[UVM_MAX_RANGE_GROUPS_PER_IOCTL_CALL] NV_ALIGN_BYTES(8); // IN
-    NvU64     numGroupIds                                        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                                             // OUT
+typedef struct {
+  NvU64 rangeGroupIds[UVM_MAX_RANGE_GROUPS_PER_IOCTL_CALL] NV_ALIGN_BYTES(
+      8);                              // IN
+  NvU64 numGroupIds NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                  // OUT
 } UVM_ALLOW_MIGRATION_RANGE_GROUPS_PARAMS;
 
 //
 // UvmSetPreferredLocation
 //
-#define UVM_SET_PREFERRED_LOCATION                                    UVM_IOCTL_BASE(42)
+#define UVM_SET_PREFERRED_LOCATION UVM_IOCTL_BASE(42)
 
-typedef struct
-{
-    NvU64           requestedBase      NV_ALIGN_BYTES(8); // IN
-    NvU64           length             NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid preferredLocation;                    // IN
-    NvS32           preferredCpuNumaNode;                 // IN
-    NV_STATUS       rmStatus;                             // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NvProcessorUuid preferredLocation;     // IN
+  NvS32 preferredCpuNumaNode;            // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_SET_PREFERRED_LOCATION_PARAMS;
 
 //
 // UvmUnsetPreferredLocation
 //
-#define UVM_UNSET_PREFERRED_LOCATION                                  UVM_IOCTL_BASE(43)
+#define UVM_UNSET_PREFERRED_LOCATION UVM_IOCTL_BASE(43)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_UNSET_PREFERRED_LOCATION_PARAMS;
 
 //
 // UvmEnableReadDuplication
 //
-#define UVM_ENABLE_READ_DUPLICATION                                   UVM_IOCTL_BASE(44)
+#define UVM_ENABLE_READ_DUPLICATION UVM_IOCTL_BASE(44)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_ENABLE_READ_DUPLICATION_PARAMS;
 
 //
 // UvmDisableReadDuplication
 //
-#define UVM_DISABLE_READ_DUPLICATION                                  UVM_IOCTL_BASE(45)
+#define UVM_DISABLE_READ_DUPLICATION UVM_IOCTL_BASE(45)
 
-typedef struct
-{
-    NvU64     requestedBase NV_ALIGN_BYTES(8); // IN
-    NvU64     length        NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                        // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_DISABLE_READ_DUPLICATION_PARAMS;
 
 //
 // UvmSetAccessedBy
 //
-#define UVM_SET_ACCESSED_BY                                           UVM_IOCTL_BASE(46)
+#define UVM_SET_ACCESSED_BY UVM_IOCTL_BASE(46)
 
-typedef struct
-{
-    NvU64           requestedBase   NV_ALIGN_BYTES(8); // IN
-    NvU64           length          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid accessedByUuid;                    // IN
-    NV_STATUS       rmStatus;                          // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NvProcessorUuid accessedByUuid;        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_SET_ACCESSED_BY_PARAMS;
 
 //
 // UvmUnsetAccessedBy
 //
-#define UVM_UNSET_ACCESSED_BY                                         UVM_IOCTL_BASE(47)
+#define UVM_UNSET_ACCESSED_BY UVM_IOCTL_BASE(47)
 
-typedef struct
-{
-    NvU64           requestedBase   NV_ALIGN_BYTES(8); // IN
-    NvU64           length          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid accessedByUuid;                    // IN
-    NV_STATUS       rmStatus;                          // OUT
+typedef struct {
+  NvU64 requestedBase NV_ALIGN_BYTES(8); // IN
+  NvU64 length NV_ALIGN_BYTES(8);        // IN
+  NvProcessorUuid accessedByUuid;        // IN
+  NV_STATUS rmStatus;                    // OUT
 } UVM_UNSET_ACCESSED_BY_PARAMS;
 
 // For managed allocations, UVM_MIGRATE implements the behavior described in
@@ -623,7 +588,7 @@ typedef struct
 // notification will be given on completion. If semaphoreAddress is non-zero
 // and the returned error code is NV_OK, semaphorePayload will be written to
 // semaphoreAddress once the migration is complete.
-#define UVM_MIGRATE_FLAG_ASYNC              0x00000001
+#define UVM_MIGRATE_FLAG_ASYNC 0x00000001
 
 // When the migration destination is the CPU, skip the step which creates new
 // virtual mappings on the CPU. Creating CPU mappings must wait for the
@@ -632,7 +597,7 @@ typedef struct
 // in the system support transparent access to pageable memory.
 //
 // The UVM driver must have builtin tests enabled for the API to use this flag.
-#define UVM_MIGRATE_FLAG_SKIP_CPU_MAP       0x00000002
+#define UVM_MIGRATE_FLAG_SKIP_CPU_MAP 0x00000002
 
 // By default UVM_MIGRATE returns an error if the destination UUID is a GPU
 // without a registered GPU VA space. Setting this flag skips that check, so the
@@ -642,13 +607,13 @@ typedef struct
 // the overhead of GPU PTE mappings.
 //
 // The UVM driver must have builtin tests enabled for the API to use this flag.
-#define UVM_MIGRATE_FLAG_NO_GPU_VA_SPACE    0x00000004
+#define UVM_MIGRATE_FLAG_NO_GPU_VA_SPACE 0x00000004
 
-#define UVM_MIGRATE_FLAGS_TEST_ALL              (UVM_MIGRATE_FLAG_SKIP_CPU_MAP      | \
-                                                 UVM_MIGRATE_FLAG_NO_GPU_VA_SPACE)
+#define UVM_MIGRATE_FLAGS_TEST_ALL                                             \
+  (UVM_MIGRATE_FLAG_SKIP_CPU_MAP | UVM_MIGRATE_FLAG_NO_GPU_VA_SPACE)
 
-#define UVM_MIGRATE_FLAGS_ALL                   (UVM_MIGRATE_FLAG_ASYNC | \
-                                                 UVM_MIGRATE_FLAGS_TEST_ALL)
+#define UVM_MIGRATE_FLAGS_ALL                                                  \
+  (UVM_MIGRATE_FLAG_ASYNC | UVM_MIGRATE_FLAGS_TEST_ALL)
 
 // If NV_ERR_INVALID_ARGUMENT is returned it is because cpuMemoryNode is not
 // valid and the destination processor is the CPU. cpuMemoryNode is considered
@@ -671,49 +636,45 @@ typedef struct
 // If NV_ERR_MORE_PROCESSING_REQUIRED is returned, user-space is responsible
 // for re-trying with a different cpuNumaNode, starting at userSpaceStart.
 //
-#define UVM_MIGRATE                                                   UVM_IOCTL_BASE(51)
-typedef struct
-{
-    NvU64           base               NV_ALIGN_BYTES(8); // IN
-    NvU64           length             NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid destinationUuid;                      // IN
-    NvU32           flags;                                // IN
-    NvU64           semaphoreAddress   NV_ALIGN_BYTES(8); // IN
-    NvU32           semaphorePayload;                     // IN
-    NvS32           cpuNumaNode;                          // IN
-    NvU64           userSpaceStart     NV_ALIGN_BYTES(8); // OUT
-    NvU64           userSpaceLength    NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS       rmStatus;                             // OUT
+#define UVM_MIGRATE UVM_IOCTL_BASE(51)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);             // IN
+  NvU64 length NV_ALIGN_BYTES(8);           // IN
+  NvProcessorUuid destinationUuid;          // IN
+  NvU32 flags;                              // IN
+  NvU64 semaphoreAddress NV_ALIGN_BYTES(8); // IN
+  NvU32 semaphorePayload;                   // IN
+  NvS32 cpuNumaNode;                        // IN
+  NvU64 userSpaceStart NV_ALIGN_BYTES(8);   // OUT
+  NvU64 userSpaceLength NV_ALIGN_BYTES(8);  // OUT
+  NV_STATUS rmStatus;                       // OUT
 } UVM_MIGRATE_PARAMS;
 
-#define UVM_MIGRATE_RANGE_GROUP                                       UVM_IOCTL_BASE(53)
-typedef struct
-{
-    NvU64           rangeGroupId       NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid destinationUuid;                      // IN
-    NV_STATUS       rmStatus;                             // OUT
+#define UVM_MIGRATE_RANGE_GROUP UVM_IOCTL_BASE(53)
+typedef struct {
+  NvU64 rangeGroupId NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid destinationUuid;      // IN
+  NV_STATUS rmStatus;                   // OUT
 } UVM_MIGRATE_RANGE_GROUP_PARAMS;
 
 //
 // UvmEnableSystemWideAtomics
 //
-#define UVM_ENABLE_SYSTEM_WIDE_ATOMICS                                UVM_IOCTL_BASE(54)
+#define UVM_ENABLE_SYSTEM_WIDE_ATOMICS UVM_IOCTL_BASE(54)
 
-typedef struct
-{
-    NvProcessorUuid gpu_uuid; // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpu_uuid; // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_ENABLE_SYSTEM_WIDE_ATOMICS_PARAMS;
 
 //
 // UvmDisableSystemWideAtomics
 //
-#define UVM_DISABLE_SYSTEM_WIDE_ATOMICS                               UVM_IOCTL_BASE(55)
+#define UVM_DISABLE_SYSTEM_WIDE_ATOMICS UVM_IOCTL_BASE(55)
 
-typedef struct
-{
-    NvProcessorUuid gpu_uuid; // IN
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NvProcessorUuid gpu_uuid; // IN
+  NV_STATUS rmStatus;       // OUT
 } UVM_DISABLE_SYSTEM_WIDE_ATOMICS_PARAMS;
 
 //
@@ -721,176 +682,160 @@ typedef struct
 // UvmToolsCreateEventQueue, UvmToolsCreateProcessAggregateCounters,
 // UvmToolsCreateProcessorCounters.
 //
-#define UVM_TOOLS_INIT_EVENT_TRACKER                                  UVM_IOCTL_BASE(56)
-typedef struct
-{
-    NvU64           queueBuffer        NV_ALIGN_BYTES(8); // IN
-    NvU64           queueBufferSize    NV_ALIGN_BYTES(8); // IN
-    NvU64           controlBuffer      NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid processor;                            // IN
-    NvU32           allProcessors;                        // IN
-    NvU32           uvmFd;                                // IN
-    NV_STATUS       rmStatus;                             // OUT
+#define UVM_TOOLS_INIT_EVENT_TRACKER UVM_IOCTL_BASE(56)
+typedef struct {
+  NvU64 queueBuffer NV_ALIGN_BYTES(8);     // IN
+  NvU64 queueBufferSize NV_ALIGN_BYTES(8); // IN
+  NvU64 controlBuffer NV_ALIGN_BYTES(8);   // IN
+  NvProcessorUuid processor;               // IN
+  NvU32 allProcessors;                     // IN
+  NvU32 uvmFd;                             // IN
+  NV_STATUS rmStatus;                      // OUT
 } UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS;
 
 //
 // UvmToolsSetNotificationThreshold
 //
-#define UVM_TOOLS_SET_NOTIFICATION_THRESHOLD                          UVM_IOCTL_BASE(57)
-typedef struct
-{
-    NvU32     notificationThreshold;                       // IN
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_SET_NOTIFICATION_THRESHOLD UVM_IOCTL_BASE(57)
+typedef struct {
+  NvU32 notificationThreshold; // IN
+  NV_STATUS rmStatus;          // OUT
 } UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS;
 
 //
 // UvmToolsEventQueueEnableEvents
 //
-#define UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS                           UVM_IOCTL_BASE(58)
-typedef struct
-{
-    NvU64     eventTypeFlags            NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS UVM_IOCTL_BASE(58)
+typedef struct {
+  NvU64 eventTypeFlags NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                     // OUT
 } UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS;
 
 //
 // UvmToolsEventQueueDisableEvents
 //
-#define UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS                          UVM_IOCTL_BASE(59)
-typedef struct
-{
-    NvU64     eventTypeFlags            NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS UVM_IOCTL_BASE(59)
+typedef struct {
+  NvU64 eventTypeFlags NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                     // OUT
 } UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS;
 
 //
 // UvmToolsEnableCounters
 //
-#define UVM_TOOLS_ENABLE_COUNTERS                                     UVM_IOCTL_BASE(60)
-typedef struct
-{
-    NvU64     counterTypeFlags          NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_ENABLE_COUNTERS UVM_IOCTL_BASE(60)
+typedef struct {
+  NvU64 counterTypeFlags NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                       // OUT
 } UVM_TOOLS_ENABLE_COUNTERS_PARAMS;
 
 //
 // UvmToolsDisableCounters
 //
-#define UVM_TOOLS_DISABLE_COUNTERS                                    UVM_IOCTL_BASE(61)
-typedef struct
-{
-    NvU64     counterTypeFlags          NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_DISABLE_COUNTERS UVM_IOCTL_BASE(61)
+typedef struct {
+  NvU64 counterTypeFlags NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;                       // OUT
 } UVM_TOOLS_DISABLE_COUNTERS_PARAMS;
 
 //
 // UvmToolsReadProcessMemory
 //
-#define UVM_TOOLS_READ_PROCESS_MEMORY                                 UVM_IOCTL_BASE(62)
-typedef struct
-{
-    NvU64     buffer                    NV_ALIGN_BYTES(8); // IN
-    NvU64     size                      NV_ALIGN_BYTES(8); // IN
-    NvU64     targetVa                  NV_ALIGN_BYTES(8); // IN
-    NvU64     bytesRead                 NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_READ_PROCESS_MEMORY UVM_IOCTL_BASE(62)
+typedef struct {
+  NvU64 buffer NV_ALIGN_BYTES(8);    // IN
+  NvU64 size NV_ALIGN_BYTES(8);      // IN
+  NvU64 targetVa NV_ALIGN_BYTES(8);  // IN
+  NvU64 bytesRead NV_ALIGN_BYTES(8); // OUT
+  NV_STATUS rmStatus;                // OUT
 } UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS;
 
 //
 // UvmToolsWriteProcessMemory
 //
-#define UVM_TOOLS_WRITE_PROCESS_MEMORY                                UVM_IOCTL_BASE(63)
-typedef struct
-{
-    NvU64     buffer                    NV_ALIGN_BYTES(8); // IN
-    NvU64     size                      NV_ALIGN_BYTES(8); // IN
-    NvU64     targetVa                  NV_ALIGN_BYTES(8); // IN
-    NvU64     bytesWritten              NV_ALIGN_BYTES(8); // OUT
-    NV_STATUS rmStatus;                                    // OUT
+#define UVM_TOOLS_WRITE_PROCESS_MEMORY UVM_IOCTL_BASE(63)
+typedef struct {
+  NvU64 buffer NV_ALIGN_BYTES(8);       // IN
+  NvU64 size NV_ALIGN_BYTES(8);         // IN
+  NvU64 targetVa NV_ALIGN_BYTES(8);     // IN
+  NvU64 bytesWritten NV_ALIGN_BYTES(8); // OUT
+  NV_STATUS rmStatus;                   // OUT
 } UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS;
 
 //
 // UvmToolsGetProcessorUuidTable
 //
-#define UVM_TOOLS_GET_PROCESSOR_UUID_TABLE                            UVM_IOCTL_BASE(64)
-typedef struct
-{
-    NvU64     tablePtr                 NV_ALIGN_BYTES(8); // IN
-    NV_STATUS rmStatus;                                   // OUT
+#define UVM_TOOLS_GET_PROCESSOR_UUID_TABLE UVM_IOCTL_BASE(64)
+typedef struct {
+  NvU64 tablePtr NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;               // OUT
 } UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS;
 
 //
 // UvmMapDynamicParallelismRegion
 //
-#define UVM_MAP_DYNAMIC_PARALLELISM_REGION                            UVM_IOCTL_BASE(65)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid         gpuUuid;                                           // IN
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_MAP_DYNAMIC_PARALLELISM_REGION UVM_IOCTL_BASE(65)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid gpuUuid;        // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_MAP_DYNAMIC_PARALLELISM_REGION_PARAMS;
 
 //
 // UvmUnmapExternal
 //
-#define UVM_UNMAP_EXTERNAL                                            UVM_IOCTL_BASE(66)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid         gpuUuid;                                           // IN
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_UNMAP_EXTERNAL UVM_IOCTL_BASE(66)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid gpuUuid;        // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_UNMAP_EXTERNAL_PARAMS;
 
-
 //
 // UvmToolsFlushEvents
 //
-#define UVM_TOOLS_FLUSH_EVENTS                                        UVM_IOCTL_BASE(67)
-typedef struct
-{
-    NV_STATUS rmStatus;                                   // OUT
+#define UVM_TOOLS_FLUSH_EVENTS UVM_IOCTL_BASE(67)
+typedef struct {
+  NV_STATUS rmStatus; // OUT
 } UVM_TOOLS_FLUSH_EVENTS_PARAMS;
 
 //
 // UvmAllocSemaphorePool
 //
-#define UVM_ALLOC_SEMAPHORE_POOL                                     UVM_IOCTL_BASE(68)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    UvmGpuMappingAttributes perGpuAttributes[UVM_MAX_GPUS];                    // IN
-    NvU64                   gpuAttributesCount              NV_ALIGN_BYTES(8); // IN
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_ALLOC_SEMAPHORE_POOL UVM_IOCTL_BASE(68)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);                           // IN
+  NvU64 length NV_ALIGN_BYTES(8);                         // IN
+  UvmGpuMappingAttributes perGpuAttributes[UVM_MAX_GPUS]; // IN
+  NvU64 gpuAttributesCount NV_ALIGN_BYTES(8);             // IN
+  NV_STATUS rmStatus;                                     // OUT
 } UVM_ALLOC_SEMAPHORE_POOL_PARAMS;
 
 //
 // UvmCleanUpZombieResources
 //
-#define UVM_CLEAN_UP_ZOMBIE_RESOURCES                                 UVM_IOCTL_BASE(69)
-typedef struct
-{
-    NV_STATUS rmStatus;                    // OUT
+#define UVM_CLEAN_UP_ZOMBIE_RESOURCES UVM_IOCTL_BASE(69)
+typedef struct {
+  NV_STATUS rmStatus; // OUT
 } UVM_CLEAN_UP_ZOMBIE_RESOURCES_PARAMS;
 
 //
 // UvmIsPageableMemoryAccessSupportedOnGpu
 //
-#define UVM_PAGEABLE_MEM_ACCESS_ON_GPU                                UVM_IOCTL_BASE(70)
+#define UVM_PAGEABLE_MEM_ACCESS_ON_GPU UVM_IOCTL_BASE(70)
 
-typedef struct
-{
-    NvProcessorUuid gpu_uuid;          // IN
-    NvBool          pageableMemAccess; // OUT
-    NV_STATUS       rmStatus;          // OUT
+typedef struct {
+  NvProcessorUuid gpu_uuid; // IN
+  NvBool pageableMemAccess; // OUT
+  NV_STATUS rmStatus;       // OUT
 } UVM_PAGEABLE_MEM_ACCESS_ON_GPU_PARAMS;
 
 //
 // UvmPopulatePageable
 //
-#define UVM_POPULATE_PAGEABLE                                         UVM_IOCTL_BASE(71)
+#define UVM_POPULATE_PAGEABLE UVM_IOCTL_BASE(71)
 
 // Allow population of managed ranges. The goal is to validate that it is
 // possible to populate pageable ranges backed by VMAs with the VM_MIXEDMAP or
@@ -899,63 +844,61 @@ typedef struct
 // flags of an already-created VMA from kernel space, we take advantage of the
 // fact that managed ranges have both special flags set at creation time (see
 // uvm_mmap).
-#define UVM_POPULATE_PAGEABLE_FLAG_ALLOW_MANAGED              0x00000001
+#define UVM_POPULATE_PAGEABLE_FLAG_ALLOW_MANAGED 0x00000001
 
 // By default UVM_POPULATE_PAGEABLE returns an error if the destination vma
 // does not have read permission. This flag skips that check.
-#define UVM_POPULATE_PAGEABLE_FLAG_SKIP_PROT_CHECK            0x00000002
+#define UVM_POPULATE_PAGEABLE_FLAG_SKIP_PROT_CHECK 0x00000002
 
 // By default UVM_POPULATE_PAGEABLE returns an error if the destination vma
 // is VM_IO or VM_PFNMAP. This flag skips that check.
-#define UVM_POPULATE_PAGEABLE_FLAG_ALLOW_SPECIAL              0x00000004
+#define UVM_POPULATE_PAGEABLE_FLAG_ALLOW_SPECIAL 0x00000004
 
 // These flags are used internally within the driver and are not allowed from
 // user space.
-#define UVM_POPULATE_PAGEABLE_FLAGS_INTERNAL    UVM_POPULATE_PAGEABLE_FLAG_ALLOW_SPECIAL
+#define UVM_POPULATE_PAGEABLE_FLAGS_INTERNAL                                   \
+  UVM_POPULATE_PAGEABLE_FLAG_ALLOW_SPECIAL
 
 // These flags are allowed from user space only when builtin tests are enabled.
 // Some of them may also be used internally within the driver in non-test use
 // cases.
-#define UVM_POPULATE_PAGEABLE_FLAGS_TEST        (UVM_POPULATE_PAGEABLE_FLAG_ALLOW_MANAGED | \
-                                                 UVM_POPULATE_PAGEABLE_FLAG_SKIP_PROT_CHECK)
-
-#define UVM_POPULATE_PAGEABLE_FLAGS_ALL         (UVM_POPULATE_PAGEABLE_FLAGS_INTERNAL | \
-                                                 UVM_POPULATE_PAGEABLE_FLAGS_TEST)
-
-typedef struct
-{
-    NvU64           base      NV_ALIGN_BYTES(8); // IN
-    NvU64           length    NV_ALIGN_BYTES(8); // IN
-    NvU32           flags;                       // IN
-    NV_STATUS       rmStatus;                    // OUT
+#define UVM_POPULATE_PAGEABLE_FLAGS_TEST                                       \
+  (UVM_POPULATE_PAGEABLE_FLAG_ALLOW_MANAGED |                                  \
+   UVM_POPULATE_PAGEABLE_FLAG_SKIP_PROT_CHECK)
+
+#define UVM_POPULATE_PAGEABLE_FLAGS_ALL                                        \
+  (UVM_POPULATE_PAGEABLE_FLAGS_INTERNAL | UVM_POPULATE_PAGEABLE_FLAGS_TEST)
+
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvU32 flags;                    // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_POPULATE_PAGEABLE_PARAMS;
 
 //
 // UvmValidateVaRange
 //
-#define UVM_VALIDATE_VA_RANGE                                         UVM_IOCTL_BASE(72)
-typedef struct
-{
-    NvU64           base      NV_ALIGN_BYTES(8); // IN
-    NvU64           length    NV_ALIGN_BYTES(8); // IN
-    NV_STATUS       rmStatus;                    // OUT
+#define UVM_VALIDATE_VA_RANGE UVM_IOCTL_BASE(72)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_VALIDATE_VA_RANGE_PARAMS;
 
-#define UVM_CREATE_EXTERNAL_RANGE                                     UVM_IOCTL_BASE(73)
-typedef struct
-{
-    NvU64                  base                             NV_ALIGN_BYTES(8); // IN
-    NvU64                  length                           NV_ALIGN_BYTES(8); // IN
-    NV_STATUS              rmStatus;                                           // OUT
+#define UVM_CREATE_EXTERNAL_RANGE UVM_IOCTL_BASE(73)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_CREATE_EXTERNAL_RANGE_PARAMS;
 
-#define UVM_MAP_EXTERNAL_SPARSE                                       UVM_IOCTL_BASE(74)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid         gpuUuid;                                           // IN
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_MAP_EXTERNAL_SPARSE UVM_IOCTL_BASE(74)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid gpuUuid;        // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_MAP_EXTERNAL_SPARSE_PARAMS;
 
 //
@@ -986,53 +929,51 @@ typedef struct
 // Not all platforms require this secondary file-descriptor. On those
 // platforms NV_WARN_NOTHING_TO_DO will be returned and users may
 // close the file-descriptor at anytime.
-#define UVM_MM_INITIALIZE                                             UVM_IOCTL_BASE(75)
-typedef struct
-{
-    NvS32                   uvmFd;    // IN
-    NV_STATUS               rmStatus; // OUT
+#define UVM_MM_INITIALIZE UVM_IOCTL_BASE(75)
+typedef struct {
+  NvS32 uvmFd;        // IN
+  NV_STATUS rmStatus; // OUT
 } UVM_MM_INITIALIZE_PARAMS;
 
-#define UVM_TOOLS_INIT_EVENT_TRACKER_V2                               UVM_IOCTL_BASE(76)
-typedef UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS;
+#define UVM_TOOLS_INIT_EVENT_TRACKER_V2 UVM_IOCTL_BASE(76)
+typedef UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS
+    UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS;
 
-#define UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2                         UVM_IOCTL_BASE(77)
-typedef UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS;
+#define UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2 UVM_IOCTL_BASE(77)
+typedef UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS;
 
 //
 // UvmAllocDeviceP2P
 //
-#define UVM_ALLOC_DEVICE_P2P                                          UVM_IOCTL_BASE(78)
-typedef struct
-{
-    NvU64                   base                            NV_ALIGN_BYTES(8); // IN
-    NvU64                   length                          NV_ALIGN_BYTES(8); // IN
-    NvU64                   offset                          NV_ALIGN_BYTES(8); // IN
-    NvProcessorUuid         gpuUuid;                                           // IN
-    NvS32                   rmCtrlFd;                                          // IN
-    NvU32                   hClient;                                           // IN
-    NvU32                   hMemory;                                           // IN
-
-    NV_STATUS               rmStatus;                                          // OUT
+#define UVM_ALLOC_DEVICE_P2P UVM_IOCTL_BASE(78)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvU64 offset NV_ALIGN_BYTES(8); // IN
+  NvProcessorUuid gpuUuid;        // IN
+  NvS32 rmCtrlFd;                 // IN
+  NvU32 hClient;                  // IN
+  NvU32 hMemory;                  // IN
+
+  NV_STATUS rmStatus; // OUT
 } UVM_ALLOC_DEVICE_P2P_PARAMS;
 
-#define UVM_CLEAR_ALL_ACCESS_COUNTERS                                 UVM_IOCTL_BASE(79)
+#define UVM_CLEAR_ALL_ACCESS_COUNTERS UVM_IOCTL_BASE(79)
 
-typedef struct
-{
-    NV_STATUS       rmStatus; // OUT
+typedef struct {
+  NV_STATUS rmStatus; // OUT
 } UVM_CLEAR_ALL_ACCESS_COUNTERS_PARAMS;
 
 //
 // UvmDiscard
 //
-#define UVM_DISCARD                                                   UVM_IOCTL_BASE(80)
-typedef struct
-{
-    NvU64           base                                    NV_ALIGN_BYTES(8); // IN
-    NvU64           length                                  NV_ALIGN_BYTES(8); // IN
-    NvU64           flags                                   NV_ALIGN_BYTES(8); // IN
-    NV_STATUS       rmStatus;                                                  // OUT
+#define UVM_DISCARD UVM_IOCTL_BASE(80)
+typedef struct {
+  NvU64 base NV_ALIGN_BYTES(8);   // IN
+  NvU64 length NV_ALIGN_BYTES(8); // IN
+  NvU64 flags NV_ALIGN_BYTES(8);  // IN
+  NV_STATUS rmStatus;             // OUT
 } UVM_DISCARD_PARAMS;
 
 //
@@ -1044,14 +985,24 @@ typedef struct
 //
 // UvmIs8Supported
 //
-#define UVM_IS_8_SUPPORTED                                            UVM_IOCTL_BASE(2047)
+#define UVM_IS_8_SUPPORTED UVM_IOCTL_BASE(2047)
 
-typedef struct
-{
-    NvU32     is8Supported; // OUT
-    NV_STATUS rmStatus;     // OUT
+typedef struct {
+  NvU32 is8Supported; // OUT
+  NV_STATUS rmStatus; // OUT
 } UVM_IS_8_SUPPORTED_PARAMS;
 
+// added by kymartin for dumping GPU memory.
+#define UVM_DUMP_GPU_MEMORY UVM_IOCTL_BASE(111)
+typedef struct {
+  NvProcessorUuid gpu_uuid;          // IN
+  NvS32 child_id;                    // IN
+  NvU64 base_addr NV_ALIGN_BYTES(8); // IN
+  NvU64 dump_size;                   // IN
+  NvU64 out_addr NV_ALIGN_BYTES(8);  // OUT
+  NV_STATUS rmStatus;                // OUT
+} UVM_DUMP_GPU_MEMORY_PARAMS;
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/kernel-open/nvidia-uvm/uvm_tools.c b/kernel-open/nvidia-uvm/uvm_tools.c
index 9d4c55be..c3e324f4 100644
--- a/kernel-open/nvidia-uvm/uvm_tools.c
+++ b/kernel-open/nvidia-uvm/uvm_tools.c
@@ -20,79 +20,77 @@
     DEALINGS IN THE SOFTWARE.
 
 *******************************************************************************/
+#include "uvm_tools.h"
+#include "nv_speculation_barrier.h"
+#include "uvm_api.h"
+#include "uvm_channel.h"
 #include "uvm_common.h"
-#include "uvm_ioctl.h"
+#include "uvm_forward_decl.h"
 #include "uvm_global.h"
 #include "uvm_gpu.h"
 #include "uvm_hal.h"
-#include "uvm_tools.h"
-#include "uvm_tools_init.h"
-#include "uvm_va_space.h"
-#include "uvm_api.h"
 #include "uvm_hal_types.h"
-#include "uvm_va_block.h"
-#include "uvm_va_range.h"
+#include "uvm_ioctl.h"
+#include "uvm_mem.h"
+#include "uvm_mmu.h"
+#include "uvm_pmm_gpu.h"
 #include "uvm_push.h"
-#include "uvm_forward_decl.h"
 #include "uvm_range_group.h"
-#include "uvm_mem.h"
-#include "nv_speculation_barrier.h"
+#include "uvm_tools_init.h"
+#include "uvm_va_block.h"
+#include "uvm_va_range.h"
+#include "uvm_va_space.h"
 
 // We limit the number of times a page can be retained by the kernel
 // to prevent the user from maliciously passing UVM tools the same page
 // over and over again in an attempt to overflow the refcount.
 #define MAX_PAGE_COUNT (1 << 20)
 
-typedef struct
-{
-    NvU32 get_ahead;
-    NvU32 get_behind;
-    NvU32 put_ahead;
-    NvU32 put_behind;
+typedef struct {
+  NvU32 get_ahead;
+  NvU32 get_behind;
+  NvU32 put_ahead;
+  NvU32 put_behind;
 } uvm_tools_queue_snapshot_t;
 
-typedef struct
-{
-    uvm_spinlock_t lock;
-    NvU64 subscribed_queues;
-    struct list_head queue_nodes[UvmEventNumTypesAll];
+typedef struct {
+  uvm_spinlock_t lock;
+  NvU64 subscribed_queues;
+  struct list_head queue_nodes[UvmEventNumTypesAll];
 
-    struct page **queue_buffer_pages;
-    void *queue_buffer;
-    NvU32 queue_buffer_count;
-    NvU32 notification_threshold;
+  struct page **queue_buffer_pages;
+  void *queue_buffer;
+  NvU32 queue_buffer_count;
+  NvU32 notification_threshold;
 
-    struct page **control_buffer_pages;
-    UvmToolsEventControlData *control;
+  struct page **control_buffer_pages;
+  UvmToolsEventControlData *control;
 
-    wait_queue_head_t wait_queue;
-    bool is_wakeup_get_valid;
-    NvU32 wakeup_get;
+  wait_queue_head_t wait_queue;
+  bool is_wakeup_get_valid;
+  NvU32 wakeup_get;
 } uvm_tools_queue_t;
 
-typedef struct
-{
-    struct list_head counter_nodes[UVM_TOTAL_COUNTERS];
-    NvU64 subscribed_counters;
+typedef struct {
+  struct list_head counter_nodes[UVM_TOTAL_COUNTERS];
+  NvU64 subscribed_counters;
 
-    struct page **counter_buffer_pages;
-    NvU64 *counters;
+  struct page **counter_buffer_pages;
+  NvU64 *counters;
 
-    bool all_processors;
-    NvProcessorUuid processor;
+  bool all_processors;
+  NvProcessorUuid processor;
 } uvm_tools_counter_t;
 
 // private_data for /dev/nvidia-uvm-tools
-typedef struct
-{
-    size_t entry_size;
-    bool is_queue;
-    struct file *uvm_file;
-    union
-    {
-        uvm_tools_queue_t queue;
-        uvm_tools_counter_t counter;
-    };
+typedef struct {
+  size_t entry_size;
+  bool is_queue;
+  struct file *uvm_file;
+  union {
+    uvm_tools_queue_t queue;
+    uvm_tools_counter_t counter;
+  };
 } uvm_tools_event_tracker_t;
 
 // Delayed events
@@ -109,83 +107,79 @@ typedef struct
 // events with gpus ids that have been removed.
 
 // This object describes the pending migrations operations within a VA block
-typedef struct
-{
-    nv_kthread_q_item_t queue_item;
-    uvm_processor_id_t dst;
-    NvS16 dst_nid;
-    uvm_processor_id_t src;
-    NvS16 src_nid;
-    uvm_va_space_t *va_space;
-
-    uvm_channel_t *channel;
-    struct list_head events;
-    NvU64 start_timestamp_cpu;
-    NvU64 end_timestamp_cpu;
-    NvU64 *start_timestamp_gpu_addr;
-    NvU64 start_timestamp_gpu;
-    NvU64 range_group_id;
+typedef struct {
+  nv_kthread_q_item_t queue_item;
+  uvm_processor_id_t dst;
+  NvS16 dst_nid;
+  uvm_processor_id_t src;
+  NvS16 src_nid;
+  uvm_va_space_t *va_space;
+
+  uvm_channel_t *channel;
+  struct list_head events;
+  NvU64 start_timestamp_cpu;
+  NvU64 end_timestamp_cpu;
+  NvU64 *start_timestamp_gpu_addr;
+  NvU64 start_timestamp_gpu;
+  NvU64 range_group_id;
 } block_migration_data_t;
 
 // This object represents a specific pending migration within a VA block
-typedef struct
-{
-    struct list_head events_node;
-    NvU64 bytes;
-    NvU64 address;
-    NvU64 *end_timestamp_gpu_addr;
-    NvU64 end_timestamp_gpu;
-    UvmEventMigrationCause cause;
+typedef struct {
+  struct list_head events_node;
+  NvU64 bytes;
+  NvU64 address;
+  NvU64 *end_timestamp_gpu_addr;
+  NvU64 end_timestamp_gpu;
+  UvmEventMigrationCause cause;
 } migration_data_t;
 
 // This object represents a pending gpu faut replay operation
-typedef struct
-{
-    nv_kthread_q_item_t queue_item;
-    uvm_channel_t *channel;
-    uvm_gpu_id_t gpu_id;
-    NvU32 batch_id;
-    uvm_fault_client_type_t client_type;
-    NvU64 timestamp;
-    NvU64 timestamp_gpu;
-    NvU64 *timestamp_gpu_addr;
+typedef struct {
+  nv_kthread_q_item_t queue_item;
+  uvm_channel_t *channel;
+  uvm_gpu_id_t gpu_id;
+  NvU32 batch_id;
+  uvm_fault_client_type_t client_type;
+  NvU64 timestamp;
+  NvU64 timestamp_gpu;
+  NvU64 *timestamp_gpu_addr;
 } replay_data_t;
 
 // This object describes the pending map remote operations within a VA block
-typedef struct
-{
-    nv_kthread_q_item_t queue_item;
-    uvm_processor_id_t src;
-    uvm_processor_id_t dst;
-    UvmEventMapRemoteCause cause;
-    NvU64 timestamp;
-    uvm_va_space_t *va_space;
-
-    uvm_channel_t *channel;
-    struct list_head events;
+typedef struct {
+  nv_kthread_q_item_t queue_item;
+  uvm_processor_id_t src;
+  uvm_processor_id_t dst;
+  UvmEventMapRemoteCause cause;
+  NvU64 timestamp;
+  uvm_va_space_t *va_space;
+
+  uvm_channel_t *channel;
+  struct list_head events;
 } block_map_remote_data_t;
 
 // This object represents a pending map remote operation
-typedef struct
-{
-    struct list_head events_node;
-
-    NvU64 address;
-    NvU64 size;
-    NvU64 timestamp_gpu;
-    NvU64 *timestamp_gpu_addr;
-} map_remote_data_t;
+typedef struct {
+  struct list_head events_node;
 
+  NvU64 address;
+  NvU64 size;
+  NvU64 timestamp_gpu;
+  NvU64 *timestamp_gpu_addr;
+} map_remote_data_t;
 
 static struct cdev g_uvm_tools_cdev;
 static LIST_HEAD(g_tools_va_space_list);
 static NvU32 g_tools_enabled_event_count[UvmEventNumTypesAll];
 static uvm_rw_semaphore_t g_tools_va_space_list_lock;
 static struct kmem_cache *g_tools_event_tracker_cache __read_mostly = NULL;
-static struct kmem_cache *g_tools_block_migration_data_cache __read_mostly = NULL;
+static struct kmem_cache *g_tools_block_migration_data_cache __read_mostly =
+    NULL;
 static struct kmem_cache *g_tools_migration_data_cache __read_mostly = NULL;
 static struct kmem_cache *g_tools_replay_data_cache __read_mostly = NULL;
-static struct kmem_cache *g_tools_block_map_remote_data_cache __read_mostly = NULL;
+static struct kmem_cache *g_tools_block_map_remote_data_cache __read_mostly =
+    NULL;
 static struct kmem_cache *g_tools_map_remote_data_cache __read_mostly = NULL;
 static uvm_spinlock_t g_tools_channel_list_lock;
 static LIST_HEAD(g_tools_channel_list);
@@ -193,564 +187,549 @@ static nv_kthread_q_t g_tools_queue;
 
 static NV_STATUS tools_update_status(uvm_va_space_t *va_space);
 
-static uvm_tools_event_tracker_t *tools_event_tracker(struct file *filp)
-{
-    return (uvm_tools_event_tracker_t *)atomic_long_read((atomic_long_t *)&filp->private_data);
+static uvm_tools_event_tracker_t *tools_event_tracker(struct file *filp) {
+  return (uvm_tools_event_tracker_t *)atomic_long_read(
+      (atomic_long_t *)&filp->private_data);
 }
 
-static bool tracker_is_queue(uvm_tools_event_tracker_t *event_tracker)
-{
-    return event_tracker != NULL && event_tracker->is_queue;
+static bool tracker_is_queue(uvm_tools_event_tracker_t *event_tracker) {
+  return event_tracker != NULL && event_tracker->is_queue;
 }
 
-static bool tracker_is_counter(uvm_tools_event_tracker_t *event_tracker)
-{
-    return event_tracker != NULL && !event_tracker->is_queue;
+static bool tracker_is_counter(uvm_tools_event_tracker_t *event_tracker) {
+  return event_tracker != NULL && !event_tracker->is_queue;
 }
 
-static uvm_va_space_t *tools_event_tracker_va_space(uvm_tools_event_tracker_t *event_tracker)
-{
-    uvm_va_space_t *va_space;
-    UVM_ASSERT(event_tracker->uvm_file);
-    va_space = uvm_va_space_get(event_tracker->uvm_file);
-    return va_space;
+static uvm_va_space_t *
+tools_event_tracker_va_space(uvm_tools_event_tracker_t *event_tracker) {
+  uvm_va_space_t *va_space;
+  UVM_ASSERT(event_tracker->uvm_file);
+  va_space = uvm_va_space_get(event_tracker->uvm_file);
+  return va_space;
 }
 
-static void uvm_put_user_pages_dirty(struct page **pages, NvU64 page_count)
-{
-    NvU64 i;
+static void uvm_put_user_pages_dirty(struct page **pages, NvU64 page_count) {
+  NvU64 i;
 
-    for (i = 0; i < page_count; i++) {
-        set_page_dirty(pages[i]);
-        NV_UNPIN_USER_PAGE(pages[i]);
-    }
+  for (i = 0; i < page_count; i++) {
+    set_page_dirty(pages[i]);
+    NV_UNPIN_USER_PAGE(pages[i]);
+  }
 }
 
-static void unmap_user_pages(struct page **pages, void *addr, NvU64 size)
-{
-    size = DIV_ROUND_UP(size, PAGE_SIZE);
-    vunmap((NvU8 *)addr);
-    uvm_put_user_pages_dirty(pages, size);
-    uvm_kvfree(pages);
+static void unmap_user_pages(struct page **pages, void *addr, NvU64 size) {
+  size = DIV_ROUND_UP(size, PAGE_SIZE);
+  vunmap((NvU8 *)addr);
+  uvm_put_user_pages_dirty(pages, size);
+  uvm_kvfree(pages);
 }
 
 // This must be called with the mmap_lock held in read mode or better.
-static NV_STATUS check_vmas(struct mm_struct *mm, NvU64 start_va, NvU64 size)
-{
-    struct vm_area_struct *vma;
-    NvU64 addr = start_va;
-    NvU64 region_end = start_va + size;
-
-    do {
-        vma = find_vma(mm, addr);
-        if (!vma || !(addr >= vma->vm_start) || uvm_file_is_nvidia_uvm(vma->vm_file))
-            return NV_ERR_INVALID_ARGUMENT;
-
-        addr = vma->vm_end;
-    } while (addr < region_end);
-
-    return NV_OK;
-}
-
-// Map virtual memory of data from [user_va, user_va + size) of current process into kernel.
-// Sets *addr to kernel mapping and *pages to the array of struct pages that contain the memory.
-static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct page ***pages)
-{
-    NV_STATUS status = NV_OK;
-    long ret = 0;
-    long num_pages;
-    long i;
-
-    *addr = NULL;
-    *pages = NULL;
-    num_pages = DIV_ROUND_UP(size, PAGE_SIZE);
-
-    if (uvm_api_range_invalid(user_va, num_pages * PAGE_SIZE)) {
-        status = NV_ERR_INVALID_ADDRESS;
-        goto fail;
-    }
-
-    *pages = uvm_kvmalloc(sizeof(struct page *) * num_pages);
-    if (*pages == NULL) {
-        status = NV_ERR_NO_MEMORY;
-        goto fail;
-    }
-
-    // Although uvm_down_read_mmap_lock() is preferable due to its participation
-    // in the UVM lock dependency tracker, it cannot be used here. That's
-    // because pin_user_pages() may fault in HMM pages which are GPU-resident.
-    // When that happens, the UVM page fault handler would record another
-    // mmap_read_lock() on the same thread as this one, leading to a false
-    // positive lock dependency report.
-    //
-    // Therefore, use the lower level nv_mmap_read_lock() here.
-    nv_mmap_read_lock(current->mm);
-    status = check_vmas(current->mm, user_va, size);
-    if (status != NV_OK) {
-        nv_mmap_read_unlock(current->mm);
-        goto fail;
-    }
-    ret = NV_PIN_USER_PAGES(user_va, num_pages, FOLL_WRITE, *pages);
+static NV_STATUS check_vmas(struct mm_struct *mm, NvU64 start_va, NvU64 size) {
+  struct vm_area_struct *vma;
+  NvU64 addr = start_va;
+  NvU64 region_end = start_va + size;
+
+  do {
+    vma = find_vma(mm, addr);
+    if (!vma || !(addr >= vma->vm_start) ||
+        uvm_file_is_nvidia_uvm(vma->vm_file))
+      return NV_ERR_INVALID_ARGUMENT;
+
+    addr = vma->vm_end;
+  } while (addr < region_end);
+
+  return NV_OK;
+}
+
+// Map virtual memory of data from [user_va, user_va + size) of current process
+// into kernel. Sets *addr to kernel mapping and *pages to the array of struct
+// pages that contain the memory.
+static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr,
+                                struct page ***pages) {
+  NV_STATUS status = NV_OK;
+  long ret = 0;
+  long num_pages;
+  long i;
+
+  *addr = NULL;
+  *pages = NULL;
+  num_pages = DIV_ROUND_UP(size, PAGE_SIZE);
+
+  if (uvm_api_range_invalid(user_va, num_pages * PAGE_SIZE)) {
+    status = NV_ERR_INVALID_ADDRESS;
+    goto fail;
+  }
+
+  *pages = uvm_kvmalloc(sizeof(struct page *) * num_pages);
+  if (*pages == NULL) {
+    status = NV_ERR_NO_MEMORY;
+    goto fail;
+  }
+
+  // Although uvm_down_read_mmap_lock() is preferable due to its participation
+  // in the UVM lock dependency tracker, it cannot be used here. That's
+  // because pin_user_pages() may fault in HMM pages which are GPU-resident.
+  // When that happens, the UVM page fault handler would record another
+  // mmap_read_lock() on the same thread as this one, leading to a false
+  // positive lock dependency report.
+  //
+  // Therefore, use the lower level nv_mmap_read_lock() here.
+  nv_mmap_read_lock(current->mm);
+  status = check_vmas(current->mm, user_va, size);
+  if (status != NV_OK) {
     nv_mmap_read_unlock(current->mm);
+    goto fail;
+  }
+  ret = NV_PIN_USER_PAGES(user_va, num_pages, FOLL_WRITE, *pages);
+  nv_mmap_read_unlock(current->mm);
 
-    if (ret != num_pages) {
-        status = NV_ERR_INVALID_ARGUMENT;
-        goto fail;
-    }
+  if (ret != num_pages) {
+    status = NV_ERR_INVALID_ARGUMENT;
+    goto fail;
+  }
 
-    for (i = 0; i < num_pages; i++) {
-        if (page_count((*pages)[i]) > MAX_PAGE_COUNT) {
-            status = NV_ERR_INVALID_ARGUMENT;
-            goto fail;
-        }
+  for (i = 0; i < num_pages; i++) {
+    if (page_count((*pages)[i]) > MAX_PAGE_COUNT) {
+      status = NV_ERR_INVALID_ARGUMENT;
+      goto fail;
     }
+  }
 
-    *addr = vmap(*pages, num_pages, VM_MAP, PAGE_KERNEL);
-    if (*addr == NULL)
-        goto fail;
+  *addr = vmap(*pages, num_pages, VM_MAP, PAGE_KERNEL);
+  if (*addr == NULL)
+    goto fail;
 
-    return NV_OK;
+  return NV_OK;
 
 fail:
-    if (*pages == NULL)
-        return status;
+  if (*pages == NULL)
+    return status;
 
-    if (ret > 0)
-        uvm_put_user_pages_dirty(*pages, ret);
-    else if (ret < 0)
-        status = errno_to_nv_status(ret);
+  if (ret > 0)
+    uvm_put_user_pages_dirty(*pages, ret);
+  else if (ret < 0)
+    status = errno_to_nv_status(ret);
 
-    uvm_kvfree(*pages);
-    *pages = NULL;
-    return status;
+  uvm_kvfree(*pages);
+  *pages = NULL;
+  return status;
 }
 
 static void insert_event_tracker(uvm_va_space_t *va_space,
-                                 struct list_head *node,
-                                 NvU32 list_count,
-                                 NvU64 list_mask,
-                                 NvU64 *subscribed_mask,
+                                 struct list_head *node, NvU32 list_count,
+                                 NvU64 list_mask, NvU64 *subscribed_mask,
                                  struct list_head *lists,
-                                 NvU64 *inserted_lists)
-{
-    NvU32 i;
-    NvU64 insertable_lists = list_mask & ~*subscribed_mask;
-
-    uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
-    uvm_assert_rwsem_locked_write(&va_space->tools.lock);
-
-    for (i = 0; i < list_count; i++) {
-        if (insertable_lists & (1ULL << i)) {
-            ++g_tools_enabled_event_count[i];
-            list_add(node + i, lists + i);
-        }
+                                 NvU64 *inserted_lists) {
+  NvU32 i;
+  NvU64 insertable_lists = list_mask & ~*subscribed_mask;
+
+  uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
+  uvm_assert_rwsem_locked_write(&va_space->tools.lock);
+
+  for (i = 0; i < list_count; i++) {
+    if (insertable_lists & (1ULL << i)) {
+      ++g_tools_enabled_event_count[i];
+      list_add(node + i, lists + i);
     }
+  }
 
-    *subscribed_mask |= list_mask;
-    *inserted_lists = insertable_lists;
+  *subscribed_mask |= list_mask;
+  *inserted_lists = insertable_lists;
 }
 
 static void remove_event_tracker(uvm_va_space_t *va_space,
-                                 struct list_head *node,
-                                 NvU32 list_count,
-                                 NvU64 list_mask,
-                                 NvU64 *subscribed_mask)
-{
-    NvU32 i;
-    NvU64 removable_lists = list_mask & *subscribed_mask;
-
-    uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
-    uvm_assert_rwsem_locked_write(&va_space->tools.lock);
-
-    for (i = 0; i < list_count; i++) {
-        if (removable_lists & (1ULL << i)) {
-            UVM_ASSERT(g_tools_enabled_event_count[i] > 0);
-            --g_tools_enabled_event_count[i];
-            list_del(node + i);
-        }
+                                 struct list_head *node, NvU32 list_count,
+                                 NvU64 list_mask, NvU64 *subscribed_mask) {
+  NvU32 i;
+  NvU64 removable_lists = list_mask & *subscribed_mask;
+
+  uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
+  uvm_assert_rwsem_locked_write(&va_space->tools.lock);
+
+  for (i = 0; i < list_count; i++) {
+    if (removable_lists & (1ULL << i)) {
+      UVM_ASSERT(g_tools_enabled_event_count[i] > 0);
+      --g_tools_enabled_event_count[i];
+      list_del(node + i);
     }
+  }
 
-    *subscribed_mask &= ~list_mask;
+  *subscribed_mask &= ~list_mask;
 }
 
-static bool queue_needs_wakeup(uvm_tools_queue_t *queue, uvm_tools_queue_snapshot_t *sn)
-{
-    NvU32 queue_mask = queue->queue_buffer_count - 1;
+static bool queue_needs_wakeup(uvm_tools_queue_t *queue,
+                               uvm_tools_queue_snapshot_t *sn) {
+  NvU32 queue_mask = queue->queue_buffer_count - 1;
 
-    uvm_assert_spinlock_locked(&queue->lock);
-    return ((queue->queue_buffer_count + sn->put_behind - sn->get_ahead) & queue_mask) >= queue->notification_threshold;
+  uvm_assert_spinlock_locked(&queue->lock);
+  return ((queue->queue_buffer_count + sn->put_behind - sn->get_ahead) &
+          queue_mask) >= queue->notification_threshold;
 }
 
-static void destroy_event_tracker(uvm_tools_event_tracker_t *event_tracker)
-{
-    if (event_tracker->uvm_file != NULL) {
-        NV_STATUS status;
-        uvm_va_space_t *va_space = tools_event_tracker_va_space(event_tracker);
+static void destroy_event_tracker(uvm_tools_event_tracker_t *event_tracker) {
+  if (event_tracker->uvm_file != NULL) {
+    NV_STATUS status;
+    uvm_va_space_t *va_space = tools_event_tracker_va_space(event_tracker);
 
-        uvm_down_write(&g_tools_va_space_list_lock);
-        uvm_down_write(&va_space->perf_events.lock);
-        uvm_down_write(&va_space->tools.lock);
+    uvm_down_write(&g_tools_va_space_list_lock);
+    uvm_down_write(&va_space->perf_events.lock);
+    uvm_down_write(&va_space->tools.lock);
 
-        if (event_tracker->is_queue) {
-            uvm_tools_queue_t *queue = &event_tracker->queue;
-            NvU64 buffer_size;
+    if (event_tracker->is_queue) {
+      uvm_tools_queue_t *queue = &event_tracker->queue;
+      NvU64 buffer_size;
 
-            buffer_size = queue->queue_buffer_count * event_tracker->entry_size;
+      buffer_size = queue->queue_buffer_count * event_tracker->entry_size;
 
-            remove_event_tracker(va_space,
-                                 queue->queue_nodes,
-                                 UvmEventNumTypesAll,
-                                 queue->subscribed_queues,
-                                 &queue->subscribed_queues);
+      remove_event_tracker(va_space, queue->queue_nodes, UvmEventNumTypesAll,
+                           queue->subscribed_queues, &queue->subscribed_queues);
 
-            if (queue->queue_buffer != NULL) {
-                unmap_user_pages(queue->queue_buffer_pages,
-                                 queue->queue_buffer,
-                                 buffer_size);
-            }
+      if (queue->queue_buffer != NULL) {
+        unmap_user_pages(queue->queue_buffer_pages, queue->queue_buffer,
+                         buffer_size);
+      }
 
-            if (queue->control != NULL) {
-                unmap_user_pages(queue->control_buffer_pages,
-                                 queue->control,
-                                 sizeof(UvmToolsEventControlData));
-            }
-        }
-        else {
-            uvm_tools_counter_t *counters = &event_tracker->counter;
+      if (queue->control != NULL) {
+        unmap_user_pages(queue->control_buffer_pages, queue->control,
+                         sizeof(UvmToolsEventControlData));
+      }
+    } else {
+      uvm_tools_counter_t *counters = &event_tracker->counter;
 
-            remove_event_tracker(va_space,
-                                 counters->counter_nodes,
-                                 UVM_TOTAL_COUNTERS,
-                                 counters->subscribed_counters,
-                                 &counters->subscribed_counters);
+      remove_event_tracker(va_space, counters->counter_nodes,
+                           UVM_TOTAL_COUNTERS, counters->subscribed_counters,
+                           &counters->subscribed_counters);
 
-            if (counters->counters != NULL) {
-                unmap_user_pages(counters->counter_buffer_pages,
-                                 counters->counters,
-                                 UVM_TOTAL_COUNTERS * sizeof(NvU64));
-            }
-        }
+      if (counters->counters != NULL) {
+        unmap_user_pages(counters->counter_buffer_pages, counters->counters,
+                         UVM_TOTAL_COUNTERS * sizeof(NvU64));
+      }
+    }
 
-        // de-registration should not fail
-        status = tools_update_status(va_space);
-        UVM_ASSERT(status == NV_OK);
+    // de-registration should not fail
+    status = tools_update_status(va_space);
+    UVM_ASSERT(status == NV_OK);
 
-        uvm_up_write(&va_space->tools.lock);
-        uvm_up_write(&va_space->perf_events.lock);
-        uvm_up_write(&g_tools_va_space_list_lock);
+    uvm_up_write(&va_space->tools.lock);
+    uvm_up_write(&va_space->perf_events.lock);
+    uvm_up_write(&g_tools_va_space_list_lock);
 
-        fput(event_tracker->uvm_file);
-    }
+    fput(event_tracker->uvm_file);
+  }
 
-    kmem_cache_free(g_tools_event_tracker_cache, event_tracker);
+  kmem_cache_free(g_tools_event_tracker_cache, event_tracker);
 }
 
-static void enqueue_event(const void *entry, size_t entry_size, NvU8 eventType, uvm_tools_queue_t *queue)
-{
-    UvmToolsEventControlData *ctrl = queue->control;
-    uvm_tools_queue_snapshot_t sn;
-    NvU32 queue_size = queue->queue_buffer_count;
-    NvU32 queue_mask = queue_size - 1;
+static void enqueue_event(const void *entry, size_t entry_size, NvU8 eventType,
+                          uvm_tools_queue_t *queue) {
+  UvmToolsEventControlData *ctrl = queue->control;
+  uvm_tools_queue_snapshot_t sn;
+  NvU32 queue_size = queue->queue_buffer_count;
+  NvU32 queue_mask = queue_size - 1;
 
-    // Prevent processor speculation prior to accessing user-mapped memory to
-    // avoid leaking information from side-channel attacks. There are many
-    // possible paths leading to this point and it would be difficult and error-
-    // prone to audit all of them to determine whether user mode could guide
-    // this access to kernel memory under speculative execution, so to be on the
-    // safe side we'll just always block speculation.
-    nv_speculation_barrier();
+  // Prevent processor speculation prior to accessing user-mapped memory to
+  // avoid leaking information from side-channel attacks. There are many
+  // possible paths leading to this point and it would be difficult and error-
+  // prone to audit all of them to determine whether user mode could guide
+  // this access to kernel memory under speculative execution, so to be on the
+  // safe side we'll just always block speculation.
+  nv_speculation_barrier();
 
-    uvm_spin_lock(&queue->lock);
+  uvm_spin_lock(&queue->lock);
 
-    // ctrl is mapped into user space with read and write permissions,
-    // so its values cannot be trusted.
-    sn.get_behind = atomic_read((atomic_t *)&ctrl->get_behind) & queue_mask;
-    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind) & queue_mask;
-    sn.put_ahead = (sn.put_behind + 1) & queue_mask;
+  // ctrl is mapped into user space with read and write permissions,
+  // so its values cannot be trusted.
+  sn.get_behind = atomic_read((atomic_t *)&ctrl->get_behind) & queue_mask;
+  sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind) & queue_mask;
+  sn.put_ahead = (sn.put_behind + 1) & queue_mask;
 
-    // one free element means that the queue is full
-    if (((queue_size + sn.get_behind - sn.put_behind) & queue_mask) == 1) {
-        atomic64_inc((atomic64_t *)&ctrl->dropped + eventType);
-        goto unlock;
-    }
+  // one free element means that the queue is full
+  if (((queue_size + sn.get_behind - sn.put_behind) & queue_mask) == 1) {
+    atomic64_inc((atomic64_t *)&ctrl->dropped + eventType);
+    goto unlock;
+  }
 
-    memcpy((char *)queue->queue_buffer + sn.put_behind * entry_size, entry, entry_size);
+  memcpy((char *)queue->queue_buffer + sn.put_behind * entry_size, entry,
+         entry_size);
 
-    sn.put_behind = sn.put_ahead;
+  sn.put_behind = sn.put_ahead;
 
-    // put_ahead and put_behind will always be the same outside of queue->lock
-    // this allows the user-space consumer to choose either a 2 or 4 pointer
-    // synchronization approach.
-    atomic_set((atomic_t *)&ctrl->put_ahead, sn.put_behind);
-    atomic_set((atomic_t *)&ctrl->put_behind, sn.put_behind);
+  // put_ahead and put_behind will always be the same outside of queue->lock
+  // this allows the user-space consumer to choose either a 2 or 4 pointer
+  // synchronization approach.
+  atomic_set((atomic_t *)&ctrl->put_ahead, sn.put_behind);
+  atomic_set((atomic_t *)&ctrl->put_behind, sn.put_behind);
 
-    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
+  sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
 
-    // if the queue needs to be woken up, only signal if we haven't signaled
-    // before for this value of get_ahead.
-    if (queue_needs_wakeup(queue, &sn) && !(queue->is_wakeup_get_valid && queue->wakeup_get == sn.get_ahead)) {
-        queue->is_wakeup_get_valid = true;
-        queue->wakeup_get = sn.get_ahead;
-        wake_up_all(&queue->wait_queue);
-    }
+  // if the queue needs to be woken up, only signal if we haven't signaled
+  // before for this value of get_ahead.
+  if (queue_needs_wakeup(queue, &sn) &&
+      !(queue->is_wakeup_get_valid && queue->wakeup_get == sn.get_ahead)) {
+    queue->is_wakeup_get_valid = true;
+    queue->wakeup_get = sn.get_ahead;
+    wake_up_all(&queue->wait_queue);
+  }
 
 unlock:
-    uvm_spin_unlock(&queue->lock);
+  uvm_spin_unlock(&queue->lock);
 }
 
-static void uvm_tools_enqueue_event(struct list_head *head, const void *entry, size_t entry_size, NvU8 eventType)
-{
-    uvm_tools_queue_t *queue;
+static void uvm_tools_enqueue_event(struct list_head *head, const void *entry,
+                                    size_t entry_size, NvU8 eventType) {
+  uvm_tools_queue_t *queue;
 
-    UVM_ASSERT(eventType < UvmEventNumTypesAll);
+  UVM_ASSERT(eventType < UvmEventNumTypesAll);
 
-    list_for_each_entry(queue, head + eventType, queue_nodes[eventType])
-        enqueue_event(entry, entry_size, eventType, queue);
+  list_for_each_entry(queue, head + eventType, queue_nodes[eventType])
+      enqueue_event(entry, entry_size, eventType, queue);
 }
 
-static void uvm_tools_record_event(uvm_va_space_t *va_space, const UvmEventEntry *entry)
-{
-    NvU8 eventType = entry->eventData.eventType;
+static void uvm_tools_record_event(uvm_va_space_t *va_space,
+                                   const UvmEventEntry *entry) {
+  NvU8 eventType = entry->eventData.eventType;
 
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    uvm_tools_enqueue_event(va_space->tools.queues, entry, sizeof(*entry), eventType);
+  uvm_tools_enqueue_event(va_space->tools.queues, entry, sizeof(*entry),
+                          eventType);
 }
 
-static void uvm_tools_record_event_v2(uvm_va_space_t *va_space, const UvmEventEntry_V2 *entry)
-{
-    NvU8 eventType = entry->eventData.eventType;
+static void uvm_tools_record_event_v2(uvm_va_space_t *va_space,
+                                      const UvmEventEntry_V2 *entry) {
+  NvU8 eventType = entry->eventData.eventType;
 
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    uvm_tools_enqueue_event(va_space->tools.queues_v2, entry, sizeof(*entry), eventType);
+  uvm_tools_enqueue_event(va_space->tools.queues_v2, entry, sizeof(*entry),
+                          eventType);
 }
 
-static bool counter_matches_processor(UvmCounterName counter, const NvProcessorUuid *processor)
-{
-    // For compatibility with older counters, CPU faults for memory with a
-    // preferred location are reported for their preferred location as well as
-    // for the CPU device itself.
-    // This check prevents double counting in the aggregate count.
-    if (counter == UvmCounterNameCpuPageFaultCount)
-        return uvm_uuid_eq(processor, &NV_PROCESSOR_UUID_CPU_DEFAULT);
-    return true;
+static bool counter_matches_processor(UvmCounterName counter,
+                                      const NvProcessorUuid *processor) {
+  // For compatibility with older counters, CPU faults for memory with a
+  // preferred location are reported for their preferred location as well as
+  // for the CPU device itself.
+  // This check prevents double counting in the aggregate count.
+  if (counter == UvmCounterNameCpuPageFaultCount)
+    return uvm_uuid_eq(processor, &NV_PROCESSOR_UUID_CPU_DEFAULT);
+  return true;
 }
 
 static void uvm_tools_inc_counter(uvm_va_space_t *va_space,
-                                  UvmCounterName counter,
-                                  NvU64 amount,
-                                  const NvProcessorUuid *processor)
-{
-    UVM_ASSERT((NvU32)counter < UVM_TOTAL_COUNTERS);
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
-
-    if (amount > 0) {
-        uvm_tools_counter_t *counters;
-
-        // Prevent processor speculation prior to accessing user-mapped memory
-        // to avoid leaking information from side-channel attacks. There are
-        // many possible paths leading to this point and it would be difficult
-        // and error-prone to audit all of them to determine whether user mode
-        // could guide this access to kernel memory under speculative execution,
-        // so to be on the safe side we'll just always block speculation.
-        nv_speculation_barrier();
-
-        list_for_each_entry(counters, va_space->tools.counters + counter, counter_nodes[counter]) {
-            if ((counters->all_processors && counter_matches_processor(counter, processor)) ||
-                uvm_uuid_eq(&counters->processor, processor)) {
-                atomic64_add(amount, (atomic64_t *)(counters->counters + counter));
-            }
-        }
+                                  UvmCounterName counter, NvU64 amount,
+                                  const NvProcessorUuid *processor) {
+  UVM_ASSERT((NvU32)counter < UVM_TOTAL_COUNTERS);
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
+
+  if (amount > 0) {
+    uvm_tools_counter_t *counters;
+
+    // Prevent processor speculation prior to accessing user-mapped memory
+    // to avoid leaking information from side-channel attacks. There are
+    // many possible paths leading to this point and it would be difficult
+    // and error-prone to audit all of them to determine whether user mode
+    // could guide this access to kernel memory under speculative execution,
+    // so to be on the safe side we'll just always block speculation.
+    nv_speculation_barrier();
+
+    list_for_each_entry(counters, va_space->tools.counters + counter,
+                        counter_nodes[counter]) {
+      if ((counters->all_processors &&
+           counter_matches_processor(counter, processor)) ||
+          uvm_uuid_eq(&counters->processor, processor)) {
+        atomic64_add(amount, (atomic64_t *)(counters->counters + counter));
+      }
     }
+  }
 }
 
-static bool tools_is_counter_enabled(uvm_va_space_t *va_space, UvmCounterName counter)
-{
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+static bool tools_is_counter_enabled(uvm_va_space_t *va_space,
+                                     UvmCounterName counter) {
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    UVM_ASSERT(counter < UVM_TOTAL_COUNTERS);
+  UVM_ASSERT(counter < UVM_TOTAL_COUNTERS);
 
-    return !list_empty(va_space->tools.counters + counter);
+  return !list_empty(va_space->tools.counters + counter);
 }
 
-static bool tools_is_event_enabled_v1(uvm_va_space_t *va_space, UvmEventType event)
-{
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+static bool tools_is_event_enabled_v1(uvm_va_space_t *va_space,
+                                      UvmEventType event) {
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    UVM_ASSERT(event < UvmEventNumTypesAll);
+  UVM_ASSERT(event < UvmEventNumTypesAll);
 
-    return !list_empty(va_space->tools.queues + event);
+  return !list_empty(va_space->tools.queues + event);
 }
 
-static bool tools_is_event_enabled_v2(uvm_va_space_t *va_space, UvmEventType event)
-{
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+static bool tools_is_event_enabled_v2(uvm_va_space_t *va_space,
+                                      UvmEventType event) {
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    UVM_ASSERT(event < UvmEventNumTypesAll);
+  UVM_ASSERT(event < UvmEventNumTypesAll);
 
-    return !list_empty(va_space->tools.queues_v2 + event);
+  return !list_empty(va_space->tools.queues_v2 + event);
 }
 
-static bool tools_is_event_enabled(uvm_va_space_t *va_space, UvmEventType event)
-{
-    return tools_is_event_enabled_v1(va_space, event) || tools_is_event_enabled_v2(va_space, event);
+static bool tools_is_event_enabled(uvm_va_space_t *va_space,
+                                   UvmEventType event) {
+  return tools_is_event_enabled_v1(va_space, event) ||
+         tools_is_event_enabled_v2(va_space, event);
 }
 
-static bool tools_is_event_enabled_in_any_va_space(UvmEventType event)
-{
-    bool ret = false;
+static bool tools_is_event_enabled_in_any_va_space(UvmEventType event) {
+  bool ret = false;
 
-    uvm_down_read(&g_tools_va_space_list_lock);
-    ret = g_tools_enabled_event_count[event] != 0;
-    uvm_up_read(&g_tools_va_space_list_lock);
+  uvm_down_read(&g_tools_va_space_list_lock);
+  ret = g_tools_enabled_event_count[event] != 0;
+  uvm_up_read(&g_tools_va_space_list_lock);
 
-    return ret;
+  return ret;
 }
 
-static bool tools_are_enabled(uvm_va_space_t *va_space)
-{
-    NvU32 i;
+static bool tools_are_enabled(uvm_va_space_t *va_space) {
+  NvU32 i;
 
-    uvm_assert_rwsem_locked(&va_space->tools.lock);
+  uvm_assert_rwsem_locked(&va_space->tools.lock);
 
-    for (i = 0; i < UVM_TOTAL_COUNTERS; i++) {
-        if (tools_is_counter_enabled(va_space, i))
-            return true;
-    }
-    for (i = 0; i < UvmEventNumTypesAll; i++) {
-        if (tools_is_event_enabled(va_space, i))
-            return true;
-    }
-    return false;
+  for (i = 0; i < UVM_TOTAL_COUNTERS; i++) {
+    if (tools_is_counter_enabled(va_space, i))
+      return true;
+  }
+  for (i = 0; i < UvmEventNumTypesAll; i++) {
+    if (tools_is_event_enabled(va_space, i))
+      return true;
+  }
+  return false;
 }
 
-static bool tools_is_fault_callback_needed(uvm_va_space_t *va_space)
-{
-    return tools_is_event_enabled(va_space, UvmEventTypeCpuFault) ||
-           tools_is_event_enabled(va_space, UvmEventTypeGpuFault) ||
-           tools_is_counter_enabled(va_space, UvmCounterNameCpuPageFaultCount) ||
-           tools_is_counter_enabled(va_space, UvmCounterNameGpuPageFaultCount);
+static bool tools_is_fault_callback_needed(uvm_va_space_t *va_space) {
+  return tools_is_event_enabled(va_space, UvmEventTypeCpuFault) ||
+         tools_is_event_enabled(va_space, UvmEventTypeGpuFault) ||
+         tools_is_counter_enabled(va_space, UvmCounterNameCpuPageFaultCount) ||
+         tools_is_counter_enabled(va_space, UvmCounterNameGpuPageFaultCount);
 }
 
-static bool tools_is_migration_callback_needed(uvm_va_space_t *va_space)
-{
-    return tools_is_event_enabled(va_space, UvmEventTypeMigration) ||
-           tools_is_event_enabled(va_space, UvmEventTypeReadDuplicate) ||
-           tools_is_counter_enabled(va_space, UvmCounterNameBytesXferDtH) ||
-           tools_is_counter_enabled(va_space, UvmCounterNameBytesXferHtD);
+static bool tools_is_migration_callback_needed(uvm_va_space_t *va_space) {
+  return tools_is_event_enabled(va_space, UvmEventTypeMigration) ||
+         tools_is_event_enabled(va_space, UvmEventTypeReadDuplicate) ||
+         tools_is_counter_enabled(va_space, UvmCounterNameBytesXferDtH) ||
+         tools_is_counter_enabled(va_space, UvmCounterNameBytesXferHtD);
 }
 
-static int uvm_tools_open(struct inode *inode, struct file *filp)
-{
-    filp->private_data = NULL;
-    return -nv_status_to_errno(uvm_global_get_status());
+static int uvm_tools_open(struct inode *inode, struct file *filp) {
+  filp->private_data = NULL;
+  return -nv_status_to_errno(uvm_global_get_status());
 }
 
-static int uvm_tools_open_entry(struct inode *inode, struct file *filp)
-{
-    UVM_ENTRY_RET(uvm_tools_open(inode, filp));
+static int uvm_tools_open_entry(struct inode *inode, struct file *filp) {
+  UVM_ENTRY_RET(uvm_tools_open(inode, filp));
 }
 
-static int uvm_tools_release(struct inode *inode, struct file *filp)
-{
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-    if (event_tracker != NULL) {
-        destroy_event_tracker(event_tracker);
-        filp->private_data = NULL;
-    }
-    return -nv_status_to_errno(uvm_global_get_status());
+static int uvm_tools_release(struct inode *inode, struct file *filp) {
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+  if (event_tracker != NULL) {
+    destroy_event_tracker(event_tracker);
+    filp->private_data = NULL;
+  }
+  return -nv_status_to_errno(uvm_global_get_status());
 }
 
-static int uvm_tools_release_entry(struct inode *inode, struct file *filp)
-{
-    UVM_ENTRY_RET(uvm_tools_release(inode, filp));
+static int uvm_tools_release_entry(struct inode *inode, struct file *filp) {
+  UVM_ENTRY_RET(uvm_tools_release(inode, filp));
 }
 
-static long uvm_tools_unlocked_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-    switch (cmd) {
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_INIT_EVENT_TRACKER,         uvm_api_tools_init_event_tracker);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD, uvm_api_tools_set_notification_threshold);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS,  uvm_api_tools_event_queue_enable_events);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS, uvm_api_tools_event_queue_disable_events);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_ENABLE_COUNTERS,            uvm_api_tools_enable_counters);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_DISABLE_COUNTERS,           uvm_api_tools_disable_counters);
-        UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_INIT_EVENT_TRACKER_V2,      uvm_api_tools_init_event_tracker_v2);
-    }
+static long uvm_tools_unlocked_ioctl(struct file *filp, unsigned int cmd,
+                                     unsigned long arg) {
+  switch (cmd) {
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_INIT_EVENT_TRACKER,
+                                      uvm_api_tools_init_event_tracker);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD,
+                                      uvm_api_tools_set_notification_threshold);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS,
+                                      uvm_api_tools_event_queue_enable_events);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS,
+                                      uvm_api_tools_event_queue_disable_events);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_ENABLE_COUNTERS,
+                                      uvm_api_tools_enable_counters);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_DISABLE_COUNTERS,
+                                      uvm_api_tools_disable_counters);
+    UVM_ROUTE_CMD_STACK_NO_INIT_CHECK(UVM_TOOLS_INIT_EVENT_TRACKER_V2,
+                                      uvm_api_tools_init_event_tracker_v2);
+  }
 
-    uvm_thread_assert_all_unlocked();
+  uvm_thread_assert_all_unlocked();
 
-    return -EINVAL;
+  return -EINVAL;
 }
 
-static long uvm_tools_unlocked_ioctl_entry(struct file *filp, unsigned int cmd, unsigned long arg)
-{
-    UVM_ENTRY_RET(uvm_tools_unlocked_ioctl(filp, cmd, arg));
+static long uvm_tools_unlocked_ioctl_entry(struct file *filp, unsigned int cmd,
+                                           unsigned long arg) {
+  UVM_ENTRY_RET(uvm_tools_unlocked_ioctl(filp, cmd, arg));
 }
 
-static unsigned uvm_tools_poll(struct file *filp, poll_table *wait)
-{
-    int flags = 0;
-    uvm_tools_queue_snapshot_t sn;
-    uvm_tools_event_tracker_t *event_tracker;
-    UvmToolsEventControlData *ctrl;
+static unsigned uvm_tools_poll(struct file *filp, poll_table *wait) {
+  int flags = 0;
+  uvm_tools_queue_snapshot_t sn;
+  uvm_tools_event_tracker_t *event_tracker;
+  UvmToolsEventControlData *ctrl;
 
-    if (uvm_global_get_status() != NV_OK)
-        return POLLERR;
+  if (uvm_global_get_status() != NV_OK)
+    return POLLERR;
 
-    event_tracker = tools_event_tracker(filp);
-    if (!tracker_is_queue(event_tracker))
-        return POLLERR;
+  event_tracker = tools_event_tracker(filp);
+  if (!tracker_is_queue(event_tracker))
+    return POLLERR;
 
-    uvm_spin_lock(&event_tracker->queue.lock);
+  uvm_spin_lock(&event_tracker->queue.lock);
 
-    event_tracker->queue.is_wakeup_get_valid = false;
-    ctrl = event_tracker->queue.control;
-    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
-    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
+  event_tracker->queue.is_wakeup_get_valid = false;
+  ctrl = event_tracker->queue.control;
+  sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
+  sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
 
-    if (queue_needs_wakeup(&event_tracker->queue, &sn))
-        flags = POLLIN | POLLRDNORM;
+  if (queue_needs_wakeup(&event_tracker->queue, &sn))
+    flags = POLLIN | POLLRDNORM;
 
-    uvm_spin_unlock(&event_tracker->queue.lock);
+  uvm_spin_unlock(&event_tracker->queue.lock);
 
-    poll_wait(filp, &event_tracker->queue.wait_queue, wait);
-    return flags;
+  poll_wait(filp, &event_tracker->queue.wait_queue, wait);
+  return flags;
 }
 
-static unsigned uvm_tools_poll_entry(struct file *filp, poll_table *wait)
-{
-    UVM_ENTRY_RET(uvm_tools_poll(filp, wait));
+static unsigned uvm_tools_poll_entry(struct file *filp, poll_table *wait) {
+  UVM_ENTRY_RET(uvm_tools_poll(filp, wait));
 }
 
-static UvmEventFaultType g_hal_to_tools_fault_type_table[UVM_FAULT_TYPE_COUNT] = {
-    [UVM_FAULT_TYPE_INVALID_PDE]          = UvmFaultTypeInvalidPde,
-    [UVM_FAULT_TYPE_INVALID_PTE]          = UvmFaultTypeInvalidPte,
-    [UVM_FAULT_TYPE_ATOMIC]               = UvmFaultTypeAtomic,
-    [UVM_FAULT_TYPE_WRITE]                = UvmFaultTypeWrite,
-    [UVM_FAULT_TYPE_PDE_SIZE]             = UvmFaultTypeInvalidPdeSize,
-    [UVM_FAULT_TYPE_VA_LIMIT_VIOLATION]   = UvmFaultTypeLimitViolation,
-    [UVM_FAULT_TYPE_UNBOUND_INST_BLOCK]   = UvmFaultTypeUnboundInstBlock,
-    [UVM_FAULT_TYPE_PRIV_VIOLATION]       = UvmFaultTypePrivViolation,
-    [UVM_FAULT_TYPE_PITCH_MASK_VIOLATION] = UvmFaultTypePitchMaskViolation,
-    [UVM_FAULT_TYPE_WORK_CREATION]        = UvmFaultTypeWorkCreation,
-    [UVM_FAULT_TYPE_UNSUPPORTED_APERTURE] = UvmFaultTypeUnsupportedAperture,
-    [UVM_FAULT_TYPE_COMPRESSION_FAILURE]  = UvmFaultTypeCompressionFailure,
-    [UVM_FAULT_TYPE_UNSUPPORTED_KIND]     = UvmFaultTypeUnsupportedKind,
-    [UVM_FAULT_TYPE_REGION_VIOLATION]     = UvmFaultTypeRegionViolation,
-    [UVM_FAULT_TYPE_POISONED]             = UvmFaultTypePoison,
-    [UVM_FAULT_TYPE_CC_VIOLATION]         = UvmFaultTypeCcViolation,
+static UvmEventFaultType g_hal_to_tools_fault_type_table[UVM_FAULT_TYPE_COUNT] =
+    {
+        [UVM_FAULT_TYPE_INVALID_PDE] = UvmFaultTypeInvalidPde,
+        [UVM_FAULT_TYPE_INVALID_PTE] = UvmFaultTypeInvalidPte,
+        [UVM_FAULT_TYPE_ATOMIC] = UvmFaultTypeAtomic,
+        [UVM_FAULT_TYPE_WRITE] = UvmFaultTypeWrite,
+        [UVM_FAULT_TYPE_PDE_SIZE] = UvmFaultTypeInvalidPdeSize,
+        [UVM_FAULT_TYPE_VA_LIMIT_VIOLATION] = UvmFaultTypeLimitViolation,
+        [UVM_FAULT_TYPE_UNBOUND_INST_BLOCK] = UvmFaultTypeUnboundInstBlock,
+        [UVM_FAULT_TYPE_PRIV_VIOLATION] = UvmFaultTypePrivViolation,
+        [UVM_FAULT_TYPE_PITCH_MASK_VIOLATION] = UvmFaultTypePitchMaskViolation,
+        [UVM_FAULT_TYPE_WORK_CREATION] = UvmFaultTypeWorkCreation,
+        [UVM_FAULT_TYPE_UNSUPPORTED_APERTURE] = UvmFaultTypeUnsupportedAperture,
+        [UVM_FAULT_TYPE_COMPRESSION_FAILURE] = UvmFaultTypeCompressionFailure,
+        [UVM_FAULT_TYPE_UNSUPPORTED_KIND] = UvmFaultTypeUnsupportedKind,
+        [UVM_FAULT_TYPE_REGION_VIOLATION] = UvmFaultTypeRegionViolation,
+        [UVM_FAULT_TYPE_POISONED] = UvmFaultTypePoison,
+        [UVM_FAULT_TYPE_CC_VIOLATION] = UvmFaultTypeCcViolation,
 };
 
 // TODO: add new value for weak atomics in tools
-static UvmEventMemoryAccessType g_hal_to_tools_fault_access_type_table[UVM_FAULT_ACCESS_TYPE_COUNT] = {
-    [UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG] = UvmEventMemoryAccessTypeAtomic,
-    [UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK]   = UvmEventMemoryAccessTypeAtomic,
-    [UVM_FAULT_ACCESS_TYPE_WRITE]         = UvmEventMemoryAccessTypeWrite,
-    [UVM_FAULT_ACCESS_TYPE_READ]          = UvmEventMemoryAccessTypeRead,
-    [UVM_FAULT_ACCESS_TYPE_PREFETCH]      = UvmEventMemoryAccessTypePrefetch
-};
+static UvmEventMemoryAccessType
+    g_hal_to_tools_fault_access_type_table[UVM_FAULT_ACCESS_TYPE_COUNT] = {
+        [UVM_FAULT_ACCESS_TYPE_ATOMIC_STRONG] = UvmEventMemoryAccessTypeAtomic,
+        [UVM_FAULT_ACCESS_TYPE_ATOMIC_WEAK] = UvmEventMemoryAccessTypeAtomic,
+        [UVM_FAULT_ACCESS_TYPE_WRITE] = UvmEventMemoryAccessTypeWrite,
+        [UVM_FAULT_ACCESS_TYPE_READ] = UvmEventMemoryAccessTypeRead,
+        [UVM_FAULT_ACCESS_TYPE_PREFETCH] = UvmEventMemoryAccessTypePrefetch};
 
 static UvmEventApertureType g_hal_to_tools_aperture_table[UVM_APERTURE_MAX] = {
     [UVM_APERTURE_PEER_0] = UvmEventAperturePeer0,
@@ -761,427 +740,432 @@ static UvmEventApertureType g_hal_to_tools_aperture_table[UVM_APERTURE_MAX] = {
     [UVM_APERTURE_PEER_5] = UvmEventAperturePeer5,
     [UVM_APERTURE_PEER_6] = UvmEventAperturePeer6,
     [UVM_APERTURE_PEER_7] = UvmEventAperturePeer7,
-    [UVM_APERTURE_SYS]    = UvmEventApertureSys,
-    [UVM_APERTURE_VID]    = UvmEventApertureVid,
+    [UVM_APERTURE_SYS] = UvmEventApertureSys,
+    [UVM_APERTURE_VID] = UvmEventApertureVid,
 };
 
-static UvmEventFaultClientType g_hal_to_tools_fault_client_type_table[UVM_FAULT_CLIENT_TYPE_COUNT] = {
-    [UVM_FAULT_CLIENT_TYPE_GPC] = UvmEventFaultClientTypeGpc,
-    [UVM_FAULT_CLIENT_TYPE_HUB] = UvmEventFaultClientTypeHub,
+static UvmEventFaultClientType
+    g_hal_to_tools_fault_client_type_table[UVM_FAULT_CLIENT_TYPE_COUNT] = {
+        [UVM_FAULT_CLIENT_TYPE_GPC] = UvmEventFaultClientTypeGpc,
+        [UVM_FAULT_CLIENT_TYPE_HUB] = UvmEventFaultClientTypeHub,
 };
 
-static void record_gpu_fault_instance(uvm_gpu_t *gpu,
-                                      uvm_va_space_t *va_space,
-                                      const uvm_fault_buffer_entry_t *fault_entry,
-                                      NvU64 batch_id,
-                                      NvU64 timestamp)
-{
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeGpuFault)) {
-        UvmEventEntry entry;
-        UvmEventGpuFaultInfo *info = &entry.eventData.gpuFault;
-        memset(&entry, 0, sizeof(entry));
-
-        info->eventType     = UvmEventTypeGpuFault;
-        info->gpuIndex      = uvm_parent_id_value_from_processor_id(gpu->id);
-        info->faultType     = g_hal_to_tools_fault_type_table[fault_entry->fault_type];
-        info->accessType    = g_hal_to_tools_fault_access_type_table[fault_entry->fault_access_type];
-        info->clientType    = g_hal_to_tools_fault_client_type_table[fault_entry->fault_source.client_type];
-        if (fault_entry->is_replayable)
-            info->gpcId     = fault_entry->fault_source.gpc_id;
-        else
-            info->channelId = fault_entry->fault_source.channel_id;
-        info->clientId      = fault_entry->fault_source.client_id;
-        info->address       = fault_entry->fault_address;
-        info->timeStamp     = timestamp;
-        info->timeStampGpu  = fault_entry->timestamp;
-        info->batchId       = batch_id;
-
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeGpuFault)) {
-        UvmEventEntry_V2 entry;
-        UvmEventGpuFaultInfo_V2 *info = &entry.eventData.gpuFault;
-        memset(&entry, 0, sizeof(entry));
-
-        info->eventType     = UvmEventTypeGpuFault;
-        info->gpuIndex      = uvm_id_value(gpu->id);
-        info->faultType     = g_hal_to_tools_fault_type_table[fault_entry->fault_type];
-        info->accessType    = g_hal_to_tools_fault_access_type_table[fault_entry->fault_access_type];
-        info->clientType    = g_hal_to_tools_fault_client_type_table[fault_entry->fault_source.client_type];
-        if (fault_entry->is_replayable)
-            info->gpcId     = fault_entry->fault_source.gpc_id;
-        else
-            info->channelId = fault_entry->fault_source.channel_id;
-        info->clientId      = fault_entry->fault_source.client_id;
-        info->address       = fault_entry->fault_address;
-        info->timeStamp     = timestamp;
-        info->timeStampGpu  = fault_entry->timestamp;
-        info->batchId       = batch_id;
-
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
+static void
+record_gpu_fault_instance(uvm_gpu_t *gpu, uvm_va_space_t *va_space,
+                          const uvm_fault_buffer_entry_t *fault_entry,
+                          NvU64 batch_id, NvU64 timestamp) {
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeGpuFault)) {
+    UvmEventEntry entry;
+    UvmEventGpuFaultInfo *info = &entry.eventData.gpuFault;
+    memset(&entry, 0, sizeof(entry));
+
+    info->eventType = UvmEventTypeGpuFault;
+    info->gpuIndex = uvm_parent_id_value_from_processor_id(gpu->id);
+    info->faultType = g_hal_to_tools_fault_type_table[fault_entry->fault_type];
+    info->accessType =
+        g_hal_to_tools_fault_access_type_table[fault_entry->fault_access_type];
+    info->clientType =
+        g_hal_to_tools_fault_client_type_table[fault_entry->fault_source
+                                                   .client_type];
+    if (fault_entry->is_replayable)
+      info->gpcId = fault_entry->fault_source.gpc_id;
+    else
+      info->channelId = fault_entry->fault_source.channel_id;
+    info->clientId = fault_entry->fault_source.client_id;
+    info->address = fault_entry->fault_address;
+    info->timeStamp = timestamp;
+    info->timeStampGpu = fault_entry->timestamp;
+    info->batchId = batch_id;
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeGpuFault)) {
+    UvmEventEntry_V2 entry;
+    UvmEventGpuFaultInfo_V2 *info = &entry.eventData.gpuFault;
+    memset(&entry, 0, sizeof(entry));
+
+    info->eventType = UvmEventTypeGpuFault;
+    info->gpuIndex = uvm_id_value(gpu->id);
+    info->faultType = g_hal_to_tools_fault_type_table[fault_entry->fault_type];
+    info->accessType =
+        g_hal_to_tools_fault_access_type_table[fault_entry->fault_access_type];
+    info->clientType =
+        g_hal_to_tools_fault_client_type_table[fault_entry->fault_source
+                                                   .client_type];
+    if (fault_entry->is_replayable)
+      info->gpcId = fault_entry->fault_source.gpc_id;
+    else
+      info->channelId = fault_entry->fault_source.channel_id;
+    info->clientId = fault_entry->fault_source.client_id;
+    info->address = fault_entry->fault_address;
+    info->timeStamp = timestamp;
+    info->timeStampGpu = fault_entry->timestamp;
+    info->batchId = batch_id;
+
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
 }
 
-static void record_cpu_fault(UvmEventCpuFaultInfo *info, uvm_perf_event_data_t *event_data)
-{
-    info->eventType = UvmEventTypeCpuFault;
-    if (event_data->fault.cpu.is_write)
-        info->accessType = UvmEventMemoryAccessTypeWrite;
-    else
-        info->accessType = UvmEventMemoryAccessTypeRead;
+static void record_cpu_fault(UvmEventCpuFaultInfo *info,
+                             uvm_perf_event_data_t *event_data) {
+  info->eventType = UvmEventTypeCpuFault;
+  if (event_data->fault.cpu.is_write)
+    info->accessType = UvmEventMemoryAccessTypeWrite;
+  else
+    info->accessType = UvmEventMemoryAccessTypeRead;
 
-    info->address = event_data->fault.cpu.fault_va;
-    info->timeStamp = NV_GETTIME();
-    // assume that current owns va_space
-    info->pid = uvm_get_stale_process_id();
-    info->threadId = uvm_get_stale_thread_id();
-    info->pc = event_data->fault.cpu.pc;
-    info->cpuId = event_data->fault.cpu.cpu_num;
+  info->address = event_data->fault.cpu.fault_va;
+  info->timeStamp = NV_GETTIME();
+  // assume that current owns va_space
+  info->pid = uvm_get_stale_process_id();
+  info->threadId = uvm_get_stale_thread_id();
+  info->pc = event_data->fault.cpu.pc;
+  info->cpuId = event_data->fault.cpu.cpu_num;
 }
 
 static void uvm_tools_record_fault(uvm_va_space_t *va_space,
                                    uvm_perf_event_t event_id,
-                                   uvm_perf_event_data_t *event_data)
-{
-    UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
-    UVM_ASSERT(va_space);
+                                   uvm_perf_event_data_t *event_data) {
+  UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
+  UVM_ASSERT(va_space);
 
-    uvm_assert_rwsem_locked(&va_space->lock);
-    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-    UVM_ASSERT(va_space->tools.enabled);
-
-    uvm_down_read(&va_space->tools.lock);
-    UVM_ASSERT(tools_is_fault_callback_needed(va_space));
-
-    if (UVM_ID_IS_CPU(event_data->fault.proc_id)) {
-        if (tools_is_event_enabled_v1(va_space, UvmEventTypeCpuFault)) {
-            UvmEventEntry entry;
-            memset(&entry, 0, sizeof(entry));
-
-            record_cpu_fault(&entry.eventData.cpuFault, event_data);
-
-            uvm_tools_record_event(va_space, &entry);
-        }
-        if (tools_is_event_enabled_v2(va_space, UvmEventTypeCpuFault)) {
-            UvmEventEntry_V2 entry;
-            memset(&entry, 0, sizeof(entry));
-
-            record_cpu_fault(&entry.eventData.cpuFault, event_data);
-
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
-        if (tools_is_counter_enabled(va_space, UvmCounterNameCpuPageFaultCount)) {
-            uvm_processor_id_t preferred_location;
-
-            // The UVM Lite tools interface did not represent the CPU as a UVM
-            // device. It reported CPU faults against the corresponding
-            // allocation's 'home location'. Though this driver's tools
-            // interface does include a CPU device, for compatibility, the
-            // driver still reports faults against a buffer's preferred
-            // location, in addition to the CPU.
-            uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1, &NV_PROCESSOR_UUID_CPU_DEFAULT);
-
-            preferred_location = event_data->fault.preferred_location;
-            if (UVM_ID_IS_GPU(preferred_location)) {
-                uvm_gpu_t *gpu = uvm_gpu_get(preferred_location);
-                uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1, &gpu->uuid);
-            }
-        }
+  uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+  UVM_ASSERT(va_space->tools.enabled);
+
+  uvm_down_read(&va_space->tools.lock);
+  UVM_ASSERT(tools_is_fault_callback_needed(va_space));
+
+  if (UVM_ID_IS_CPU(event_data->fault.proc_id)) {
+    if (tools_is_event_enabled_v1(va_space, UvmEventTypeCpuFault)) {
+      UvmEventEntry entry;
+      memset(&entry, 0, sizeof(entry));
+
+      record_cpu_fault(&entry.eventData.cpuFault, event_data);
+
+      uvm_tools_record_event(va_space, &entry);
     }
-    else {
-        uvm_gpu_t *gpu = uvm_gpu_get(event_data->fault.proc_id);
-        UVM_ASSERT(gpu);
+    if (tools_is_event_enabled_v2(va_space, UvmEventTypeCpuFault)) {
+      UvmEventEntry_V2 entry;
+      memset(&entry, 0, sizeof(entry));
 
-        if (tools_is_event_enabled(va_space, UvmEventTypeGpuFault)) {
-            NvU64 timestamp = NV_GETTIME();
-            uvm_fault_buffer_entry_t *fault_entry = event_data->fault.gpu.buffer_entry;
-            uvm_fault_buffer_entry_t *fault_instance;
+      record_cpu_fault(&entry.eventData.cpuFault, event_data);
 
-            record_gpu_fault_instance(gpu, va_space, fault_entry, event_data->fault.gpu.batch_id, timestamp);
+      uvm_tools_record_event_v2(va_space, &entry);
+    }
+    if (tools_is_counter_enabled(va_space, UvmCounterNameCpuPageFaultCount)) {
+      uvm_processor_id_t preferred_location;
 
-            list_for_each_entry(fault_instance, &fault_entry->merged_instances_list, merged_instances_list)
-                record_gpu_fault_instance(gpu, va_space, fault_instance, event_data->fault.gpu.batch_id, timestamp);
-        }
+      // The UVM Lite tools interface did not represent the CPU as a UVM
+      // device. It reported CPU faults against the corresponding
+      // allocation's 'home location'. Though this driver's tools
+      // interface does include a CPU device, for compatibility, the
+      // driver still reports faults against a buffer's preferred
+      // location, in addition to the CPU.
+      uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1,
+                            &NV_PROCESSOR_UUID_CPU_DEFAULT);
 
-        if (tools_is_counter_enabled(va_space, UvmCounterNameGpuPageFaultCount))
-            uvm_tools_inc_counter(va_space, UvmCounterNameGpuPageFaultCount, 1, &gpu->uuid);
+      preferred_location = event_data->fault.preferred_location;
+      if (UVM_ID_IS_GPU(preferred_location)) {
+        uvm_gpu_t *gpu = uvm_gpu_get(preferred_location);
+        uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1,
+                              &gpu->uuid);
+      }
     }
-    uvm_up_read(&va_space->tools.lock);
-}
-
-static void add_pending_event_for_channel(uvm_channel_t *channel)
-{
-    uvm_assert_spinlock_locked(&g_tools_channel_list_lock);
-
-    if (channel->tools.pending_event_count++ == 0)
-        list_add_tail(&channel->tools.channel_list_node, &g_tools_channel_list);
-}
-
-static void remove_pending_event_for_channel(uvm_channel_t *channel)
-{
-    uvm_assert_spinlock_locked(&g_tools_channel_list_lock);
-    UVM_ASSERT(channel->tools.pending_event_count > 0);
-    if (--channel->tools.pending_event_count == 0)
-        list_del_init(&channel->tools.channel_list_node);
-}
-
-static void record_migration_events(void *args)
-{
-    block_migration_data_t *block_mig = (block_migration_data_t *)args;
-    migration_data_t *mig;
-    migration_data_t *next;
-    uvm_va_space_t *va_space = block_mig->va_space;
-    NvU64 gpu_timestamp = block_mig->start_timestamp_gpu;
-
-    uvm_down_read(&va_space->tools.lock);
-    list_for_each_entry_safe(mig, next, &block_mig->events, events_node) {
-        UVM_ASSERT(mig->bytes > 0);
-        list_del(&mig->events_node);
-
-        if (tools_is_event_enabled_v1(va_space, UvmEventTypeMigration)) {
-            UvmEventEntry entry;
-            UvmEventMigrationInfo *info = &entry.eventData.migration;
-
-            // Initialize fields that are constant throughout the whole block
-            memset(&entry, 0, sizeof(entry));
-            info->eventType         = UvmEventTypeMigration;
-            info->srcIndex          = uvm_parent_id_value_from_processor_id(block_mig->src);
-            info->dstIndex          = uvm_parent_id_value_from_processor_id(block_mig->dst);
-            info->beginTimeStamp    = block_mig->start_timestamp_cpu;
-            info->endTimeStamp      = block_mig->end_timestamp_cpu;
-            info->rangeGroupId      = block_mig->range_group_id;
-            info->address           = mig->address;
-            info->migratedBytes     = mig->bytes;
-            info->beginTimeStampGpu = gpu_timestamp;
-            info->endTimeStampGpu   = mig->end_timestamp_gpu;
-            info->migrationCause    = mig->cause;
-
-            uvm_tools_record_event(va_space, &entry);
-        }
-
-        if (tools_is_event_enabled_v2(va_space, UvmEventTypeMigration)) {
-            UvmEventEntry_V2 entry;
-            UvmEventMigrationInfo_V2 *info = &entry.eventData.migration;
-
-            // Initialize fields that are constant throughout the whole block
-            memset(&entry, 0, sizeof(entry));
-            info->eventType         = UvmEventTypeMigration;
-            info->srcIndex          = uvm_id_value(block_mig->src);
-            info->srcNid            = block_mig->src_nid;
-            info->dstIndex          = uvm_id_value(block_mig->dst);
-            info->dstNid            = block_mig->dst_nid;
-            info->beginTimeStamp    = block_mig->start_timestamp_cpu;
-            info->endTimeStamp      = block_mig->end_timestamp_cpu;
-            info->rangeGroupId      = block_mig->range_group_id;
-            info->address           = mig->address;
-            info->migratedBytes     = mig->bytes;
-            info->beginTimeStampGpu = gpu_timestamp;
-            info->endTimeStampGpu   = mig->end_timestamp_gpu;
-            info->migrationCause    = mig->cause;
-
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
-
-        gpu_timestamp = mig->end_timestamp_gpu;
-        kmem_cache_free(g_tools_migration_data_cache, mig);
+  } else {
+    uvm_gpu_t *gpu = uvm_gpu_get(event_data->fault.proc_id);
+    UVM_ASSERT(gpu);
+
+    if (tools_is_event_enabled(va_space, UvmEventTypeGpuFault)) {
+      NvU64 timestamp = NV_GETTIME();
+      uvm_fault_buffer_entry_t *fault_entry =
+          event_data->fault.gpu.buffer_entry;
+      uvm_fault_buffer_entry_t *fault_instance;
+
+      record_gpu_fault_instance(gpu, va_space, fault_entry,
+                                event_data->fault.gpu.batch_id, timestamp);
+
+      list_for_each_entry(fault_instance, &fault_entry->merged_instances_list,
+                          merged_instances_list)
+          record_gpu_fault_instance(gpu, va_space, fault_instance,
+                                    event_data->fault.gpu.batch_id, timestamp);
     }
-    uvm_up_read(&va_space->tools.lock);
 
-    UVM_ASSERT(list_empty(&block_mig->events));
-    kmem_cache_free(g_tools_block_migration_data_cache, block_mig);
+    if (tools_is_counter_enabled(va_space, UvmCounterNameGpuPageFaultCount))
+      uvm_tools_inc_counter(va_space, UvmCounterNameGpuPageFaultCount, 1,
+                            &gpu->uuid);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-static void record_migration_events_entry(void *args)
-{
-    UVM_ENTRY_VOID(record_migration_events(args));
+static void add_pending_event_for_channel(uvm_channel_t *channel) {
+  uvm_assert_spinlock_locked(&g_tools_channel_list_lock);
+
+  if (channel->tools.pending_event_count++ == 0)
+    list_add_tail(&channel->tools.channel_list_node, &g_tools_channel_list);
 }
 
-static void on_block_migration_complete(void *ptr)
-{
-    migration_data_t *mig;
-    block_migration_data_t *block_mig = (block_migration_data_t *)ptr;
+static void remove_pending_event_for_channel(uvm_channel_t *channel) {
+  uvm_assert_spinlock_locked(&g_tools_channel_list_lock);
+  UVM_ASSERT(channel->tools.pending_event_count > 0);
+  if (--channel->tools.pending_event_count == 0)
+    list_del_init(&channel->tools.channel_list_node);
+}
 
-    block_mig->end_timestamp_cpu = NV_GETTIME();
-    block_mig->start_timestamp_gpu = *block_mig->start_timestamp_gpu_addr;
-    list_for_each_entry(mig, &block_mig->events, events_node)
-        mig->end_timestamp_gpu = *mig->end_timestamp_gpu_addr;
+static void record_migration_events(void *args) {
+  block_migration_data_t *block_mig = (block_migration_data_t *)args;
+  migration_data_t *mig;
+  migration_data_t *next;
+  uvm_va_space_t *va_space = block_mig->va_space;
+  NvU64 gpu_timestamp = block_mig->start_timestamp_gpu;
 
-    nv_kthread_q_item_init(&block_mig->queue_item, record_migration_events_entry, block_mig);
+  uvm_down_read(&va_space->tools.lock);
+  list_for_each_entry_safe(mig, next, &block_mig->events, events_node) {
+    UVM_ASSERT(mig->bytes > 0);
+    list_del(&mig->events_node);
 
-    // The UVM driver may notice that work in a channel is complete in a variety of situations
-    // and the va_space lock is not always held in all of them, nor can it always be taken safely on them.
-    // Dispatching events requires the va_space lock to be held in at least read mode, so
-    // this callback simply enqueues the dispatching onto a queue, where the
-    // va_space lock is always safe to acquire.
-    uvm_spin_lock(&g_tools_channel_list_lock);
-    remove_pending_event_for_channel(block_mig->channel);
-    nv_kthread_q_schedule_q_item(&g_tools_queue, &block_mig->queue_item);
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+    if (tools_is_event_enabled_v1(va_space, UvmEventTypeMigration)) {
+      UvmEventEntry entry;
+      UvmEventMigrationInfo *info = &entry.eventData.migration;
+
+      // Initialize fields that are constant throughout the whole block
+      memset(&entry, 0, sizeof(entry));
+      info->eventType = UvmEventTypeMigration;
+      info->srcIndex = uvm_parent_id_value_from_processor_id(block_mig->src);
+      info->dstIndex = uvm_parent_id_value_from_processor_id(block_mig->dst);
+      info->beginTimeStamp = block_mig->start_timestamp_cpu;
+      info->endTimeStamp = block_mig->end_timestamp_cpu;
+      info->rangeGroupId = block_mig->range_group_id;
+      info->address = mig->address;
+      info->migratedBytes = mig->bytes;
+      info->beginTimeStampGpu = gpu_timestamp;
+      info->endTimeStampGpu = mig->end_timestamp_gpu;
+      info->migrationCause = mig->cause;
+
+      uvm_tools_record_event(va_space, &entry);
+    }
+
+    if (tools_is_event_enabled_v2(va_space, UvmEventTypeMigration)) {
+      UvmEventEntry_V2 entry;
+      UvmEventMigrationInfo_V2 *info = &entry.eventData.migration;
+
+      // Initialize fields that are constant throughout the whole block
+      memset(&entry, 0, sizeof(entry));
+      info->eventType = UvmEventTypeMigration;
+      info->srcIndex = uvm_id_value(block_mig->src);
+      info->srcNid = block_mig->src_nid;
+      info->dstIndex = uvm_id_value(block_mig->dst);
+      info->dstNid = block_mig->dst_nid;
+      info->beginTimeStamp = block_mig->start_timestamp_cpu;
+      info->endTimeStamp = block_mig->end_timestamp_cpu;
+      info->rangeGroupId = block_mig->range_group_id;
+      info->address = mig->address;
+      info->migratedBytes = mig->bytes;
+      info->beginTimeStampGpu = gpu_timestamp;
+      info->endTimeStampGpu = mig->end_timestamp_gpu;
+      info->migrationCause = mig->cause;
+
+      uvm_tools_record_event_v2(va_space, &entry);
+    }
+
+    gpu_timestamp = mig->end_timestamp_gpu;
+    kmem_cache_free(g_tools_migration_data_cache, mig);
+  }
+  uvm_up_read(&va_space->tools.lock);
+
+  UVM_ASSERT(list_empty(&block_mig->events));
+  kmem_cache_free(g_tools_block_migration_data_cache, block_mig);
+}
+
+static void record_migration_events_entry(void *args) {
+  UVM_ENTRY_VOID(record_migration_events(args));
+}
+
+static void on_block_migration_complete(void *ptr) {
+  migration_data_t *mig;
+  block_migration_data_t *block_mig = (block_migration_data_t *)ptr;
+
+  block_mig->end_timestamp_cpu = NV_GETTIME();
+  block_mig->start_timestamp_gpu = *block_mig->start_timestamp_gpu_addr;
+  list_for_each_entry(mig, &block_mig->events, events_node)
+      mig->end_timestamp_gpu = *mig->end_timestamp_gpu_addr;
+
+  nv_kthread_q_item_init(&block_mig->queue_item, record_migration_events_entry,
+                         block_mig);
+
+  // The UVM driver may notice that work in a channel is complete in a variety
+  // of situations and the va_space lock is not always held in all of them, nor
+  // can it always be taken safely on them. Dispatching events requires the
+  // va_space lock to be held in at least read mode, so this callback simply
+  // enqueues the dispatching onto a queue, where the va_space lock is always
+  // safe to acquire.
+  uvm_spin_lock(&g_tools_channel_list_lock);
+  remove_pending_event_for_channel(block_mig->channel);
+  nv_kthread_q_schedule_q_item(&g_tools_queue, &block_mig->queue_item);
+  uvm_spin_unlock(&g_tools_channel_list_lock);
 }
 
 static void record_replay_event_helper(uvm_va_space_t *va_space,
-                                       uvm_gpu_id_t gpu_id,
-                                       NvU32 batch_id,
+                                       uvm_gpu_id_t gpu_id, NvU32 batch_id,
                                        uvm_fault_client_type_t client_type,
-                                       NvU64 timestamp,
-                                       NvU64 timestamp_gpu)
-{
-    uvm_down_read(&va_space->tools.lock);
-
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeGpuFaultReplay)) {
-        UvmEventEntry entry;
-
-        memset(&entry, 0, sizeof(entry));
-        entry.eventData.gpuFaultReplay.eventType    = UvmEventTypeGpuFaultReplay;
-        entry.eventData.gpuFaultReplay.gpuIndex     = uvm_parent_id_value_from_processor_id(gpu_id);
-        entry.eventData.gpuFaultReplay.batchId      = batch_id;
-        entry.eventData.gpuFaultReplay.clientType   = g_hal_to_tools_fault_client_type_table[client_type];
-        entry.eventData.gpuFaultReplay.timeStamp    = timestamp;
-        entry.eventData.gpuFaultReplay.timeStampGpu = timestamp_gpu;
-
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeGpuFaultReplay)) {
-        UvmEventEntry_V2 entry;
-
-        memset(&entry, 0, sizeof(entry));
-        entry.eventData.gpuFaultReplay.eventType    = UvmEventTypeGpuFaultReplay;
-        entry.eventData.gpuFaultReplay.gpuIndex     = uvm_id_value(gpu_id);
-        entry.eventData.gpuFaultReplay.batchId      = batch_id;
-        entry.eventData.gpuFaultReplay.clientType   = g_hal_to_tools_fault_client_type_table[client_type];
-        entry.eventData.gpuFaultReplay.timeStamp    = timestamp;
-        entry.eventData.gpuFaultReplay.timeStampGpu = timestamp_gpu;
-
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
+                                       NvU64 timestamp, NvU64 timestamp_gpu) {
+  uvm_down_read(&va_space->tools.lock);
+
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeGpuFaultReplay)) {
+    UvmEventEntry entry;
+
+    memset(&entry, 0, sizeof(entry));
+    entry.eventData.gpuFaultReplay.eventType = UvmEventTypeGpuFaultReplay;
+    entry.eventData.gpuFaultReplay.gpuIndex =
+        uvm_parent_id_value_from_processor_id(gpu_id);
+    entry.eventData.gpuFaultReplay.batchId = batch_id;
+    entry.eventData.gpuFaultReplay.clientType =
+        g_hal_to_tools_fault_client_type_table[client_type];
+    entry.eventData.gpuFaultReplay.timeStamp = timestamp;
+    entry.eventData.gpuFaultReplay.timeStampGpu = timestamp_gpu;
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeGpuFaultReplay)) {
+    UvmEventEntry_V2 entry;
+
+    memset(&entry, 0, sizeof(entry));
+    entry.eventData.gpuFaultReplay.eventType = UvmEventTypeGpuFaultReplay;
+    entry.eventData.gpuFaultReplay.gpuIndex = uvm_id_value(gpu_id);
+    entry.eventData.gpuFaultReplay.batchId = batch_id;
+    entry.eventData.gpuFaultReplay.clientType =
+        g_hal_to_tools_fault_client_type_table[client_type];
+    entry.eventData.gpuFaultReplay.timeStamp = timestamp;
+    entry.eventData.gpuFaultReplay.timeStampGpu = timestamp_gpu;
+
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
 
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 }
 
-static void record_replay_event_broadcast(uvm_gpu_id_t gpu_id,
-                                          NvU32 batch_id,
+static void record_replay_event_broadcast(uvm_gpu_id_t gpu_id, NvU32 batch_id,
                                           uvm_fault_client_type_t client_type,
                                           NvU64 timestamp,
-                                          NvU64 timestamp_gpu)
-{
-    uvm_va_space_t *va_space;
-
-    uvm_down_read(&g_tools_va_space_list_lock);
-
-    list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
-        record_replay_event_helper(va_space,
-                                   gpu_id,
-                                   batch_id,
-                                   client_type,
-                                   timestamp,
-                                   timestamp_gpu);
-    }
+                                          NvU64 timestamp_gpu) {
+  uvm_va_space_t *va_space;
+
+  uvm_down_read(&g_tools_va_space_list_lock);
+
+  list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
+    record_replay_event_helper(va_space, gpu_id, batch_id, client_type,
+                               timestamp, timestamp_gpu);
+  }
 
-    uvm_up_read(&g_tools_va_space_list_lock);
+  uvm_up_read(&g_tools_va_space_list_lock);
 }
 
-static void record_replay_events(void *args)
-{
-    replay_data_t *replay = (replay_data_t *)args;
+static void record_replay_events(void *args) {
+  replay_data_t *replay = (replay_data_t *)args;
 
-    record_replay_event_broadcast(replay->gpu_id,
-                                  replay->batch_id,
-                                  replay->client_type,
-                                  replay->timestamp,
-                                  replay->timestamp_gpu);
+  record_replay_event_broadcast(replay->gpu_id, replay->batch_id,
+                                replay->client_type, replay->timestamp,
+                                replay->timestamp_gpu);
 
-    kmem_cache_free(g_tools_replay_data_cache, replay);
+  kmem_cache_free(g_tools_replay_data_cache, replay);
 }
 
-static void record_replay_events_entry(void *args)
-{
-    UVM_ENTRY_VOID(record_replay_events(args));
+static void record_replay_events_entry(void *args) {
+  UVM_ENTRY_VOID(record_replay_events(args));
 }
 
-static void on_replay_complete(void *ptr)
-{
-    replay_data_t *replay = (replay_data_t *)ptr;
-    replay->timestamp_gpu = *replay->timestamp_gpu_addr;
+static void on_replay_complete(void *ptr) {
+  replay_data_t *replay = (replay_data_t *)ptr;
+  replay->timestamp_gpu = *replay->timestamp_gpu_addr;
 
-    nv_kthread_q_item_init(&replay->queue_item, record_replay_events_entry, ptr);
-
-    uvm_spin_lock(&g_tools_channel_list_lock);
-    remove_pending_event_for_channel(replay->channel);
-    nv_kthread_q_schedule_q_item(&g_tools_queue, &replay->queue_item);
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+  nv_kthread_q_item_init(&replay->queue_item, record_replay_events_entry, ptr);
 
+  uvm_spin_lock(&g_tools_channel_list_lock);
+  remove_pending_event_for_channel(replay->channel);
+  nv_kthread_q_schedule_q_item(&g_tools_queue, &replay->queue_item);
+  uvm_spin_unlock(&g_tools_channel_list_lock);
 }
 
-static UvmEventMigrationCause g_make_resident_to_tools_migration_cause[UVM_MAKE_RESIDENT_CAUSE_MAX] = {
-    [UVM_MAKE_RESIDENT_CAUSE_REPLAYABLE_FAULT]     = UvmEventMigrationCauseCoherence,
-    [UVM_MAKE_RESIDENT_CAUSE_NON_REPLAYABLE_FAULT] = UvmEventMigrationCauseCoherence,
-    [UVM_MAKE_RESIDENT_CAUSE_ACCESS_COUNTER]       = UvmEventMigrationCauseAccessCounters,
-    [UVM_MAKE_RESIDENT_CAUSE_PREFETCH]             = UvmEventMigrationCausePrefetch,
-    [UVM_MAKE_RESIDENT_CAUSE_EVICTION]             = UvmEventMigrationCauseEviction,
-    [UVM_MAKE_RESIDENT_CAUSE_API_TOOLS]            = UvmEventMigrationCauseInvalid,
-    [UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE]          = UvmEventMigrationCauseUser,
-    [UVM_MAKE_RESIDENT_CAUSE_API_SET_RANGE_GROUP]  = UvmEventMigrationCauseCoherence,
-    [UVM_MAKE_RESIDENT_CAUSE_API_HINT]             = UvmEventMigrationCauseUser,
+static UvmEventMigrationCause
+    g_make_resident_to_tools_migration_cause[UVM_MAKE_RESIDENT_CAUSE_MAX] = {
+        [UVM_MAKE_RESIDENT_CAUSE_REPLAYABLE_FAULT] =
+            UvmEventMigrationCauseCoherence,
+        [UVM_MAKE_RESIDENT_CAUSE_NON_REPLAYABLE_FAULT] =
+            UvmEventMigrationCauseCoherence,
+        [UVM_MAKE_RESIDENT_CAUSE_ACCESS_COUNTER] =
+            UvmEventMigrationCauseAccessCounters,
+        [UVM_MAKE_RESIDENT_CAUSE_PREFETCH] = UvmEventMigrationCausePrefetch,
+        [UVM_MAKE_RESIDENT_CAUSE_EVICTION] = UvmEventMigrationCauseEviction,
+        [UVM_MAKE_RESIDENT_CAUSE_API_TOOLS] = UvmEventMigrationCauseInvalid,
+        [UVM_MAKE_RESIDENT_CAUSE_API_MIGRATE] = UvmEventMigrationCauseUser,
+        [UVM_MAKE_RESIDENT_CAUSE_API_SET_RANGE_GROUP] =
+            UvmEventMigrationCauseCoherence,
+        [UVM_MAKE_RESIDENT_CAUSE_API_HINT] = UvmEventMigrationCauseUser,
 };
 
-static void uvm_tools_record_migration_cpu_to_cpu(uvm_va_space_t *va_space, uvm_perf_event_data_t *event_data)
-{
-    UVM_ASSERT(va_space);
-
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeMigration)) {
-        UvmEventEntry entry;
-        UvmEventMigrationInfo *info = &entry.eventData.migration;
-
-        // CPU-to-CPU migration events can be added directly to the queue.
-        memset(&entry, 0, sizeof(entry));
-        info->eventType = UvmEventTypeMigration;
-        info->srcIndex = uvm_parent_id_value_from_processor_id(event_data->migration.src);
-        info->dstIndex = uvm_parent_id_value_from_processor_id(event_data->migration.dst);
-        info->address = event_data->migration.address;
-        info->migratedBytes = event_data->migration.bytes;
-        info->beginTimeStamp = event_data->migration.cpu_start_timestamp;
-        info->endTimeStamp = NV_GETTIME();
-        info->migrationCause = event_data->migration.cause;
-        info->rangeGroupId = UVM_RANGE_GROUP_ID_NONE;
-
-        // During evictions, it is not safe to uvm_range_group_range_find()
-        // because the va_space lock is not held.
-        if (event_data->migration.cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION) {
-            uvm_range_group_range_t *range = uvm_range_group_range_find(va_space, event_data->migration.address);
-            if (range != NULL)
-                info->rangeGroupId = range->range_group->id;
-        }
-
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeMigration)) {
-        UvmEventEntry_V2 entry;
-        UvmEventMigrationInfo_V2 *info = &entry.eventData.migration;
-
-        // CPU-to-CPU migration events can be added directly to the queue.
-        memset(&entry, 0, sizeof(entry));
-        info->eventType = UvmEventTypeMigration;
-        info->srcIndex = uvm_id_value(event_data->migration.src);
-        info->dstIndex = uvm_id_value(event_data->migration.dst);
-        info->srcNid = event_data->migration.src_nid;
-        info->dstNid = event_data->migration.dst_nid;
-        info->address = event_data->migration.address;
-        info->migratedBytes = event_data->migration.bytes;
-        info->beginTimeStamp = event_data->migration.cpu_start_timestamp;
-        info->endTimeStamp = NV_GETTIME();
-        info->migrationCause = event_data->migration.cause;
-        info->rangeGroupId = UVM_RANGE_GROUP_ID_NONE;
-
-        // During evictions, it is not safe to uvm_range_group_range_find()
-        // because the va_space lock is not held.
-        if (event_data->migration.cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION) {
-            uvm_range_group_range_t *range = uvm_range_group_range_find(va_space, event_data->migration.address);
-            if (range != NULL)
-                info->rangeGroupId = range->range_group->id;
-        }
-
-        uvm_tools_record_event_v2(va_space, &entry);
+static void
+uvm_tools_record_migration_cpu_to_cpu(uvm_va_space_t *va_space,
+                                      uvm_perf_event_data_t *event_data) {
+  UVM_ASSERT(va_space);
+
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeMigration)) {
+    UvmEventEntry entry;
+    UvmEventMigrationInfo *info = &entry.eventData.migration;
+
+    // CPU-to-CPU migration events can be added directly to the queue.
+    memset(&entry, 0, sizeof(entry));
+    info->eventType = UvmEventTypeMigration;
+    info->srcIndex =
+        uvm_parent_id_value_from_processor_id(event_data->migration.src);
+    info->dstIndex =
+        uvm_parent_id_value_from_processor_id(event_data->migration.dst);
+    info->address = event_data->migration.address;
+    info->migratedBytes = event_data->migration.bytes;
+    info->beginTimeStamp = event_data->migration.cpu_start_timestamp;
+    info->endTimeStamp = NV_GETTIME();
+    info->migrationCause = event_data->migration.cause;
+    info->rangeGroupId = UVM_RANGE_GROUP_ID_NONE;
+
+    // During evictions, it is not safe to uvm_range_group_range_find()
+    // because the va_space lock is not held.
+    if (event_data->migration.cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION) {
+      uvm_range_group_range_t *range =
+          uvm_range_group_range_find(va_space, event_data->migration.address);
+      if (range != NULL)
+        info->rangeGroupId = range->range_group->id;
+    }
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeMigration)) {
+    UvmEventEntry_V2 entry;
+    UvmEventMigrationInfo_V2 *info = &entry.eventData.migration;
+
+    // CPU-to-CPU migration events can be added directly to the queue.
+    memset(&entry, 0, sizeof(entry));
+    info->eventType = UvmEventTypeMigration;
+    info->srcIndex = uvm_id_value(event_data->migration.src);
+    info->dstIndex = uvm_id_value(event_data->migration.dst);
+    info->srcNid = event_data->migration.src_nid;
+    info->dstNid = event_data->migration.dst_nid;
+    info->address = event_data->migration.address;
+    info->migratedBytes = event_data->migration.bytes;
+    info->beginTimeStamp = event_data->migration.cpu_start_timestamp;
+    info->endTimeStamp = NV_GETTIME();
+    info->migrationCause = event_data->migration.cause;
+    info->rangeGroupId = UVM_RANGE_GROUP_ID_NONE;
+
+    // During evictions, it is not safe to uvm_range_group_range_find()
+    // because the va_space lock is not held.
+    if (event_data->migration.cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION) {
+      uvm_range_group_range_t *range =
+          uvm_range_group_range_find(va_space, event_data->migration.address);
+      if (range != NULL)
+        info->rangeGroupId = range->range_group->id;
     }
+
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
 }
 
 // For non-CPU-to-CPU migrations (or CPU-to-CPU copies using CEs), this event is
@@ -1191,1673 +1175,1789 @@ static void uvm_tools_record_migration_cpu_to_cpu(uvm_va_space_t *va_space, uvm_
 // page copies does by block_copy_resident_pages have finished.
 static void uvm_tools_record_migration(uvm_va_space_t *va_space,
                                        uvm_perf_event_t event_id,
-                                       uvm_perf_event_data_t *event_data)
-{
-    uvm_va_block_t *va_block = event_data->migration.block;
-
-    UVM_ASSERT(va_space);
-    UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
-
-    if (va_block)
-        uvm_assert_mutex_locked(&va_block->lock);
-
-    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-    UVM_ASSERT(va_space->tools.enabled);
-
-    uvm_down_read(&va_space->tools.lock);
-    UVM_ASSERT(tools_is_migration_callback_needed(va_space));
-
-    if (tools_is_event_enabled(va_space, UvmEventTypeMigration)) {
-        if (!UVM_ID_IS_CPU(event_data->migration.src) || !UVM_ID_IS_CPU(event_data->migration.dst)) {
-            migration_data_t *mig;
-            uvm_push_info_t *push_info = uvm_push_info_from_push(event_data->migration.push);
-            block_migration_data_t *block_mig = (block_migration_data_t *)push_info->on_complete_data;
-
-            if (push_info->on_complete != NULL) {
-                mig = kmem_cache_alloc(g_tools_migration_data_cache, NV_UVM_GFP_FLAGS);
-                if (mig == NULL)
-                    goto done_unlock;
-
-                mig->address = event_data->migration.address;
-                mig->bytes = event_data->migration.bytes;
-                mig->end_timestamp_gpu_addr = uvm_push_timestamp(event_data->migration.push);
-                mig->cause = g_make_resident_to_tools_migration_cause[event_data->migration.cause];
-
-                list_add_tail(&mig->events_node, &block_mig->events);
-            }
-        }
-        else {
-            uvm_tools_record_migration_cpu_to_cpu(va_space, event_data);
-        }
-    }
-
-    // We don't want to increment neither UvmCounterNameBytesXferDtH nor
-    // UvmCounterNameBytesXferHtD in a CPU-to-CPU migration.
-    if (UVM_ID_IS_CPU(event_data->migration.src) && UVM_ID_IS_CPU(event_data->migration.dst))
-        goto done_unlock;
-
-    // Increment counters
-    if (UVM_ID_IS_CPU(event_data->migration.src) &&
-        tools_is_counter_enabled(va_space, UvmCounterNameBytesXferHtD)) {
-        uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.dst);
-        uvm_tools_inc_counter(va_space,
-                              UvmCounterNameBytesXferHtD,
-                              event_data->migration.bytes,
-                              &gpu->uuid);
-    }
-    if (UVM_ID_IS_CPU(event_data->migration.dst) &&
-        tools_is_counter_enabled(va_space, UvmCounterNameBytesXferDtH)) {
-        uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.src);
-        uvm_tools_inc_counter(va_space,
-                              UvmCounterNameBytesXferDtH,
-                              event_data->migration.bytes,
-                              &gpu->uuid);
-    }
+                                       uvm_perf_event_data_t *event_data) {
+  uvm_va_block_t *va_block = event_data->migration.block;
+
+  UVM_ASSERT(va_space);
+  UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
+
+  if (va_block)
+    uvm_assert_mutex_locked(&va_block->lock);
+
+  uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+  UVM_ASSERT(va_space->tools.enabled);
+
+  uvm_down_read(&va_space->tools.lock);
+  UVM_ASSERT(tools_is_migration_callback_needed(va_space));
+
+  if (tools_is_event_enabled(va_space, UvmEventTypeMigration)) {
+    if (!UVM_ID_IS_CPU(event_data->migration.src) ||
+        !UVM_ID_IS_CPU(event_data->migration.dst)) {
+      migration_data_t *mig;
+      uvm_push_info_t *push_info =
+          uvm_push_info_from_push(event_data->migration.push);
+      block_migration_data_t *block_mig =
+          (block_migration_data_t *)push_info->on_complete_data;
+
+      if (push_info->on_complete != NULL) {
+        mig = kmem_cache_alloc(g_tools_migration_data_cache, NV_UVM_GFP_FLAGS);
+        if (mig == NULL)
+          goto done_unlock;
+
+        mig->address = event_data->migration.address;
+        mig->bytes = event_data->migration.bytes;
+        mig->end_timestamp_gpu_addr =
+            uvm_push_timestamp(event_data->migration.push);
+        mig->cause =
+            g_make_resident_to_tools_migration_cause[event_data->migration
+                                                         .cause];
+
+        list_add_tail(&mig->events_node, &block_mig->events);
+      }
+    } else {
+      uvm_tools_record_migration_cpu_to_cpu(va_space, event_data);
+    }
+  }
+
+  // We don't want to increment neither UvmCounterNameBytesXferDtH nor
+  // UvmCounterNameBytesXferHtD in a CPU-to-CPU migration.
+  if (UVM_ID_IS_CPU(event_data->migration.src) &&
+      UVM_ID_IS_CPU(event_data->migration.dst))
+    goto done_unlock;
+
+  // Increment counters
+  if (UVM_ID_IS_CPU(event_data->migration.src) &&
+      tools_is_counter_enabled(va_space, UvmCounterNameBytesXferHtD)) {
+    uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.dst);
+    uvm_tools_inc_counter(va_space, UvmCounterNameBytesXferHtD,
+                          event_data->migration.bytes, &gpu->uuid);
+  }
+  if (UVM_ID_IS_CPU(event_data->migration.dst) &&
+      tools_is_counter_enabled(va_space, UvmCounterNameBytesXferDtH)) {
+    uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.src);
+    uvm_tools_inc_counter(va_space, UvmCounterNameBytesXferDtH,
+                          event_data->migration.bytes, &gpu->uuid);
+  }
 
 done_unlock:
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 }
 
 // This event is notified asynchronously when it is marked as completed in the
 // pushbuffer the replay method belongs to.
-void uvm_tools_broadcast_replay(uvm_gpu_t *gpu,
-                                uvm_push_t *push,
+void uvm_tools_broadcast_replay(uvm_gpu_t *gpu, uvm_push_t *push,
                                 NvU32 batch_id,
-                                uvm_fault_client_type_t client_type)
-{
-    uvm_push_info_t *push_info = uvm_push_info_from_push(push);
-    replay_data_t *replay;
-
-    // Perform delayed notification only if some VA space has signed up for
-    // UvmEventTypeGpuFaultReplay
-    if (!tools_is_event_enabled_in_any_va_space(UvmEventTypeGpuFaultReplay))
-        return;
-
-    replay = kmem_cache_alloc(g_tools_replay_data_cache, NV_UVM_GFP_FLAGS);
-    if (replay == NULL)
-        return;
-
-    UVM_ASSERT(push_info->on_complete == NULL && push_info->on_complete_data == NULL);
-
-    replay->timestamp_gpu_addr = uvm_push_timestamp(push);
-    replay->gpu_id             = gpu->id;
-    replay->batch_id           = batch_id;
-    replay->client_type        = client_type;
-    replay->timestamp          = NV_GETTIME();
-    replay->channel            = push->channel;
+                                uvm_fault_client_type_t client_type) {
+  uvm_push_info_t *push_info = uvm_push_info_from_push(push);
+  replay_data_t *replay;
+
+  // Perform delayed notification only if some VA space has signed up for
+  // UvmEventTypeGpuFaultReplay
+  if (!tools_is_event_enabled_in_any_va_space(UvmEventTypeGpuFaultReplay))
+    return;
+
+  replay = kmem_cache_alloc(g_tools_replay_data_cache, NV_UVM_GFP_FLAGS);
+  if (replay == NULL)
+    return;
+
+  UVM_ASSERT(push_info->on_complete == NULL &&
+             push_info->on_complete_data == NULL);
+
+  replay->timestamp_gpu_addr = uvm_push_timestamp(push);
+  replay->gpu_id = gpu->id;
+  replay->batch_id = batch_id;
+  replay->client_type = client_type;
+  replay->timestamp = NV_GETTIME();
+  replay->channel = push->channel;
+
+  push_info->on_complete_data = replay;
+  push_info->on_complete = on_replay_complete;
+
+  uvm_spin_lock(&g_tools_channel_list_lock);
+  add_pending_event_for_channel(replay->channel);
+  uvm_spin_unlock(&g_tools_channel_list_lock);
+}
+
+void uvm_tools_broadcast_replay_sync(uvm_gpu_t *gpu, NvU32 batch_id,
+                                     uvm_fault_client_type_t client_type) {
+  UVM_ASSERT(!gpu->parent->has_clear_faulted_channel_method);
+
+  if (!tools_is_event_enabled_in_any_va_space(UvmEventTypeGpuFaultReplay))
+    return;
+
+  record_replay_event_broadcast(gpu->id, batch_id, client_type, NV_GETTIME(),
+                                gpu->parent->host_hal->get_time(gpu));
+}
+
+void uvm_tools_record_access_counter(
+    uvm_va_space_t *va_space, uvm_gpu_id_t gpu_id,
+    const uvm_access_counter_buffer_entry_t *buffer_entry) {
+  uvm_down_read(&va_space->tools.lock);
+
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeTestAccessCounter)) {
+    UvmEventEntry entry;
+    UvmEventTestAccessCounterInfo *info = &entry.testEventData.accessCounter;
+
+    memset(&entry, 0, sizeof(entry));
+
+    info->eventType = UvmEventTypeTestAccessCounter;
+    info->srcIndex = uvm_parent_id_value_from_processor_id(gpu_id);
+    info->address = buffer_entry->address;
+    info->instancePtr = buffer_entry->instance_ptr.address;
+    info->instancePtrAperture =
+        g_hal_to_tools_aperture_table[buffer_entry->instance_ptr.aperture];
+    info->veId = buffer_entry->ve_id;
+    info->value = buffer_entry->counter_value;
+    info->subGranularity = buffer_entry->sub_granularity;
+    info->bank = buffer_entry->bank;
+    info->tag = buffer_entry->tag;
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeTestAccessCounter)) {
+    UvmEventEntry_V2 entry;
+    UvmEventTestAccessCounterInfo_V2 *info = &entry.testEventData.accessCounter;
+
+    memset(&entry, 0, sizeof(entry));
+
+    info->eventType = UvmEventTypeTestAccessCounter;
+    info->srcIndex = uvm_id_value(gpu_id);
+    info->address = buffer_entry->address;
+    info->instancePtr = buffer_entry->instance_ptr.address;
+    info->instancePtrAperture =
+        g_hal_to_tools_aperture_table[buffer_entry->instance_ptr.aperture];
+    info->veId = buffer_entry->ve_id;
+    info->value = buffer_entry->counter_value;
+    info->subGranularity = buffer_entry->sub_granularity;
+    info->bank = buffer_entry->bank;
+    info->tag = buffer_entry->tag;
 
-    push_info->on_complete_data = replay;
-    push_info->on_complete = on_replay_complete;
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
 
-    uvm_spin_lock(&g_tools_channel_list_lock);
-    add_pending_event_for_channel(replay->channel);
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+  uvm_up_read(&va_space->tools.lock);
 }
 
-void uvm_tools_broadcast_replay_sync(uvm_gpu_t *gpu, NvU32 batch_id, uvm_fault_client_type_t client_type)
-{
-    UVM_ASSERT(!gpu->parent->has_clear_faulted_channel_method);
-
-    if (!tools_is_event_enabled_in_any_va_space(UvmEventTypeGpuFaultReplay))
-        return;
+void uvm_tools_broadcast_access_counter(
+    uvm_gpu_t *gpu, const uvm_access_counter_buffer_entry_t *buffer_entry) {
+  uvm_va_space_t *va_space;
 
-    record_replay_event_broadcast(gpu->id,
-                                  batch_id,
-                                  client_type,
-                                  NV_GETTIME(),
-                                  gpu->parent->host_hal->get_time(gpu));
+  uvm_down_read(&g_tools_va_space_list_lock);
+  list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
+    uvm_tools_record_access_counter(va_space, gpu->id, buffer_entry);
+  }
+  uvm_up_read(&g_tools_va_space_list_lock);
 }
 
-void uvm_tools_record_access_counter(uvm_va_space_t *va_space,
-                                     uvm_gpu_id_t gpu_id,
-                                     const uvm_access_counter_buffer_entry_t *buffer_entry)
-{
-    uvm_down_read(&va_space->tools.lock);
+void uvm_tools_test_hmm_split_invalidate(uvm_va_space_t *va_space) {
+  UvmEventEntry_V2 entry;
 
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeTestAccessCounter)) {
-        UvmEventEntry entry;
-        UvmEventTestAccessCounterInfo *info = &entry.testEventData.accessCounter;
+  if (!va_space->tools.enabled)
+    return;
 
-        memset(&entry, 0, sizeof(entry));
+  entry.testEventData.splitInvalidate.eventType =
+      UvmEventTypeTestHmmSplitInvalidate;
+  uvm_down_read(&va_space->tools.lock);
+  uvm_tools_record_event_v2(va_space, &entry);
+  uvm_up_read(&va_space->tools.lock);
+}
 
-        info->eventType           = UvmEventTypeTestAccessCounter;
-        info->srcIndex            = uvm_parent_id_value_from_processor_id(gpu_id);
-        info->address             = buffer_entry->address;
-        info->instancePtr         = buffer_entry->instance_ptr.address;
-        info->instancePtrAperture = g_hal_to_tools_aperture_table[buffer_entry->instance_ptr.aperture];
-        info->veId                = buffer_entry->ve_id;
-        info->value               = buffer_entry->counter_value;
-        info->subGranularity      = buffer_entry->sub_granularity;
-        info->bank                = buffer_entry->bank;
-        info->tag                 = buffer_entry->tag;
+void uvm_tools_record_migration_begin(
+    uvm_va_space_t *va_space, uvm_push_t *push, uvm_processor_id_t dst_id,
+    int dst_nid, uvm_processor_id_t src_id, int src_nid, NvU64 start,
+    uvm_make_resident_cause_t cause, uvm_api_range_type_t type) {
+  uvm_range_group_range_t *range;
 
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeTestAccessCounter)) {
-        UvmEventEntry_V2 entry;
-        UvmEventTestAccessCounterInfo_V2 *info = &entry.testEventData.accessCounter;
-
-        memset(&entry, 0, sizeof(entry));
-
-        info->eventType           = UvmEventTypeTestAccessCounter;
-        info->srcIndex            = uvm_id_value(gpu_id);
-        info->address             = buffer_entry->address;
-        info->instancePtr         = buffer_entry->instance_ptr.address;
-        info->instancePtrAperture = g_hal_to_tools_aperture_table[buffer_entry->instance_ptr.aperture];
-        info->veId                = buffer_entry->ve_id;
-        info->value               = buffer_entry->counter_value;
-        info->subGranularity      = buffer_entry->sub_granularity;
-        info->bank                = buffer_entry->bank;
-        info->tag                 = buffer_entry->tag;
-
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
+  // Calls from tools read/write functions to make_resident must not trigger
+  // any migration
+  UVM_ASSERT(cause != UVM_MAKE_RESIDENT_CAUSE_API_TOOLS);
 
-    uvm_up_read(&va_space->tools.lock);
-}
+  // During evictions the va_space lock is not held.
+  if (cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION)
+    uvm_assert_rwsem_locked(&va_space->lock);
 
-void uvm_tools_broadcast_access_counter(uvm_gpu_t *gpu, const uvm_access_counter_buffer_entry_t *buffer_entry)
-{
-    uvm_va_space_t *va_space;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&g_tools_va_space_list_lock);
-    list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
-        uvm_tools_record_access_counter(va_space, gpu->id, buffer_entry);
-    }
-    uvm_up_read(&g_tools_va_space_list_lock);
-}
+  uvm_down_read(&va_space->tools.lock);
 
-void uvm_tools_test_hmm_split_invalidate(uvm_va_space_t *va_space)
-{
-    UvmEventEntry_V2 entry;
+  // Perform delayed notification only if the VA space has signed up for
+  // UvmEventTypeMigration
+  if (tools_is_event_enabled(va_space, UvmEventTypeMigration)) {
+    block_migration_data_t *block_mig;
+    uvm_push_info_t *push_info = uvm_push_info_from_push(push);
 
-    if (!va_space->tools.enabled)
-        return;
+    UVM_ASSERT(push_info->on_complete == NULL &&
+               push_info->on_complete_data == NULL);
+
+    block_mig =
+        kmem_cache_alloc(g_tools_block_migration_data_cache, NV_UVM_GFP_FLAGS);
+    if (block_mig == NULL)
+      goto done_unlock;
+
+    block_mig->start_timestamp_gpu_addr = uvm_push_timestamp(push);
+    block_mig->channel = push->channel;
+    block_mig->start_timestamp_cpu = NV_GETTIME();
+    block_mig->dst = dst_id;
+    block_mig->dst_nid = dst_nid;
+    block_mig->src = src_id;
+    block_mig->src_nid = src_nid;
+    block_mig->range_group_id = UVM_RANGE_GROUP_ID_NONE;
+
+    // During evictions, it is not safe to uvm_range_group_range_find() because
+    // the va_space lock is not held.
+    if ((type == UVM_API_RANGE_TYPE_MANAGED) &&
+        (cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION)) {
+      range = uvm_range_group_range_find(va_space, start);
+      if (range != NULL)
+        block_mig->range_group_id = range->range_group->id;
+    }
+    block_mig->va_space = va_space;
+
+    INIT_LIST_HEAD(&block_mig->events);
+    push_info->on_complete_data = block_mig;
+    push_info->on_complete = on_block_migration_complete;
 
-    entry.testEventData.splitInvalidate.eventType = UvmEventTypeTestHmmSplitInvalidate;
-    uvm_down_read(&va_space->tools.lock);
-    uvm_tools_record_event_v2(va_space, &entry);
-    uvm_up_read(&va_space->tools.lock);
-}
-
-void uvm_tools_record_migration_begin(uvm_va_space_t *va_space,
-                                      uvm_push_t *push,
-                                      uvm_processor_id_t dst_id,
-                                      int dst_nid,
-                                      uvm_processor_id_t src_id,
-                                      int src_nid,
-                                      NvU64 start,
-                                      uvm_make_resident_cause_t cause,
-                                      uvm_api_range_type_t type)
-{
-    uvm_range_group_range_t *range;
-
-    // Calls from tools read/write functions to make_resident must not trigger
-    // any migration
-    UVM_ASSERT(cause != UVM_MAKE_RESIDENT_CAUSE_API_TOOLS);
-
-    // During evictions the va_space lock is not held.
-    if (cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION)
-        uvm_assert_rwsem_locked(&va_space->lock);
-
-    if (!va_space->tools.enabled)
-        return;
-
-    uvm_down_read(&va_space->tools.lock);
-
-    // Perform delayed notification only if the VA space has signed up for
-    // UvmEventTypeMigration
-    if (tools_is_event_enabled(va_space, UvmEventTypeMigration)) {
-        block_migration_data_t *block_mig;
-        uvm_push_info_t *push_info = uvm_push_info_from_push(push);
-
-        UVM_ASSERT(push_info->on_complete == NULL && push_info->on_complete_data == NULL);
-
-        block_mig = kmem_cache_alloc(g_tools_block_migration_data_cache, NV_UVM_GFP_FLAGS);
-        if (block_mig == NULL)
-            goto done_unlock;
-
-        block_mig->start_timestamp_gpu_addr = uvm_push_timestamp(push);
-        block_mig->channel = push->channel;
-        block_mig->start_timestamp_cpu = NV_GETTIME();
-        block_mig->dst = dst_id;
-        block_mig->dst_nid = dst_nid;
-        block_mig->src = src_id;
-        block_mig->src_nid = src_nid;
-        block_mig->range_group_id = UVM_RANGE_GROUP_ID_NONE;
-
-        // During evictions, it is not safe to uvm_range_group_range_find() because the va_space lock is not held.
-        if ((type == UVM_API_RANGE_TYPE_MANAGED) && (cause != UVM_MAKE_RESIDENT_CAUSE_EVICTION)) {
-            range = uvm_range_group_range_find(va_space, start);
-            if (range != NULL)
-                block_mig->range_group_id = range->range_group->id;
-        }
-        block_mig->va_space = va_space;
-
-        INIT_LIST_HEAD(&block_mig->events);
-        push_info->on_complete_data = block_mig;
-        push_info->on_complete = on_block_migration_complete;
-
-        uvm_spin_lock(&g_tools_channel_list_lock);
-        add_pending_event_for_channel(block_mig->channel);
-        uvm_spin_unlock(&g_tools_channel_list_lock);
-    }
+    uvm_spin_lock(&g_tools_channel_list_lock);
+    add_pending_event_for_channel(block_mig->channel);
+    uvm_spin_unlock(&g_tools_channel_list_lock);
+  }
 
 done_unlock:
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 }
 
 void uvm_tools_record_read_duplicate(uvm_va_block_t *va_block,
                                      uvm_processor_id_t dst,
                                      uvm_va_block_region_t region,
-                                     const uvm_page_mask_t *page_mask)
-{
-    uvm_processor_mask_t *resident_processors;
-    uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
+                                     const uvm_page_mask_t *page_mask) {
+  uvm_processor_mask_t *resident_processors;
+  uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    resident_processors = uvm_processor_mask_cache_alloc();
-    if (!resident_processors)
-        return;
+  resident_processors = uvm_processor_mask_cache_alloc();
+  if (!resident_processors)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
+  uvm_down_read(&va_space->tools.lock);
 
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeReadDuplicate)) {
-        UvmEventEntry entry;
-        UvmEventReadDuplicateInfo *info_read_duplicate = &entry.eventData.readDuplicate;
-        uvm_page_index_t page_index;
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeReadDuplicate)) {
+    UvmEventEntry entry;
+    UvmEventReadDuplicateInfo *info_read_duplicate =
+        &entry.eventData.readDuplicate;
+    uvm_page_index_t page_index;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info_read_duplicate->eventType = UvmEventTypeReadDuplicate;
-        info_read_duplicate->size      = PAGE_SIZE;
-        info_read_duplicate->timeStamp = NV_GETTIME();
+    info_read_duplicate->eventType = UvmEventTypeReadDuplicate;
+    info_read_duplicate->size = PAGE_SIZE;
+    info_read_duplicate->timeStamp = NV_GETTIME();
 
-        for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
-            uvm_processor_id_t id;
+    for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
+      uvm_processor_id_t id;
 
-            info_read_duplicate->address = uvm_va_block_cpu_page_address(va_block, page_index);
-            info_read_duplicate->processors = 0;
+      info_read_duplicate->address =
+          uvm_va_block_cpu_page_address(va_block, page_index);
+      info_read_duplicate->processors = 0;
 
-            uvm_va_block_page_resident_processors(va_block, page_index, resident_processors);
+      uvm_va_block_page_resident_processors(va_block, page_index,
+                                            resident_processors);
 
-            for_each_id_in_mask(id, resident_processors)
-                __set_bit(uvm_parent_id_value_from_processor_id(id), (unsigned long *)&info_read_duplicate->processors);
+      for_each_id_in_mask(id, resident_processors)
+          __set_bit(uvm_parent_id_value_from_processor_id(id),
+                    (unsigned long *)&info_read_duplicate->processors);
 
-            uvm_tools_record_event(va_space, &entry);
-        }
+      uvm_tools_record_event(va_space, &entry);
     }
+  }
 
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeReadDuplicate)) {
-        UvmEventEntry_V2 entry;
-        UvmEventReadDuplicateInfo_V2 *info_read_duplicate = &entry.eventData.readDuplicate;
-        uvm_page_index_t page_index;
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeReadDuplicate)) {
+    UvmEventEntry_V2 entry;
+    UvmEventReadDuplicateInfo_V2 *info_read_duplicate =
+        &entry.eventData.readDuplicate;
+    uvm_page_index_t page_index;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info_read_duplicate->eventType = UvmEventTypeReadDuplicate;
-        info_read_duplicate->size      = PAGE_SIZE;
-        info_read_duplicate->timeStamp = NV_GETTIME();
+    info_read_duplicate->eventType = UvmEventTypeReadDuplicate;
+    info_read_duplicate->size = PAGE_SIZE;
+    info_read_duplicate->timeStamp = NV_GETTIME();
 
-        for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
-            uvm_processor_id_t id;
+    for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
+      uvm_processor_id_t id;
 
-            info_read_duplicate->address = uvm_va_block_cpu_page_address(va_block, page_index);
-            memset(info_read_duplicate->processors, 0, sizeof(info_read_duplicate->processors));
+      info_read_duplicate->address =
+          uvm_va_block_cpu_page_address(va_block, page_index);
+      memset(info_read_duplicate->processors, 0,
+             sizeof(info_read_duplicate->processors));
 
-            uvm_va_block_page_resident_processors(va_block, page_index, resident_processors);
+      uvm_va_block_page_resident_processors(va_block, page_index,
+                                            resident_processors);
 
-            for_each_id_in_mask(id, resident_processors)
-                __set_bit(uvm_id_value(id), (unsigned long *)info_read_duplicate->processors);
+      for_each_id_in_mask(id, resident_processors) __set_bit(
+          uvm_id_value(id), (unsigned long *)info_read_duplicate->processors);
 
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
+      uvm_tools_record_event_v2(va_space, &entry);
     }
+  }
 
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 
-    uvm_processor_mask_cache_free(resident_processors);
+  uvm_processor_mask_cache_free(resident_processors);
 }
 
-void uvm_tools_record_read_duplicate_invalidate(uvm_va_block_t *va_block,
-                                                uvm_processor_id_t dst,
-                                                uvm_va_block_region_t region,
-                                                const uvm_page_mask_t *page_mask)
-{
-    uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
+void uvm_tools_record_read_duplicate_invalidate(
+    uvm_va_block_t *va_block, uvm_processor_id_t dst,
+    uvm_va_block_region_t region, const uvm_page_mask_t *page_mask) {
+  uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeReadDuplicateInvalidate)) {
-        UvmEventEntry entry;
-        uvm_page_index_t page_index;
-        UvmEventReadDuplicateInvalidateInfo *info = &entry.eventData.readDuplicateInvalidate;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space,
+                                UvmEventTypeReadDuplicateInvalidate)) {
+    UvmEventEntry entry;
+    uvm_page_index_t page_index;
+    UvmEventReadDuplicateInvalidateInfo *info =
+        &entry.eventData.readDuplicateInvalidate;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType     = UvmEventTypeReadDuplicateInvalidate;
-        info->residentIndex = uvm_parent_id_value_from_processor_id(dst);
-        info->size          = PAGE_SIZE;
-        info->timeStamp     = NV_GETTIME();
+    info->eventType = UvmEventTypeReadDuplicateInvalidate;
+    info->residentIndex = uvm_parent_id_value_from_processor_id(dst);
+    info->size = PAGE_SIZE;
+    info->timeStamp = NV_GETTIME();
 
-        for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
-            UVM_ASSERT(uvm_page_mask_test(&va_block->read_duplicated_pages, page_index));
+    for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
+      UVM_ASSERT(
+          uvm_page_mask_test(&va_block->read_duplicated_pages, page_index));
 
-            info->address = uvm_va_block_cpu_page_address(va_block, page_index);
-            uvm_tools_record_event(va_space, &entry);
-        }
+      info->address = uvm_va_block_cpu_page_address(va_block, page_index);
+      uvm_tools_record_event(va_space, &entry);
     }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeReadDuplicateInvalidate)) {
-        UvmEventEntry_V2 entry;
-        uvm_page_index_t page_index;
-        UvmEventReadDuplicateInvalidateInfo_V2 *info = &entry.eventData.readDuplicateInvalidate;
+  }
+  if (tools_is_event_enabled_v2(va_space,
+                                UvmEventTypeReadDuplicateInvalidate)) {
+    UvmEventEntry_V2 entry;
+    uvm_page_index_t page_index;
+    UvmEventReadDuplicateInvalidateInfo_V2 *info =
+        &entry.eventData.readDuplicateInvalidate;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType     = UvmEventTypeReadDuplicateInvalidate;
-        info->residentIndex = uvm_id_value(dst);
-        info->size          = PAGE_SIZE;
-        info->timeStamp     = NV_GETTIME();
+    info->eventType = UvmEventTypeReadDuplicateInvalidate;
+    info->residentIndex = uvm_id_value(dst);
+    info->size = PAGE_SIZE;
+    info->timeStamp = NV_GETTIME();
 
-        for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
-            UVM_ASSERT(uvm_page_mask_test(&va_block->read_duplicated_pages, page_index));
+    for_each_va_block_page_in_region_mask(page_index, page_mask, region) {
+      UVM_ASSERT(
+          uvm_page_mask_test(&va_block->read_duplicated_pages, page_index));
 
-            info->address = uvm_va_block_cpu_page_address(va_block, page_index);
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
+      info->address = uvm_va_block_cpu_page_address(va_block, page_index);
+      uvm_tools_record_event_v2(va_space, &entry);
     }
-    uvm_up_read(&va_space->tools.lock);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-static void tools_schedule_completed_events(void)
-{
-    uvm_channel_t *channel;
-    uvm_channel_t *next_channel;
-    NvU64 channel_count = 0;
-    NvU64 i;
+static void tools_schedule_completed_events(void) {
+  uvm_channel_t *channel;
+  uvm_channel_t *next_channel;
+  NvU64 channel_count = 0;
+  NvU64 i;
 
-    uvm_spin_lock(&g_tools_channel_list_lock);
+  uvm_spin_lock(&g_tools_channel_list_lock);
 
-    // retain every channel list entry currently in the list and keep track of their count.
-    list_for_each_entry(channel, &g_tools_channel_list, tools.channel_list_node) {
-        ++channel->tools.pending_event_count;
-        ++channel_count;
-    }
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+  // retain every channel list entry currently in the list and keep track of
+  // their count.
+  list_for_each_entry(channel, &g_tools_channel_list, tools.channel_list_node) {
+    ++channel->tools.pending_event_count;
+    ++channel_count;
+  }
+  uvm_spin_unlock(&g_tools_channel_list_lock);
 
-    if (channel_count == 0)
-        return;
+  if (channel_count == 0)
+    return;
 
-    // new entries always appear at the end, and all the entries seen in the first loop have been retained
-    // so it is safe to go through them
-    channel = list_first_entry(&g_tools_channel_list, uvm_channel_t, tools.channel_list_node);
-    for (i = 0; i < channel_count; i++) {
-        uvm_channel_update_progress_all(channel);
-        channel = list_next_entry(channel, tools.channel_list_node);
-    }
+  // new entries always appear at the end, and all the entries seen in the first
+  // loop have been retained so it is safe to go through them
+  channel = list_first_entry(&g_tools_channel_list, uvm_channel_t,
+                             tools.channel_list_node);
+  for (i = 0; i < channel_count; i++) {
+    uvm_channel_update_progress_all(channel);
+    channel = list_next_entry(channel, tools.channel_list_node);
+  }
 
-    // now release all the entries we retained in the beginning
-    i = 0;
-    uvm_spin_lock(&g_tools_channel_list_lock);
-    list_for_each_entry_safe(channel, next_channel, &g_tools_channel_list, tools.channel_list_node) {
-        if (i++ == channel_count)
-            break;
+  // now release all the entries we retained in the beginning
+  i = 0;
+  uvm_spin_lock(&g_tools_channel_list_lock);
+  list_for_each_entry_safe(channel, next_channel, &g_tools_channel_list,
+                           tools.channel_list_node) {
+    if (i++ == channel_count)
+      break;
 
-        remove_pending_event_for_channel(channel);
-    }
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+    remove_pending_event_for_channel(channel);
+  }
+  uvm_spin_unlock(&g_tools_channel_list_lock);
 }
 
-void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t *va_space,
-                                      NvU64 address,
+void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t *va_space, NvU64 address,
                                       bool is_write,
-                                      UvmEventFatalReason reason)
-{
-    uvm_assert_rwsem_locked(&va_space->lock);
+                                      UvmEventFatalReason reason) {
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeFatalFault)) {
-        UvmEventEntry entry;
-        UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeFatalFault)) {
+    UvmEventEntry entry;
+    UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeFatalFault;
-        info->processorIndex = UVM_ID_CPU_VALUE;
-        info->timeStamp      = NV_GETTIME();
-        info->address        = address;
-        info->accessType     = is_write? UvmEventMemoryAccessTypeWrite: UvmEventMemoryAccessTypeRead;
-        // info->faultType is not valid for cpu faults
-        info->reason         = reason;
-
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeFatalFault)) {
-        UvmEventEntry_V2 entry;
-        UvmEventFatalFaultInfo_V2 *info = &entry.eventData.fatalFault;
+    info->eventType = UvmEventTypeFatalFault;
+    info->processorIndex = UVM_ID_CPU_VALUE;
+    info->timeStamp = NV_GETTIME();
+    info->address = address;
+    info->accessType =
+        is_write ? UvmEventMemoryAccessTypeWrite : UvmEventMemoryAccessTypeRead;
+    // info->faultType is not valid for cpu faults
+    info->reason = reason;
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeFatalFault)) {
+    UvmEventEntry_V2 entry;
+    UvmEventFatalFaultInfo_V2 *info = &entry.eventData.fatalFault;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeFatalFault;
-        info->processorIndex = UVM_ID_CPU_VALUE;
-        info->timeStamp      = NV_GETTIME();
-        info->address        = address;
-        info->accessType     = is_write? UvmEventMemoryAccessTypeWrite: UvmEventMemoryAccessTypeRead;
-        // info->faultType is not valid for cpu faults
-        info->reason         = reason;
+    info->eventType = UvmEventTypeFatalFault;
+    info->processorIndex = UVM_ID_CPU_VALUE;
+    info->timeStamp = NV_GETTIME();
+    info->address = address;
+    info->accessType =
+        is_write ? UvmEventMemoryAccessTypeWrite : UvmEventMemoryAccessTypeRead;
+    // info->faultType is not valid for cpu faults
+    info->reason = reason;
 
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
-    uvm_up_read(&va_space->tools.lock);
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-void uvm_tools_record_gpu_fatal_fault(uvm_gpu_id_t gpu_id,
-                                      uvm_va_space_t *va_space,
-                                      const uvm_fault_buffer_entry_t *buffer_entry,
-                                      UvmEventFatalReason reason)
-{
-    uvm_assert_rwsem_locked(&va_space->lock);
+void uvm_tools_record_gpu_fatal_fault(
+    uvm_gpu_id_t gpu_id, uvm_va_space_t *va_space,
+    const uvm_fault_buffer_entry_t *buffer_entry, UvmEventFatalReason reason) {
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeFatalFault)) {
-        UvmEventEntry entry;
-        UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeFatalFault)) {
+    UvmEventEntry entry;
+    UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeFatalFault;
-        info->processorIndex = uvm_parent_id_value_from_processor_id(gpu_id);
-        info->timeStamp      = NV_GETTIME();
-        info->address        = buffer_entry->fault_address;
-        info->accessType     = g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
-        info->faultType      = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
-        info->reason         = reason;
-
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeFatalFault)) {
-        UvmEventEntry_V2 entry;
-        UvmEventFatalFaultInfo_V2 *info = &entry.eventData.fatalFault;
+    info->eventType = UvmEventTypeFatalFault;
+    info->processorIndex = uvm_parent_id_value_from_processor_id(gpu_id);
+    info->timeStamp = NV_GETTIME();
+    info->address = buffer_entry->fault_address;
+    info->accessType =
+        g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
+    info->faultType = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
+    info->reason = reason;
+
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeFatalFault)) {
+    UvmEventEntry_V2 entry;
+    UvmEventFatalFaultInfo_V2 *info = &entry.eventData.fatalFault;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeFatalFault;
-        info->processorIndex = uvm_id_value(gpu_id);
-        info->timeStamp      = NV_GETTIME();
-        info->address        = buffer_entry->fault_address;
-        info->accessType     = g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
-        info->faultType      = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
-        info->reason         = reason;
+    info->eventType = UvmEventTypeFatalFault;
+    info->processorIndex = uvm_id_value(gpu_id);
+    info->timeStamp = NV_GETTIME();
+    info->address = buffer_entry->fault_address;
+    info->accessType =
+        g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
+    info->faultType = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
+    info->reason = reason;
 
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
-    uvm_up_read(&va_space->tools.lock);
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-void uvm_tools_record_thrashing(uvm_va_space_t *va_space,
-                                NvU64 address,
+void uvm_tools_record_thrashing(uvm_va_space_t *va_space, NvU64 address,
                                 size_t region_size,
-                                const uvm_processor_mask_t *processors)
-{
-    UVM_ASSERT(address);
-    UVM_ASSERT(PAGE_ALIGNED(address));
-    UVM_ASSERT(region_size > 0);
+                                const uvm_processor_mask_t *processors) {
+  UVM_ASSERT(address);
+  UVM_ASSERT(PAGE_ALIGNED(address));
+  UVM_ASSERT(region_size > 0);
 
-    uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrashingDetected)) {
-        UvmEventEntry entry;
-        UvmEventThrashingDetectedInfo *info = &entry.eventData.thrashing;
-        uvm_processor_id_t id;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrashingDetected)) {
+    UvmEventEntry entry;
+    UvmEventThrashingDetectedInfo *info = &entry.eventData.thrashing;
+    uvm_processor_id_t id;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType = UvmEventTypeThrashingDetected;
-        info->address   = address;
-        info->size      = region_size;
-        info->timeStamp = NV_GETTIME();
+    info->eventType = UvmEventTypeThrashingDetected;
+    info->address = address;
+    info->size = region_size;
+    info->timeStamp = NV_GETTIME();
 
-        for_each_id_in_mask(id, processors)
-            __set_bit(uvm_parent_id_value_from_processor_id(id),
-                      (unsigned long *)&info->processors);
+    for_each_id_in_mask(id, processors)
+        __set_bit(uvm_parent_id_value_from_processor_id(id),
+                  (unsigned long *)&info->processors);
 
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrashingDetected)) {
-        UvmEventEntry_V2 entry;
-        UvmEventThrashingDetectedInfo_V2 *info = &entry.eventData.thrashing;
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrashingDetected)) {
+    UvmEventEntry_V2 entry;
+    UvmEventThrashingDetectedInfo_V2 *info = &entry.eventData.thrashing;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType = UvmEventTypeThrashingDetected;
-        info->address   = address;
-        info->size      = region_size;
-        info->timeStamp = NV_GETTIME();
+    info->eventType = UvmEventTypeThrashingDetected;
+    info->address = address;
+    info->size = region_size;
+    info->timeStamp = NV_GETTIME();
 
-        BUILD_BUG_ON(UVM_MAX_PROCESSORS < UVM_ID_MAX_PROCESSORS);
-        bitmap_copy((long unsigned *)&info->processors, processors->bitmap, UVM_ID_MAX_PROCESSORS);
+    BUILD_BUG_ON(UVM_MAX_PROCESSORS < UVM_ID_MAX_PROCESSORS);
+    bitmap_copy((long unsigned *)&info->processors, processors->bitmap,
+                UVM_ID_MAX_PROCESSORS);
 
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
-    uvm_up_read(&va_space->tools.lock);
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-void uvm_tools_record_throttling_start(uvm_va_space_t *va_space, NvU64 address, uvm_processor_id_t processor)
-{
-    UVM_ASSERT(address);
-    UVM_ASSERT(PAGE_ALIGNED(address));
-    UVM_ASSERT(UVM_ID_IS_VALID(processor));
+void uvm_tools_record_throttling_start(uvm_va_space_t *va_space, NvU64 address,
+                                       uvm_processor_id_t processor) {
+  UVM_ASSERT(address);
+  UVM_ASSERT(PAGE_ALIGNED(address));
+  UVM_ASSERT(UVM_ID_IS_VALID(processor));
 
-    uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrottlingStart)) {
-        UvmEventEntry entry;
-        UvmEventThrottlingStartInfo *info = &entry.eventData.throttlingStart;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrottlingStart)) {
+    UvmEventEntry entry;
+    UvmEventThrottlingStartInfo *info = &entry.eventData.throttlingStart;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeThrottlingStart;
-        info->processorIndex = uvm_parent_id_value_from_processor_id(processor);
-        info->address        = address;
-        info->timeStamp      = NV_GETTIME();
+    info->eventType = UvmEventTypeThrottlingStart;
+    info->processorIndex = uvm_parent_id_value_from_processor_id(processor);
+    info->address = address;
+    info->timeStamp = NV_GETTIME();
 
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrottlingStart)) {
-        UvmEventEntry_V2 entry;
-        UvmEventThrottlingStartInfo_V2 *info = &entry.eventData.throttlingStart;
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrottlingStart)) {
+    UvmEventEntry_V2 entry;
+    UvmEventThrottlingStartInfo_V2 *info = &entry.eventData.throttlingStart;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeThrottlingStart;
-        info->processorIndex = uvm_id_value(processor);
-        info->address        = address;
-        info->timeStamp      = NV_GETTIME();
+    info->eventType = UvmEventTypeThrottlingStart;
+    info->processorIndex = uvm_id_value(processor);
+    info->address = address;
+    info->timeStamp = NV_GETTIME();
 
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
-    uvm_up_read(&va_space->tools.lock);
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-void uvm_tools_record_throttling_end(uvm_va_space_t *va_space, NvU64 address, uvm_processor_id_t processor)
-{
-    UVM_ASSERT(address);
-    UVM_ASSERT(PAGE_ALIGNED(address));
-    UVM_ASSERT(UVM_ID_IS_VALID(processor));
+void uvm_tools_record_throttling_end(uvm_va_space_t *va_space, NvU64 address,
+                                     uvm_processor_id_t processor) {
+  UVM_ASSERT(address);
+  UVM_ASSERT(PAGE_ALIGNED(address));
+  UVM_ASSERT(UVM_ID_IS_VALID(processor));
 
-    uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
-    if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrottlingEnd)) {
-        UvmEventEntry entry;
-        UvmEventThrottlingEndInfo *info = &entry.eventData.throttlingEnd;
+  uvm_down_read(&va_space->tools.lock);
+  if (tools_is_event_enabled_v1(va_space, UvmEventTypeThrottlingEnd)) {
+    UvmEventEntry entry;
+    UvmEventThrottlingEndInfo *info = &entry.eventData.throttlingEnd;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeThrottlingEnd;
-        info->processorIndex = uvm_parent_id_value_from_processor_id(processor);
-        info->address        = address;
-        info->timeStamp      = NV_GETTIME();
+    info->eventType = UvmEventTypeThrottlingEnd;
+    info->processorIndex = uvm_parent_id_value_from_processor_id(processor);
+    info->address = address;
+    info->timeStamp = NV_GETTIME();
 
-        uvm_tools_record_event(va_space, &entry);
-    }
-    if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrottlingEnd)) {
-        UvmEventEntry_V2 entry;
-        UvmEventThrottlingEndInfo_V2 *info = &entry.eventData.throttlingEnd;
+    uvm_tools_record_event(va_space, &entry);
+  }
+  if (tools_is_event_enabled_v2(va_space, UvmEventTypeThrottlingEnd)) {
+    UvmEventEntry_V2 entry;
+    UvmEventThrottlingEndInfo_V2 *info = &entry.eventData.throttlingEnd;
 
-        memset(&entry, 0, sizeof(entry));
+    memset(&entry, 0, sizeof(entry));
 
-        info->eventType      = UvmEventTypeThrottlingEnd;
-        info->processorIndex = uvm_id_value(processor);
-        info->address        = address;
-        info->timeStamp      = NV_GETTIME();
+    info->eventType = UvmEventTypeThrottlingEnd;
+    info->processorIndex = uvm_id_value(processor);
+    info->address = address;
+    info->timeStamp = NV_GETTIME();
 
-        uvm_tools_record_event_v2(va_space, &entry);
-    }
-    uvm_up_read(&va_space->tools.lock);
+    uvm_tools_record_event_v2(va_space, &entry);
+  }
+  uvm_up_read(&va_space->tools.lock);
 }
 
-static void record_map_remote_events(void *args)
-{
-    block_map_remote_data_t *block_map_remote = (block_map_remote_data_t *)args;
-    map_remote_data_t *map_remote, *next;
-    uvm_va_space_t *va_space = block_map_remote->va_space;
-
-    uvm_down_read(&va_space->tools.lock);
-    list_for_each_entry_safe(map_remote, next, &block_map_remote->events, events_node) {
-        list_del(&map_remote->events_node);
+static void record_map_remote_events(void *args) {
+  block_map_remote_data_t *block_map_remote = (block_map_remote_data_t *)args;
+  map_remote_data_t *map_remote, *next;
+  uvm_va_space_t *va_space = block_map_remote->va_space;
 
-        if (tools_is_event_enabled_v1(va_space, UvmEventTypeMapRemote)) {
-            UvmEventEntry entry;
+  uvm_down_read(&va_space->tools.lock);
+  list_for_each_entry_safe(map_remote, next, &block_map_remote->events,
+                           events_node) {
+    list_del(&map_remote->events_node);
 
-            memset(&entry, 0, sizeof(entry));
+    if (tools_is_event_enabled_v1(va_space, UvmEventTypeMapRemote)) {
+      UvmEventEntry entry;
 
-            entry.eventData.mapRemote.eventType      = UvmEventTypeMapRemote;
-            entry.eventData.mapRemote.srcIndex       = uvm_parent_id_value_from_processor_id(block_map_remote->src);
-            entry.eventData.mapRemote.dstIndex       = uvm_parent_id_value_from_processor_id(block_map_remote->dst);
-            entry.eventData.mapRemote.mapRemoteCause = block_map_remote->cause;
-            entry.eventData.mapRemote.timeStamp      = block_map_remote->timestamp;
-            entry.eventData.mapRemote.address        = map_remote->address;
-            entry.eventData.mapRemote.size           = map_remote->size;
-            entry.eventData.mapRemote.timeStampGpu   = map_remote->timestamp_gpu;
+      memset(&entry, 0, sizeof(entry));
 
-            uvm_tools_record_event(va_space, &entry);
-        }
+      entry.eventData.mapRemote.eventType = UvmEventTypeMapRemote;
+      entry.eventData.mapRemote.srcIndex =
+          uvm_parent_id_value_from_processor_id(block_map_remote->src);
+      entry.eventData.mapRemote.dstIndex =
+          uvm_parent_id_value_from_processor_id(block_map_remote->dst);
+      entry.eventData.mapRemote.mapRemoteCause = block_map_remote->cause;
+      entry.eventData.mapRemote.timeStamp = block_map_remote->timestamp;
+      entry.eventData.mapRemote.address = map_remote->address;
+      entry.eventData.mapRemote.size = map_remote->size;
+      entry.eventData.mapRemote.timeStampGpu = map_remote->timestamp_gpu;
 
-        if (tools_is_event_enabled_v2(va_space, UvmEventTypeMapRemote)) {
-            UvmEventEntry_V2 entry;
+      uvm_tools_record_event(va_space, &entry);
+    }
 
-            memset(&entry, 0, sizeof(entry));
+    if (tools_is_event_enabled_v2(va_space, UvmEventTypeMapRemote)) {
+      UvmEventEntry_V2 entry;
 
-            entry.eventData.mapRemote.eventType      = UvmEventTypeMapRemote;
-            entry.eventData.mapRemote.srcIndex       = uvm_id_value(block_map_remote->src);
-            entry.eventData.mapRemote.dstIndex       = uvm_id_value(block_map_remote->dst);
-            entry.eventData.mapRemote.mapRemoteCause = block_map_remote->cause;
-            entry.eventData.mapRemote.timeStamp      = block_map_remote->timestamp;
-            entry.eventData.mapRemote.address        = map_remote->address;
-            entry.eventData.mapRemote.size           = map_remote->size;
-            entry.eventData.mapRemote.timeStampGpu   = map_remote->timestamp_gpu;
+      memset(&entry, 0, sizeof(entry));
 
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
+      entry.eventData.mapRemote.eventType = UvmEventTypeMapRemote;
+      entry.eventData.mapRemote.srcIndex = uvm_id_value(block_map_remote->src);
+      entry.eventData.mapRemote.dstIndex = uvm_id_value(block_map_remote->dst);
+      entry.eventData.mapRemote.mapRemoteCause = block_map_remote->cause;
+      entry.eventData.mapRemote.timeStamp = block_map_remote->timestamp;
+      entry.eventData.mapRemote.address = map_remote->address;
+      entry.eventData.mapRemote.size = map_remote->size;
+      entry.eventData.mapRemote.timeStampGpu = map_remote->timestamp_gpu;
 
-        kmem_cache_free(g_tools_map_remote_data_cache, map_remote);
+      uvm_tools_record_event_v2(va_space, &entry);
     }
-    uvm_up_read(&va_space->tools.lock);
 
-    UVM_ASSERT(list_empty(&block_map_remote->events));
-    kmem_cache_free(g_tools_block_map_remote_data_cache, block_map_remote);
+    kmem_cache_free(g_tools_map_remote_data_cache, map_remote);
+  }
+  uvm_up_read(&va_space->tools.lock);
+
+  UVM_ASSERT(list_empty(&block_map_remote->events));
+  kmem_cache_free(g_tools_block_map_remote_data_cache, block_map_remote);
 }
 
-static void record_map_remote_events_entry(void *args)
-{
-    UVM_ENTRY_VOID(record_map_remote_events(args));
+static void record_map_remote_events_entry(void *args) {
+  UVM_ENTRY_VOID(record_map_remote_events(args));
 }
 
-static void on_map_remote_complete(void *ptr)
-{
-    block_map_remote_data_t *block_map_remote = (block_map_remote_data_t *)ptr;
-    map_remote_data_t *map_remote;
+static void on_map_remote_complete(void *ptr) {
+  block_map_remote_data_t *block_map_remote = (block_map_remote_data_t *)ptr;
+  map_remote_data_t *map_remote;
 
-    // Only GPU mappings use the deferred mechanism
-    UVM_ASSERT(UVM_ID_IS_GPU(block_map_remote->src));
-    list_for_each_entry(map_remote, &block_map_remote->events, events_node)
-        map_remote->timestamp_gpu = *map_remote->timestamp_gpu_addr;
+  // Only GPU mappings use the deferred mechanism
+  UVM_ASSERT(UVM_ID_IS_GPU(block_map_remote->src));
+  list_for_each_entry(map_remote, &block_map_remote->events, events_node)
+      map_remote->timestamp_gpu = *map_remote->timestamp_gpu_addr;
 
-    nv_kthread_q_item_init(&block_map_remote->queue_item, record_map_remote_events_entry, ptr);
+  nv_kthread_q_item_init(&block_map_remote->queue_item,
+                         record_map_remote_events_entry, ptr);
 
-    uvm_spin_lock(&g_tools_channel_list_lock);
-    remove_pending_event_for_channel(block_map_remote->channel);
-    nv_kthread_q_schedule_q_item(&g_tools_queue, &block_map_remote->queue_item);
-    uvm_spin_unlock(&g_tools_channel_list_lock);
+  uvm_spin_lock(&g_tools_channel_list_lock);
+  remove_pending_event_for_channel(block_map_remote->channel);
+  nv_kthread_q_schedule_q_item(&g_tools_queue, &block_map_remote->queue_item);
+  uvm_spin_unlock(&g_tools_channel_list_lock);
 }
 
-void uvm_tools_record_map_remote(uvm_va_block_t *va_block,
-                                 uvm_push_t *push,
+void uvm_tools_record_map_remote(uvm_va_block_t *va_block, uvm_push_t *push,
                                  uvm_processor_id_t processor,
-                                 uvm_processor_id_t residency,
-                                 NvU64 address,
+                                 uvm_processor_id_t residency, NvU64 address,
                                  size_t region_size,
-                                 UvmEventMapRemoteCause cause)
-{
-    uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
+                                 UvmEventMapRemoteCause cause) {
+  uvm_va_space_t *va_space = uvm_va_block_get_va_space(va_block);
 
-    UVM_ASSERT(UVM_ID_IS_VALID(processor));
-    UVM_ASSERT(UVM_ID_IS_VALID(residency));
-    UVM_ASSERT(cause != UvmEventMapRemoteCauseInvalid);
+  UVM_ASSERT(UVM_ID_IS_VALID(processor));
+  UVM_ASSERT(UVM_ID_IS_VALID(residency));
+  UVM_ASSERT(cause != UvmEventMapRemoteCauseInvalid);
 
-    uvm_assert_rwsem_locked(&va_space->lock);
+  uvm_assert_rwsem_locked(&va_space->lock);
 
-    if (!va_space->tools.enabled)
-        return;
+  if (!va_space->tools.enabled)
+    return;
 
-    uvm_down_read(&va_space->tools.lock);
+  uvm_down_read(&va_space->tools.lock);
 
-    if (UVM_ID_IS_CPU(processor)) {
-        if (tools_is_event_enabled_v1(va_space, UvmEventTypeMapRemote)) {
-            UvmEventEntry entry;
+  if (UVM_ID_IS_CPU(processor)) {
+    if (tools_is_event_enabled_v1(va_space, UvmEventTypeMapRemote)) {
+      UvmEventEntry entry;
 
-            memset(&entry, 0, sizeof(entry));
+      memset(&entry, 0, sizeof(entry));
 
-            entry.eventData.mapRemote.eventType      = UvmEventTypeMapRemote;
-            entry.eventData.mapRemote.srcIndex       = uvm_parent_id_value_from_processor_id(processor);
-            entry.eventData.mapRemote.dstIndex       = uvm_parent_id_value_from_processor_id(residency);
-            entry.eventData.mapRemote.mapRemoteCause = cause;
-            entry.eventData.mapRemote.timeStamp      = NV_GETTIME();
-            entry.eventData.mapRemote.address        = address;
-            entry.eventData.mapRemote.size           = region_size;
-            entry.eventData.mapRemote.timeStampGpu   = 0;
+      entry.eventData.mapRemote.eventType = UvmEventTypeMapRemote;
+      entry.eventData.mapRemote.srcIndex =
+          uvm_parent_id_value_from_processor_id(processor);
+      entry.eventData.mapRemote.dstIndex =
+          uvm_parent_id_value_from_processor_id(residency);
+      entry.eventData.mapRemote.mapRemoteCause = cause;
+      entry.eventData.mapRemote.timeStamp = NV_GETTIME();
+      entry.eventData.mapRemote.address = address;
+      entry.eventData.mapRemote.size = region_size;
+      entry.eventData.mapRemote.timeStampGpu = 0;
 
-            UVM_ASSERT(entry.eventData.mapRemote.mapRemoteCause != UvmEventMapRemoteCauseInvalid);
+      UVM_ASSERT(entry.eventData.mapRemote.mapRemoteCause !=
+                 UvmEventMapRemoteCauseInvalid);
 
-            uvm_tools_record_event(va_space, &entry);
-        }
-        if (tools_is_event_enabled_v2(va_space, UvmEventTypeMapRemote)) {
-            UvmEventEntry_V2 entry;
+      uvm_tools_record_event(va_space, &entry);
+    }
+    if (tools_is_event_enabled_v2(va_space, UvmEventTypeMapRemote)) {
+      UvmEventEntry_V2 entry;
 
-            memset(&entry, 0, sizeof(entry));
+      memset(&entry, 0, sizeof(entry));
 
-            entry.eventData.mapRemote.eventType      = UvmEventTypeMapRemote;
-            entry.eventData.mapRemote.srcIndex       = uvm_id_value(processor);
-            entry.eventData.mapRemote.dstIndex       = uvm_id_value(residency);
-            entry.eventData.mapRemote.mapRemoteCause = cause;
-            entry.eventData.mapRemote.timeStamp      = NV_GETTIME();
-            entry.eventData.mapRemote.address        = address;
-            entry.eventData.mapRemote.size           = region_size;
-            entry.eventData.mapRemote.timeStampGpu   = 0;
+      entry.eventData.mapRemote.eventType = UvmEventTypeMapRemote;
+      entry.eventData.mapRemote.srcIndex = uvm_id_value(processor);
+      entry.eventData.mapRemote.dstIndex = uvm_id_value(residency);
+      entry.eventData.mapRemote.mapRemoteCause = cause;
+      entry.eventData.mapRemote.timeStamp = NV_GETTIME();
+      entry.eventData.mapRemote.address = address;
+      entry.eventData.mapRemote.size = region_size;
+      entry.eventData.mapRemote.timeStampGpu = 0;
 
-            UVM_ASSERT(entry.eventData.mapRemote.mapRemoteCause != UvmEventMapRemoteCauseInvalid);
+      UVM_ASSERT(entry.eventData.mapRemote.mapRemoteCause !=
+                 UvmEventMapRemoteCauseInvalid);
 
-            uvm_tools_record_event_v2(va_space, &entry);
-        }
+      uvm_tools_record_event_v2(va_space, &entry);
     }
-    else if (tools_is_event_enabled(va_space, UvmEventTypeMapRemote)) {
-        uvm_push_info_t *push_info = uvm_push_info_from_push(push);
-        block_map_remote_data_t *block_map_remote;
-        map_remote_data_t *map_remote;
-
-        // The first call on this pushbuffer creates the per-VA block structure
-        if (push_info->on_complete == NULL) {
-            UVM_ASSERT(push_info->on_complete_data == NULL);
-
-            block_map_remote = kmem_cache_alloc(g_tools_block_map_remote_data_cache, NV_UVM_GFP_FLAGS);
-            if (block_map_remote == NULL)
-                goto done;
-
-            block_map_remote->src = processor;
-            block_map_remote->dst = residency;
-            block_map_remote->cause = cause;
-            block_map_remote->timestamp = NV_GETTIME();
-            block_map_remote->va_space = va_space;
-            block_map_remote->channel = push->channel;
-            INIT_LIST_HEAD(&block_map_remote->events);
-
-            push_info->on_complete_data = block_map_remote;
-            push_info->on_complete = on_map_remote_complete;
-
-            uvm_spin_lock(&g_tools_channel_list_lock);
-            add_pending_event_for_channel(block_map_remote->channel);
-            uvm_spin_unlock(&g_tools_channel_list_lock);
-        }
-        else {
-            block_map_remote = push_info->on_complete_data;
-        }
-        UVM_ASSERT(block_map_remote);
-
-        map_remote = kmem_cache_alloc(g_tools_map_remote_data_cache, NV_UVM_GFP_FLAGS);
-        if (map_remote == NULL)
-            goto done;
-
-        map_remote->address = address;
-        map_remote->size = region_size;
-        map_remote->timestamp_gpu_addr = uvm_push_timestamp(push);
-
-        list_add_tail(&map_remote->events_node, &block_map_remote->events);
+  } else if (tools_is_event_enabled(va_space, UvmEventTypeMapRemote)) {
+    uvm_push_info_t *push_info = uvm_push_info_from_push(push);
+    block_map_remote_data_t *block_map_remote;
+    map_remote_data_t *map_remote;
+
+    // The first call on this pushbuffer creates the per-VA block structure
+    if (push_info->on_complete == NULL) {
+      UVM_ASSERT(push_info->on_complete_data == NULL);
+
+      block_map_remote = kmem_cache_alloc(g_tools_block_map_remote_data_cache,
+                                          NV_UVM_GFP_FLAGS);
+      if (block_map_remote == NULL)
+        goto done;
+
+      block_map_remote->src = processor;
+      block_map_remote->dst = residency;
+      block_map_remote->cause = cause;
+      block_map_remote->timestamp = NV_GETTIME();
+      block_map_remote->va_space = va_space;
+      block_map_remote->channel = push->channel;
+      INIT_LIST_HEAD(&block_map_remote->events);
+
+      push_info->on_complete_data = block_map_remote;
+      push_info->on_complete = on_map_remote_complete;
+
+      uvm_spin_lock(&g_tools_channel_list_lock);
+      add_pending_event_for_channel(block_map_remote->channel);
+      uvm_spin_unlock(&g_tools_channel_list_lock);
+    } else {
+      block_map_remote = push_info->on_complete_data;
     }
+    UVM_ASSERT(block_map_remote);
+
+    map_remote =
+        kmem_cache_alloc(g_tools_map_remote_data_cache, NV_UVM_GFP_FLAGS);
+    if (map_remote == NULL)
+      goto done;
+
+    map_remote->address = address;
+    map_remote->size = region_size;
+    map_remote->timestamp_gpu_addr = uvm_push_timestamp(push);
+
+    list_add_tail(&map_remote->events_node, &block_map_remote->events);
+  }
 
 done:
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 }
 
-static NV_STATUS create_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params,
-                                      size_t entry_size,
-                                      struct file *filp)
-{
-    NV_STATUS status = NV_OK;
-    uvm_tools_event_tracker_t *event_tracker;
+static NV_STATUS
+create_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params,
+                     size_t entry_size, struct file *filp) {
+  NV_STATUS status = NV_OK;
+  uvm_tools_event_tracker_t *event_tracker;
 
-    event_tracker = nv_kmem_cache_zalloc(g_tools_event_tracker_cache, NV_UVM_GFP_FLAGS);
-    if (event_tracker == NULL)
-        return NV_ERR_NO_MEMORY;
+  event_tracker =
+      nv_kmem_cache_zalloc(g_tools_event_tracker_cache, NV_UVM_GFP_FLAGS);
+  if (event_tracker == NULL)
+    return NV_ERR_NO_MEMORY;
 
-    event_tracker->entry_size = entry_size;
+  event_tracker->entry_size = entry_size;
 
-    event_tracker->uvm_file = fget(params->uvmFd);
-    if (event_tracker->uvm_file == NULL) {
-        status = NV_ERR_INSUFFICIENT_PERMISSIONS;
-        goto fail;
-    }
+  event_tracker->uvm_file = fget(params->uvmFd);
+  if (event_tracker->uvm_file == NULL) {
+    status = NV_ERR_INSUFFICIENT_PERMISSIONS;
+    goto fail;
+  }
 
-    if (!uvm_file_is_nvidia_uvm(event_tracker->uvm_file)) {
-        fput(event_tracker->uvm_file);
-        event_tracker->uvm_file = NULL;
-        status = NV_ERR_INSUFFICIENT_PERMISSIONS;
-        goto fail;
-    }
+  if (!uvm_file_is_nvidia_uvm(event_tracker->uvm_file)) {
+    fput(event_tracker->uvm_file);
+    event_tracker->uvm_file = NULL;
+    status = NV_ERR_INSUFFICIENT_PERMISSIONS;
+    goto fail;
+  }
 
-    // We don't use uvm_fd_va_space() here because tools can work
-    // without an associated va_space_mm.
-    if (!uvm_fd_get_type(event_tracker->uvm_file, UVM_FD_VA_SPACE)) {
-        fput(event_tracker->uvm_file);
-        event_tracker->uvm_file = NULL;
-        status = NV_ERR_ILLEGAL_ACTION;
-        goto fail;
-    }
+  // We don't use uvm_fd_va_space() here because tools can work
+  // without an associated va_space_mm.
+  if (!uvm_fd_get_type(event_tracker->uvm_file, UVM_FD_VA_SPACE)) {
+    fput(event_tracker->uvm_file);
+    event_tracker->uvm_file = NULL;
+    status = NV_ERR_ILLEGAL_ACTION;
+    goto fail;
+  }
 
-    event_tracker->is_queue = params->queueBufferSize != 0;
-    if (event_tracker->is_queue) {
-        uvm_tools_queue_t *queue = &event_tracker->queue;
-        NvU64 buffer_size;
-
-        uvm_spin_lock_init(&queue->lock, UVM_LOCK_ORDER_LEAF);
-        init_waitqueue_head(&queue->wait_queue);
-
-        if (params->queueBufferSize > UINT_MAX) {
-            status = NV_ERR_INVALID_ARGUMENT;
-            goto fail;
-        }
-
-        queue->queue_buffer_count = (NvU32)params->queueBufferSize;
-        queue->notification_threshold = queue->queue_buffer_count / 2;
-
-        // queue_buffer_count must be a power of 2, of at least 2
-        if (!is_power_of_2(queue->queue_buffer_count) || queue->queue_buffer_count < 2) {
-            status = NV_ERR_INVALID_ARGUMENT;
-            goto fail;
-        }
-
-        buffer_size = queue->queue_buffer_count * entry_size;
-
-        status = map_user_pages(params->queueBuffer,
-                                buffer_size,
-                                &queue->queue_buffer,
-                                &queue->queue_buffer_pages);
-        if (status != NV_OK)
-            goto fail;
-
-        status = map_user_pages(params->controlBuffer,
-                                sizeof(UvmToolsEventControlData),
-                                (void **)&queue->control,
-                                &queue->control_buffer_pages);
-
-        if (status != NV_OK)
-            goto fail;
-    }
-    else {
-        uvm_tools_counter_t *counter = &event_tracker->counter;
-        counter->all_processors = params->allProcessors;
-        counter->processor = params->processor;
-        status = map_user_pages(params->controlBuffer,
-                                sizeof(NvU64) * UVM_TOTAL_COUNTERS,
-                                (void **)&counter->counters,
-                                &counter->counter_buffer_pages);
-        if (status != NV_OK)
-            goto fail;
+  event_tracker->is_queue = params->queueBufferSize != 0;
+  if (event_tracker->is_queue) {
+    uvm_tools_queue_t *queue = &event_tracker->queue;
+    NvU64 buffer_size;
+
+    uvm_spin_lock_init(&queue->lock, UVM_LOCK_ORDER_LEAF);
+    init_waitqueue_head(&queue->wait_queue);
+
+    if (params->queueBufferSize > UINT_MAX) {
+      status = NV_ERR_INVALID_ARGUMENT;
+      goto fail;
     }
 
-    if (atomic_long_cmpxchg((atomic_long_t *)&filp->private_data, 0, (long) event_tracker) != 0) {
-        status = NV_ERR_INVALID_ARGUMENT;
-        goto fail;
+    queue->queue_buffer_count = (NvU32)params->queueBufferSize;
+    queue->notification_threshold = queue->queue_buffer_count / 2;
+
+    // queue_buffer_count must be a power of 2, of at least 2
+    if (!is_power_of_2(queue->queue_buffer_count) ||
+        queue->queue_buffer_count < 2) {
+      status = NV_ERR_INVALID_ARGUMENT;
+      goto fail;
     }
 
-    return NV_OK;
+    buffer_size = queue->queue_buffer_count * entry_size;
+
+    status = map_user_pages(params->queueBuffer, buffer_size,
+                            &queue->queue_buffer, &queue->queue_buffer_pages);
+    if (status != NV_OK)
+      goto fail;
+
+    status =
+        map_user_pages(params->controlBuffer, sizeof(UvmToolsEventControlData),
+                       (void **)&queue->control, &queue->control_buffer_pages);
+
+    if (status != NV_OK)
+      goto fail;
+  } else {
+    uvm_tools_counter_t *counter = &event_tracker->counter;
+    counter->all_processors = params->allProcessors;
+    counter->processor = params->processor;
+    status = map_user_pages(
+        params->controlBuffer, sizeof(NvU64) * UVM_TOTAL_COUNTERS,
+        (void **)&counter->counters, &counter->counter_buffer_pages);
+    if (status != NV_OK)
+      goto fail;
+  }
+
+  if (atomic_long_cmpxchg((atomic_long_t *)&filp->private_data, 0,
+                          (long)event_tracker) != 0) {
+    status = NV_ERR_INVALID_ARGUMENT;
+    goto fail;
+  }
+
+  return NV_OK;
 
 fail:
-    destroy_event_tracker(event_tracker);
-    return status;
+  destroy_event_tracker(event_tracker);
+  return status;
 }
 
-NV_STATUS uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *params, struct file *filp)
-{
-    UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params_v2 = (UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *) params;
+NV_STATUS
+uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *params,
+                                 struct file *filp) {
+  UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params_v2 =
+      (UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *)params;
 
-    BUILD_BUG_ON(!__same_type(params, params_v2));
+  BUILD_BUG_ON(!__same_type(params, params_v2));
 
-    return create_event_tracker(params_v2, sizeof(UvmEventEntry), filp);
+  return create_event_tracker(params_v2, sizeof(UvmEventEntry), filp);
 }
 
-NV_STATUS uvm_api_tools_init_event_tracker_v2(UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params, struct file *filp)
-{
-    return create_event_tracker(params, sizeof(UvmEventEntry_V2), filp);
+NV_STATUS uvm_api_tools_init_event_tracker_v2(
+    UVM_TOOLS_INIT_EVENT_TRACKER_V2_PARAMS *params, struct file *filp) {
+  return create_event_tracker(params, sizeof(UvmEventEntry_V2), filp);
 }
 
-NV_STATUS uvm_api_tools_set_notification_threshold(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS *params, struct file *filp)
-{
-    uvm_tools_queue_snapshot_t sn;
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-    UvmToolsEventControlData *ctrl;
+NV_STATUS uvm_api_tools_set_notification_threshold(
+    UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS *params, struct file *filp) {
+  uvm_tools_queue_snapshot_t sn;
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+  UvmToolsEventControlData *ctrl;
 
-    if (!tracker_is_queue(event_tracker))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (!tracker_is_queue(event_tracker))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_spin_lock(&event_tracker->queue.lock);
+  uvm_spin_lock(&event_tracker->queue.lock);
 
-    event_tracker->queue.notification_threshold = params->notificationThreshold;
+  event_tracker->queue.notification_threshold = params->notificationThreshold;
 
-    ctrl = event_tracker->queue.control;
-    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
-    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
+  ctrl = event_tracker->queue.control;
+  sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
+  sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
 
-    if (queue_needs_wakeup(&event_tracker->queue, &sn))
-        wake_up_all(&event_tracker->queue.wait_queue);
+  if (queue_needs_wakeup(&event_tracker->queue, &sn))
+    wake_up_all(&event_tracker->queue.wait_queue);
 
-    uvm_spin_unlock(&event_tracker->queue.lock);
+  uvm_spin_unlock(&event_tracker->queue.lock);
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static NV_STATUS tools_update_perf_events_callbacks(uvm_va_space_t *va_space)
-{
-    NV_STATUS status;
+static NV_STATUS tools_update_perf_events_callbacks(uvm_va_space_t *va_space) {
+  NV_STATUS status;
 
-    uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
-    uvm_assert_rwsem_locked_write(&va_space->tools.lock);
+  uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
+  uvm_assert_rwsem_locked_write(&va_space->tools.lock);
 
-    if (tools_is_fault_callback_needed(va_space)) {
-        if (!uvm_perf_is_event_callback_registered(&va_space->perf_events, UVM_PERF_EVENT_FAULT, uvm_tools_record_fault)) {
-            status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
-                                                             UVM_PERF_EVENT_FAULT,
-                                                             uvm_tools_record_fault);
+  if (tools_is_fault_callback_needed(va_space)) {
+    if (!uvm_perf_is_event_callback_registered(&va_space->perf_events,
+                                               UVM_PERF_EVENT_FAULT,
+                                               uvm_tools_record_fault)) {
+      status = uvm_perf_register_event_callback_locked(
+          &va_space->perf_events, UVM_PERF_EVENT_FAULT, uvm_tools_record_fault);
 
-            if (status != NV_OK)
-                return status;
-        }
+      if (status != NV_OK)
+        return status;
     }
-    else {
-        if (uvm_perf_is_event_callback_registered(&va_space->perf_events, UVM_PERF_EVENT_FAULT, uvm_tools_record_fault)) {
-            uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
-                                                      UVM_PERF_EVENT_FAULT,
-                                                      uvm_tools_record_fault);
-        }
+  } else {
+    if (uvm_perf_is_event_callback_registered(&va_space->perf_events,
+                                              UVM_PERF_EVENT_FAULT,
+                                              uvm_tools_record_fault)) {
+      uvm_perf_unregister_event_callback_locked(
+          &va_space->perf_events, UVM_PERF_EVENT_FAULT, uvm_tools_record_fault);
     }
+  }
 
-    if (tools_is_migration_callback_needed(va_space)) {
-        if (!uvm_perf_is_event_callback_registered(&va_space->perf_events, UVM_PERF_EVENT_MIGRATION, uvm_tools_record_migration)) {
-            status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
-                                                             UVM_PERF_EVENT_MIGRATION,
-                                                             uvm_tools_record_migration);
+  if (tools_is_migration_callback_needed(va_space)) {
+    if (!uvm_perf_is_event_callback_registered(&va_space->perf_events,
+                                               UVM_PERF_EVENT_MIGRATION,
+                                               uvm_tools_record_migration)) {
+      status = uvm_perf_register_event_callback_locked(
+          &va_space->perf_events, UVM_PERF_EVENT_MIGRATION,
+          uvm_tools_record_migration);
 
-            if (status != NV_OK)
-                return status;
-        }
+      if (status != NV_OK)
+        return status;
     }
-    else {
-        if (uvm_perf_is_event_callback_registered(&va_space->perf_events, UVM_PERF_EVENT_MIGRATION, uvm_tools_record_migration)) {
-            uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
-                                                      UVM_PERF_EVENT_MIGRATION,
-                                                      uvm_tools_record_migration);
-        }
+  } else {
+    if (uvm_perf_is_event_callback_registered(&va_space->perf_events,
+                                              UVM_PERF_EVENT_MIGRATION,
+                                              uvm_tools_record_migration)) {
+      uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
+                                                UVM_PERF_EVENT_MIGRATION,
+                                                uvm_tools_record_migration);
     }
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static NV_STATUS tools_update_status(uvm_va_space_t *va_space)
-{
-    NV_STATUS status;
-    bool should_be_enabled;
-    uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
-    uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
-    uvm_assert_rwsem_locked_write(&va_space->tools.lock);
+static NV_STATUS tools_update_status(uvm_va_space_t *va_space) {
+  NV_STATUS status;
+  bool should_be_enabled;
+  uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
+  uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
+  uvm_assert_rwsem_locked_write(&va_space->tools.lock);
 
-    status = tools_update_perf_events_callbacks(va_space);
-    if (status != NV_OK)
-        return status;
+  status = tools_update_perf_events_callbacks(va_space);
+  if (status != NV_OK)
+    return status;
 
-    should_be_enabled = tools_are_enabled(va_space);
-    if (should_be_enabled != va_space->tools.enabled) {
-        if (should_be_enabled)
-            list_add(&va_space->tools.node, &g_tools_va_space_list);
-        else
-            list_del(&va_space->tools.node);
+  should_be_enabled = tools_are_enabled(va_space);
+  if (should_be_enabled != va_space->tools.enabled) {
+    if (should_be_enabled)
+      list_add(&va_space->tools.node, &g_tools_va_space_list);
+    else
+      list_del(&va_space->tools.node);
 
-        va_space->tools.enabled = should_be_enabled;
-    }
+    va_space->tools.enabled = should_be_enabled;
+  }
 
-    return NV_OK;
+  return NV_OK;
 }
 
 #define EVENT_FLAGS_BITS (sizeof(NvU64) * 8)
 
-static bool mask_contains_invalid_events(NvU64 event_flags)
-{
-    const unsigned long *event_mask = (const unsigned long *)&event_flags;
-    DECLARE_BITMAP(helper_mask, EVENT_FLAGS_BITS);
-    DECLARE_BITMAP(valid_events_mask, EVENT_FLAGS_BITS);
-    DECLARE_BITMAP(tests_events_mask, EVENT_FLAGS_BITS);
+static bool mask_contains_invalid_events(NvU64 event_flags) {
+  const unsigned long *event_mask = (const unsigned long *)&event_flags;
+  DECLARE_BITMAP(helper_mask, EVENT_FLAGS_BITS);
+  DECLARE_BITMAP(valid_events_mask, EVENT_FLAGS_BITS);
+  DECLARE_BITMAP(tests_events_mask, EVENT_FLAGS_BITS);
 
-    bitmap_zero(tests_events_mask, EVENT_FLAGS_BITS);
-    bitmap_set(tests_events_mask,
-               UvmEventTestTypesFirst,
-               UvmEventTestTypesLast - UvmEventTestTypesFirst + 1);
+  bitmap_zero(tests_events_mask, EVENT_FLAGS_BITS);
+  bitmap_set(tests_events_mask, UvmEventTestTypesFirst,
+             UvmEventTestTypesLast - UvmEventTestTypesFirst + 1);
 
-    bitmap_zero(valid_events_mask, EVENT_FLAGS_BITS);
-    bitmap_set(valid_events_mask, 1, UvmEventNumTypes - 1);
+  bitmap_zero(valid_events_mask, EVENT_FLAGS_BITS);
+  bitmap_set(valid_events_mask, 1, UvmEventNumTypes - 1);
 
-    if (uvm_enable_builtin_tests)
-        bitmap_or(valid_events_mask, valid_events_mask, tests_events_mask, EVENT_FLAGS_BITS);
+  if (uvm_enable_builtin_tests)
+    bitmap_or(valid_events_mask, valid_events_mask, tests_events_mask,
+              EVENT_FLAGS_BITS);
 
-    // Make sure that test event ids do not overlap with regular events
-    BUILD_BUG_ON(UvmEventTestTypesFirst < UvmEventNumTypes);
-    BUILD_BUG_ON(UvmEventTestTypesFirst > UvmEventTestTypesLast);
-    BUILD_BUG_ON(UvmEventTestTypesLast >= UvmEventNumTypesAll);
+  // Make sure that test event ids do not overlap with regular events
+  BUILD_BUG_ON(UvmEventTestTypesFirst < UvmEventNumTypes);
+  BUILD_BUG_ON(UvmEventTestTypesFirst > UvmEventTestTypesLast);
+  BUILD_BUG_ON(UvmEventTestTypesLast >= UvmEventNumTypesAll);
 
-    // Make sure that no test event ever changes the size of UvmEventEntry_V2
-    BUILD_BUG_ON(sizeof(((UvmEventEntry_V2 *)NULL)->testEventData) >
-                 sizeof(((UvmEventEntry_V2 *)NULL)->eventData));
-    BUILD_BUG_ON(UvmEventNumTypesAll > EVENT_FLAGS_BITS);
+  // Make sure that no test event ever changes the size of UvmEventEntry_V2
+  BUILD_BUG_ON(sizeof(((UvmEventEntry_V2 *)NULL)->testEventData) >
+               sizeof(((UvmEventEntry_V2 *)NULL)->eventData));
+  BUILD_BUG_ON(UvmEventNumTypesAll > EVENT_FLAGS_BITS);
 
-    if (!bitmap_andnot(helper_mask, event_mask, valid_events_mask, EVENT_FLAGS_BITS))
-        return false;
+  if (!bitmap_andnot(helper_mask, event_mask, valid_events_mask,
+                     EVENT_FLAGS_BITS))
+    return false;
 
-    if (!uvm_enable_builtin_tests && bitmap_and(helper_mask, event_mask, tests_events_mask, EVENT_FLAGS_BITS))
-        UVM_INFO_PRINT("Event index not found. Did you mean to insmod with uvm_enable_builtin_tests=1?\n");
+  if (!uvm_enable_builtin_tests &&
+      bitmap_and(helper_mask, event_mask, tests_events_mask, EVENT_FLAGS_BITS))
+    UVM_INFO_PRINT("Event index not found. Did you mean to insmod with "
+                   "uvm_enable_builtin_tests=1?\n");
 
-    return true;
+  return true;
 }
 
-NV_STATUS uvm_api_tools_event_queue_enable_events(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space;
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-    NV_STATUS status = NV_OK;
-    NvU64 inserted_lists;
+NV_STATUS uvm_api_tools_event_queue_enable_events(
+    UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS *params, struct file *filp) {
+  uvm_va_space_t *va_space;
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+  NV_STATUS status = NV_OK;
+  NvU64 inserted_lists;
 
-    if (!tracker_is_queue(event_tracker))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (!tracker_is_queue(event_tracker))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    if (mask_contains_invalid_events(params->eventTypeFlags))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (mask_contains_invalid_events(params->eventTypeFlags))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    va_space = tools_event_tracker_va_space(event_tracker);
+  va_space = tools_event_tracker_va_space(event_tracker);
 
-    uvm_down_write(&g_tools_va_space_list_lock);
-    uvm_down_write(&va_space->perf_events.lock);
-    uvm_down_write(&va_space->tools.lock);
+  uvm_down_write(&g_tools_va_space_list_lock);
+  uvm_down_write(&va_space->perf_events.lock);
+  uvm_down_write(&va_space->tools.lock);
 
-    insert_event_tracker(va_space,
-                         event_tracker->queue.queue_nodes,
-                         UvmEventNumTypesAll,
-                         params->eventTypeFlags,
-                         &event_tracker->queue.subscribed_queues,
-                         event_tracker->entry_size == sizeof(UvmEventEntry) ?
-                                                      va_space->tools.queues : va_space->tools.queues_v2,
-                         &inserted_lists);
+  insert_event_tracker(va_space, event_tracker->queue.queue_nodes,
+                       UvmEventNumTypesAll, params->eventTypeFlags,
+                       &event_tracker->queue.subscribed_queues,
+                       event_tracker->entry_size == sizeof(UvmEventEntry)
+                           ? va_space->tools.queues
+                           : va_space->tools.queues_v2,
+                       &inserted_lists);
 
-    // perform any necessary registration
-    status = tools_update_status(va_space);
-    if (status != NV_OK) {
-        // on error, unregister any newly registered event
-        remove_event_tracker(va_space,
-                             event_tracker->queue.queue_nodes,
-                             UvmEventNumTypes,
-                             inserted_lists,
-                             &event_tracker->queue.subscribed_queues);
-    }
+  // perform any necessary registration
+  status = tools_update_status(va_space);
+  if (status != NV_OK) {
+    // on error, unregister any newly registered event
+    remove_event_tracker(va_space, event_tracker->queue.queue_nodes,
+                         UvmEventNumTypes, inserted_lists,
+                         &event_tracker->queue.subscribed_queues);
+  }
 
-    uvm_up_write(&va_space->tools.lock);
-    uvm_up_write(&va_space->perf_events.lock);
-    uvm_up_write(&g_tools_va_space_list_lock);
+  uvm_up_write(&va_space->tools.lock);
+  uvm_up_write(&va_space->perf_events.lock);
+  uvm_up_write(&g_tools_va_space_list_lock);
 
-    return status;
+  return status;
 }
 
-NV_STATUS uvm_api_tools_event_queue_disable_events(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS *params, struct file *filp)
-{
-    NV_STATUS status;
-    uvm_va_space_t *va_space;
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+NV_STATUS uvm_api_tools_event_queue_disable_events(
+    UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS *params, struct file *filp) {
+  NV_STATUS status;
+  uvm_va_space_t *va_space;
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
 
-    if (!tracker_is_queue(event_tracker))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (!tracker_is_queue(event_tracker))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    va_space = tools_event_tracker_va_space(event_tracker);
+  va_space = tools_event_tracker_va_space(event_tracker);
 
-    uvm_down_write(&g_tools_va_space_list_lock);
-    uvm_down_write(&va_space->perf_events.lock);
-    uvm_down_write(&va_space->tools.lock);
-    remove_event_tracker(va_space,
-                         event_tracker->queue.queue_nodes,
-                         UvmEventNumTypesAll,
-                         params->eventTypeFlags,
-                         &event_tracker->queue.subscribed_queues);
+  uvm_down_write(&g_tools_va_space_list_lock);
+  uvm_down_write(&va_space->perf_events.lock);
+  uvm_down_write(&va_space->tools.lock);
+  remove_event_tracker(va_space, event_tracker->queue.queue_nodes,
+                       UvmEventNumTypesAll, params->eventTypeFlags,
+                       &event_tracker->queue.subscribed_queues);
 
-    // de-registration should not fail
-    status = tools_update_status(va_space);
-    UVM_ASSERT(status == NV_OK);
+  // de-registration should not fail
+  status = tools_update_status(va_space);
+  UVM_ASSERT(status == NV_OK);
 
-    uvm_up_write(&va_space->tools.lock);
-    uvm_up_write(&va_space->perf_events.lock);
-    uvm_up_write(&g_tools_va_space_list_lock);
-    return NV_OK;
+  uvm_up_write(&va_space->tools.lock);
+  uvm_up_write(&va_space->perf_events.lock);
+  uvm_up_write(&g_tools_va_space_list_lock);
+  return NV_OK;
 }
 
-NV_STATUS uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *params, struct file *filp)
-{
-    uvm_va_space_t *va_space;
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-    NV_STATUS status = NV_OK;
-    NvU64 inserted_lists;
+NV_STATUS
+uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *params,
+                              struct file *filp) {
+  uvm_va_space_t *va_space;
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+  NV_STATUS status = NV_OK;
+  NvU64 inserted_lists;
 
-    if (!tracker_is_counter(event_tracker))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (!tracker_is_counter(event_tracker))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    va_space = tools_event_tracker_va_space(event_tracker);
+  va_space = tools_event_tracker_va_space(event_tracker);
 
-    uvm_down_write(&g_tools_va_space_list_lock);
-    uvm_down_write(&va_space->perf_events.lock);
-    uvm_down_write(&va_space->tools.lock);
+  uvm_down_write(&g_tools_va_space_list_lock);
+  uvm_down_write(&va_space->perf_events.lock);
+  uvm_down_write(&va_space->tools.lock);
 
-    insert_event_tracker(va_space,
-                         event_tracker->counter.counter_nodes,
-                         UVM_TOTAL_COUNTERS,
-                         params->counterTypeFlags,
-                         &event_tracker->counter.subscribed_counters,
-                         va_space->tools.counters,
-                         &inserted_lists);
+  insert_event_tracker(va_space, event_tracker->counter.counter_nodes,
+                       UVM_TOTAL_COUNTERS, params->counterTypeFlags,
+                       &event_tracker->counter.subscribed_counters,
+                       va_space->tools.counters, &inserted_lists);
 
-    // perform any necessary registration
-    status = tools_update_status(va_space);
-    if (status != NV_OK) {
-        remove_event_tracker(va_space,
-                             event_tracker->counter.counter_nodes,
-                             UVM_TOTAL_COUNTERS,
-                             inserted_lists,
-                             &event_tracker->counter.subscribed_counters);
-    }
+  // perform any necessary registration
+  status = tools_update_status(va_space);
+  if (status != NV_OK) {
+    remove_event_tracker(va_space, event_tracker->counter.counter_nodes,
+                         UVM_TOTAL_COUNTERS, inserted_lists,
+                         &event_tracker->counter.subscribed_counters);
+  }
 
-    uvm_up_write(&va_space->tools.lock);
-    uvm_up_write(&va_space->perf_events.lock);
-    uvm_up_write(&g_tools_va_space_list_lock);
+  uvm_up_write(&va_space->tools.lock);
+  uvm_up_write(&va_space->perf_events.lock);
+  uvm_up_write(&g_tools_va_space_list_lock);
 
-    return status;
+  return status;
 }
 
-NV_STATUS uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *params, struct file *filp)
-{
-    NV_STATUS status;
-    uvm_va_space_t *va_space;
-    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+NV_STATUS
+uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *params,
+                               struct file *filp) {
+  NV_STATUS status;
+  uvm_va_space_t *va_space;
+  uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
 
-    if (!tracker_is_counter(event_tracker))
-        return NV_ERR_INVALID_ARGUMENT;
+  if (!tracker_is_counter(event_tracker))
+    return NV_ERR_INVALID_ARGUMENT;
 
-    va_space = tools_event_tracker_va_space(event_tracker);
+  va_space = tools_event_tracker_va_space(event_tracker);
 
-    uvm_down_write(&g_tools_va_space_list_lock);
-    uvm_down_write(&va_space->perf_events.lock);
-    uvm_down_write(&va_space->tools.lock);
-    remove_event_tracker(va_space,
-                         event_tracker->counter.counter_nodes,
-                         UVM_TOTAL_COUNTERS,
-                         params->counterTypeFlags,
-                         &event_tracker->counter.subscribed_counters);
+  uvm_down_write(&g_tools_va_space_list_lock);
+  uvm_down_write(&va_space->perf_events.lock);
+  uvm_down_write(&va_space->tools.lock);
+  remove_event_tracker(va_space, event_tracker->counter.counter_nodes,
+                       UVM_TOTAL_COUNTERS, params->counterTypeFlags,
+                       &event_tracker->counter.subscribed_counters);
 
-    // de-registration should not fail
-    status = tools_update_status(va_space);
-    UVM_ASSERT(status == NV_OK);
+  // de-registration should not fail
+  status = tools_update_status(va_space);
+  UVM_ASSERT(status == NV_OK);
 
-    uvm_up_write(&va_space->tools.lock);
-    uvm_up_write(&va_space->perf_events.lock);
-    uvm_up_write(&g_tools_va_space_list_lock);
+  uvm_up_write(&va_space->tools.lock);
+  uvm_up_write(&va_space->perf_events.lock);
+  uvm_up_write(&g_tools_va_space_list_lock);
 
-    return NV_OK;
+  return NV_OK;
 }
 
 static NV_STATUS tools_access_va_block(uvm_va_block_t *va_block,
                                        uvm_va_block_context_t *block_context,
-                                       NvU64 target_va,
-                                       NvU64 size,
-                                       bool is_write,
-                                       uvm_mem_t *stage_mem)
-{
+                                       NvU64 target_va, NvU64 size,
+                                       bool is_write, uvm_mem_t *stage_mem) {
+  if (is_write) {
+    return UVM_VA_BLOCK_LOCK_RETRY(
+        va_block, NULL,
+        uvm_va_block_write_from_cpu(va_block, block_context, target_va,
+                                    stage_mem, size));
+  } else {
+    return UVM_VA_BLOCK_LOCK_RETRY(
+        va_block, NULL,
+        uvm_va_block_read_to_cpu(va_block, block_context, stage_mem, target_va,
+                                 size));
+  }
+}
+
+static NV_STATUS tools_access_process_memory(uvm_va_space_t *va_space,
+                                             NvU64 target_va, NvU64 size,
+                                             NvU64 user_va, NvU64 *bytes,
+                                             bool is_write) {
+  NV_STATUS status;
+  uvm_mem_t *stage_mem = NULL;
+  void *stage_addr;
+  uvm_processor_mask_t *retained_gpus = NULL;
+  uvm_va_block_context_t *block_context = NULL;
+  struct mm_struct *mm = NULL;
+
+  retained_gpus = uvm_processor_mask_cache_alloc();
+  if (!retained_gpus)
+    return NV_ERR_NO_MEMORY;
+
+  uvm_processor_mask_zero(retained_gpus);
+
+  mm = uvm_va_space_mm_or_current_retain(va_space);
+
+  status = uvm_mem_alloc_sysmem_and_map_cpu_kernel(PAGE_SIZE, mm, &stage_mem);
+  if (status != NV_OK)
+    goto exit;
+
+  block_context = uvm_va_block_context_alloc(mm);
+  if (!block_context) {
+    status = NV_ERR_NO_MEMORY;
+    goto exit;
+  }
+
+  stage_addr = uvm_mem_get_cpu_addr_kernel(stage_mem);
+  *bytes = 0;
+
+  while (*bytes < size) {
+    uvm_gpu_t *gpu;
+    uvm_va_block_t *block;
+    void *user_va_start = (void *)(user_va + *bytes);
+    NvU64 target_va_start = target_va + *bytes;
+    NvU64 bytes_left = size - *bytes;
+    NvU64 page_offset = target_va_start & (PAGE_SIZE - 1);
+    NvU64 bytes_now = min(bytes_left, (NvU64)(PAGE_SIZE - page_offset));
+
     if (is_write) {
-        return UVM_VA_BLOCK_LOCK_RETRY(va_block,
-                                       NULL,
-                                       uvm_va_block_write_from_cpu(va_block, block_context, target_va, stage_mem, size));
+      NvU64 remaining = copy_from_user(stage_addr, user_va_start, bytes_now);
+      if (remaining != 0) {
+        status = NV_ERR_INVALID_ARGUMENT;
+        goto exit;
+      }
     }
-    else {
-        return UVM_VA_BLOCK_LOCK_RETRY(va_block,
-                                       NULL,
-                                       uvm_va_block_read_to_cpu(va_block, block_context, stage_mem, target_va, size));
 
+    if (mm)
+      uvm_down_read_mmap_lock(mm);
+
+    uvm_va_space_down_read(va_space);
+
+    if (mm)
+      status = uvm_va_block_find_create(va_space,
+                                        UVM_PAGE_ALIGN_DOWN(target_va_start),
+                                        &block_context->hmm.vma, &block);
+    else
+      status = uvm_va_block_find_create_managed(
+          va_space, UVM_PAGE_ALIGN_DOWN(target_va_start), &block);
+
+    if (status != NV_OK)
+      goto unlock_and_exit;
+
+    for_each_gpu_in_mask(gpu, &va_space->registered_gpus) {
+      if (uvm_processor_mask_test_and_set(retained_gpus, gpu->id))
+        continue;
+
+      // The retention of each GPU ensures that the staging memory is
+      // freed before the unregistration of any of the GPUs is mapped
+      // on. Each GPU is retained once.
+      uvm_gpu_retain(gpu);
+
+      // In Confidential Computing, the staging memory cannot be mapped on
+      // the GPU (it is protected sysmem), but it is still used to store
+      // the unencrypted version of the page contents when the page is
+      // resident on vidmem.
+      if (g_uvm_global.conf_computing_enabled)
+        continue;
+
+      // Accessing the VA block may result in copying data between the
+      // CPU and a GPU. Conservatively add virtual mappings to all the
+      // GPUs (even if those mappings may never be used) as tools
+      // read/write is not on a performance critical path.
+      status = uvm_mem_map_gpu_kernel(stage_mem, gpu);
+      if (status != NV_OK)
+        goto unlock_and_exit;
     }
-}
 
-static NV_STATUS tools_access_process_memory(uvm_va_space_t *va_space,
-                                             NvU64 target_va,
-                                             NvU64 size,
-                                             NvU64 user_va,
-                                             NvU64 *bytes,
-                                             bool is_write)
-{
-    NV_STATUS status;
-    uvm_mem_t *stage_mem = NULL;
-    void *stage_addr;
-    uvm_processor_mask_t *retained_gpus = NULL;
-    uvm_va_block_context_t *block_context = NULL;
-    struct mm_struct *mm = NULL;
+    // Make sure a CPU resident page has an up to date struct page pointer.
+    if (uvm_va_block_is_hmm(block)) {
+      status = uvm_hmm_va_block_update_residency_info(
+          block, mm, UVM_PAGE_ALIGN_DOWN(target_va_start), true);
+      if (status != NV_OK)
+        goto unlock_and_exit;
+    }
 
-    retained_gpus = uvm_processor_mask_cache_alloc();
-    if (!retained_gpus)
-        return NV_ERR_NO_MEMORY;
+    status = tools_access_va_block(block, block_context, target_va_start,
+                                   bytes_now, is_write, stage_mem);
 
-    uvm_processor_mask_zero(retained_gpus);
+    uvm_va_space_up_read(va_space);
+    if (mm)
+      uvm_up_read_mmap_lock(mm);
 
-    mm = uvm_va_space_mm_or_current_retain(va_space);
+    // Check for ECC errors on all retained GPUs, even if they are no longer
+    // registered in the VA space.
+    if (status == NV_OK)
+      status = uvm_global_gpu_check_ecc_error(retained_gpus);
 
-    status = uvm_mem_alloc_sysmem_and_map_cpu_kernel(PAGE_SIZE, mm, &stage_mem);
     if (status != NV_OK)
+      goto exit;
+
+    if (!is_write) {
+      NvU64 remaining;
+
+      // Prevent processor speculation prior to accessing user-mapped
+      // memory to avoid leaking information from side-channel attacks.
+      // Under speculation, a valid VA range which does not contain
+      // target_va could be used, and the block index could run off the
+      // end of the array. Information about the state of that kernel
+      // memory could be inferred if speculative execution gets to the
+      // point where the data is copied out.
+      nv_speculation_barrier();
+
+      remaining = copy_to_user(user_va_start, stage_addr, bytes_now);
+      if (remaining > 0) {
+        status = NV_ERR_INVALID_ARGUMENT;
         goto exit;
-
-    block_context = uvm_va_block_context_alloc(mm);
-    if (!block_context) {
-        status = NV_ERR_NO_MEMORY;
-        goto exit;
+      }
     }
 
-    stage_addr = uvm_mem_get_cpu_addr_kernel(stage_mem);
-    *bytes = 0;
-
-    while (*bytes < size) {
-        uvm_gpu_t *gpu;
-        uvm_va_block_t *block;
-        void *user_va_start = (void *) (user_va + *bytes);
-        NvU64 target_va_start = target_va + *bytes;
-        NvU64 bytes_left = size - *bytes;
-        NvU64 page_offset = target_va_start & (PAGE_SIZE - 1);
-        NvU64 bytes_now = min(bytes_left, (NvU64)(PAGE_SIZE - page_offset));
-
-        if (is_write) {
-            NvU64 remaining = copy_from_user(stage_addr, user_va_start, bytes_now);
-            if (remaining != 0)  {
-                status = NV_ERR_INVALID_ARGUMENT;
-                goto exit;
-            }
-        }
-
-        if (mm)
-            uvm_down_read_mmap_lock(mm);
-
-        uvm_va_space_down_read(va_space);
-
-        if (mm)
-            status = uvm_va_block_find_create(va_space, UVM_PAGE_ALIGN_DOWN(target_va_start), &block_context->hmm.vma, &block);
-        else
-            status = uvm_va_block_find_create_managed(va_space, UVM_PAGE_ALIGN_DOWN(target_va_start), &block);
-
-        if (status != NV_OK)
-            goto unlock_and_exit;
-
-        for_each_gpu_in_mask(gpu, &va_space->registered_gpus) {
-            if (uvm_processor_mask_test_and_set(retained_gpus, gpu->id))
-                continue;
-
-            // The retention of each GPU ensures that the staging memory is
-            // freed before the unregistration of any of the GPUs is mapped
-            // on. Each GPU is retained once.
-            uvm_gpu_retain(gpu);
-
-            // In Confidential Computing, the staging memory cannot be mapped on
-            // the GPU (it is protected sysmem), but it is still used to store
-            // the unencrypted version of the page contents when the page is
-            // resident on vidmem.
-            if (g_uvm_global.conf_computing_enabled)
-                continue;
-
-            // Accessing the VA block may result in copying data between the
-            // CPU and a GPU. Conservatively add virtual mappings to all the
-            // GPUs (even if those mappings may never be used) as tools
-            // read/write is not on a performance critical path.
-            status = uvm_mem_map_gpu_kernel(stage_mem, gpu);
-            if (status != NV_OK)
-                goto unlock_and_exit;
-        }
-
-        // Make sure a CPU resident page has an up to date struct page pointer.
-        if (uvm_va_block_is_hmm(block)) {
-            status = uvm_hmm_va_block_update_residency_info(block, mm, UVM_PAGE_ALIGN_DOWN(target_va_start), true);
-            if (status != NV_OK)
-                goto unlock_and_exit;
-        }
-
-        status = tools_access_va_block(block, block_context, target_va_start, bytes_now, is_write, stage_mem);
-
-        uvm_va_space_up_read(va_space);
-        if (mm)
-            uvm_up_read_mmap_lock(mm);
-
-        // Check for ECC errors on all retained GPUs, even if they are no longer
-        // registered in the VA space.
-        if (status == NV_OK)
-            status = uvm_global_gpu_check_ecc_error(retained_gpus);
-
-        if (status != NV_OK)
-            goto exit;
-
-        if (!is_write) {
-            NvU64 remaining;
-
-            // Prevent processor speculation prior to accessing user-mapped
-            // memory to avoid leaking information from side-channel attacks.
-            // Under speculation, a valid VA range which does not contain
-            // target_va could be used, and the block index could run off the
-            // end of the array. Information about the state of that kernel
-            // memory could be inferred if speculative execution gets to the
-            // point where the data is copied out.
-            nv_speculation_barrier();
-
-            remaining = copy_to_user(user_va_start, stage_addr, bytes_now);
-            if (remaining > 0) {
-                status = NV_ERR_INVALID_ARGUMENT;
-                goto exit;
-            }
-        }
-
-        *bytes += bytes_now;
-    }
+    *bytes += bytes_now;
+  }
 
 unlock_and_exit:
-    if (status != NV_OK) {
-        uvm_va_space_up_read(va_space);
-        if (mm)
-            uvm_up_read_mmap_lock(mm);
-    }
+  if (status != NV_OK) {
+    uvm_va_space_up_read(va_space);
+    if (mm)
+      uvm_up_read_mmap_lock(mm);
+  }
 
 exit:
-    uvm_va_block_context_free(block_context);
+  uvm_va_block_context_free(block_context);
 
-    uvm_mem_free(stage_mem);
+  uvm_mem_free(stage_mem);
 
-    uvm_global_gpu_release(retained_gpus);
+  uvm_global_gpu_release(retained_gpus);
 
-    uvm_va_space_mm_or_current_release(va_space, mm);
+  uvm_va_space_mm_or_current_release(va_space, mm);
 
-    uvm_processor_mask_cache_free(retained_gpus);
+  uvm_processor_mask_cache_free(retained_gpus);
 
-    return status;
+  return status;
 }
 
-NV_STATUS uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params, struct file *filp)
-{
-    return tools_access_process_memory(uvm_va_space_get(filp),
-                                       params->targetVa,
-                                       params->size,
-                                       params->buffer,
-                                       &params->bytesRead,
-                                       false);
+NV_STATUS
+uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params,
+                                  struct file *filp) {
+  return tools_access_process_memory(uvm_va_space_get(filp), params->targetVa,
+                                     params->size, params->buffer,
+                                     &params->bytesRead, false);
 }
 
-NV_STATUS uvm_api_tools_write_process_memory(UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp)
-{
-    return tools_access_process_memory(uvm_va_space_get(filp),
-                                       params->targetVa,
-                                       params->size,
-                                       params->buffer,
-                                       &params->bytesWritten,
-                                       true);
+NV_STATUS uvm_api_tools_write_process_memory(
+    UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp) {
+  return tools_access_process_memory(uvm_va_space_get(filp), params->targetVa,
+                                     params->size, params->buffer,
+                                     &params->bytesWritten, true);
 }
 
-NV_STATUS uvm_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *params, struct file *filp)
-{
-    NvU32 i;
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+NV_STATUS
+uvm_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *params,
+                            struct file *filp) {
+  NvU32 i;
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-    if (params->entry.eventData.eventType >= UvmEventNumTypesAll)
-        return NV_ERR_INVALID_ARGUMENT;
+  if (params->entry.eventData.eventType >= UvmEventNumTypesAll)
+    return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_read(&va_space->tools.lock);
+  uvm_down_read(&va_space->tools.lock);
 
-    for (i = 0; i < params->count; i++)
-        uvm_tools_record_event(va_space, &params->entry);
+  for (i = 0; i < params->count; i++)
+    uvm_tools_record_event(va_space, &params->entry);
 
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 
-    return NV_OK;
+  return NV_OK;
 }
 
-NV_STATUS uvm_test_inject_tools_event_v2(UVM_TEST_INJECT_TOOLS_EVENT_V2_PARAMS *params, struct file *filp)
-{
-    NvU32 i;
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+NV_STATUS
+uvm_test_inject_tools_event_v2(UVM_TEST_INJECT_TOOLS_EVENT_V2_PARAMS *params,
+                               struct file *filp) {
+  NvU32 i;
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-    if (params->entry_v2.eventData.eventType >= UvmEventNumTypesAll)
-        return NV_ERR_INVALID_ARGUMENT;
+  if (params->entry_v2.eventData.eventType >= UvmEventNumTypesAll)
+    return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_read(&va_space->tools.lock);
+  uvm_down_read(&va_space->tools.lock);
 
-    for (i = 0; i < params->count; i++)
-        uvm_tools_record_event_v2(va_space, &params->entry_v2);
+  for (i = 0; i < params->count; i++)
+    uvm_tools_record_event_v2(va_space, &params->entry_v2);
 
-    uvm_up_read(&va_space->tools.lock);
+  uvm_up_read(&va_space->tools.lock);
 
-    return NV_OK;
+  return NV_OK;
 }
 
-NV_STATUS uvm_test_increment_tools_counter(UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *params, struct file *filp)
-{
-    NvU32 i;
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+NV_STATUS uvm_test_increment_tools_counter(
+    UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *params, struct file *filp) {
+  NvU32 i;
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-    if (params->counter >= UVM_TOTAL_COUNTERS)
-        return NV_ERR_INVALID_ARGUMENT;
+  if (params->counter >= UVM_TOTAL_COUNTERS)
+    return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_read(&va_space->tools.lock);
-    for (i = 0; i < params->count; i++)
-        uvm_tools_inc_counter(va_space, params->counter, params->amount, &params->processor);
-    uvm_up_read(&va_space->tools.lock);
+  uvm_down_read(&va_space->tools.lock);
+  for (i = 0; i < params->count; i++)
+    uvm_tools_inc_counter(va_space, params->counter, params->amount,
+                          &params->processor);
+  uvm_up_read(&va_space->tools.lock);
 
-    return NV_OK;
+  return NV_OK;
 }
 
-static NV_STATUS uvm_tools_get_processor_uuid_table_common(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params,
-                                                           uvm_va_space_t *va_space,
-                                                           NvU32 max_processors_count)
-{
-    NvProcessorUuid *uuids;
-    NvU64 remaining;
-    uvm_gpu_t *gpu;
+static NV_STATUS uvm_tools_get_processor_uuid_table_common(
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params,
+    uvm_va_space_t *va_space, NvU32 max_processors_count) {
+  NvProcessorUuid *uuids;
+  NvU64 remaining;
+  uvm_gpu_t *gpu;
 
-    uuids = uvm_kvmalloc_zero(sizeof(NvProcessorUuid) * max_processors_count);
-    if (uuids == NULL)
-        return NV_ERR_NO_MEMORY;
+  uuids = uvm_kvmalloc_zero(sizeof(NvProcessorUuid) * max_processors_count);
+  if (uuids == NULL)
+    return NV_ERR_NO_MEMORY;
 
-    uvm_uuid_copy(&uuids[UVM_ID_CPU_VALUE], &NV_PROCESSOR_UUID_CPU_DEFAULT);
+  uvm_uuid_copy(&uuids[UVM_ID_CPU_VALUE], &NV_PROCESSOR_UUID_CPU_DEFAULT);
 
-    uvm_va_space_down_read(va_space);
+  uvm_va_space_down_read(va_space);
+
+  for_each_va_space_gpu(gpu, va_space) {
+    NvU32 id_value;
+    const NvProcessorUuid *uuid;
 
-    for_each_va_space_gpu(gpu, va_space) {
-        NvU32 id_value;
-        const NvProcessorUuid *uuid;
-
-        // Version 1 only supports processors 0..32 and uses the parent
-        // GPU UUID.
-        if (max_processors_count == UVM_MAX_PROCESSORS_V1) {
-            id_value = uvm_parent_id_value(gpu->parent->id);
-            uuid = &gpu->parent->uuid;
-        }
-        else {
-            id_value = uvm_id_value(gpu->id);
-            uuid = &gpu->uuid;
-        }
-
-        uvm_uuid_copy(&uuids[id_value], uuid);
+    // Version 1 only supports processors 0..32 and uses the parent
+    // GPU UUID.
+    if (max_processors_count == UVM_MAX_PROCESSORS_V1) {
+      id_value = uvm_parent_id_value(gpu->parent->id);
+      uuid = &gpu->parent->uuid;
+    } else {
+      id_value = uvm_id_value(gpu->id);
+      uuid = &gpu->uuid;
     }
 
-    uvm_va_space_up_read(va_space);
+    uvm_uuid_copy(&uuids[id_value], uuid);
+  }
+
+  uvm_va_space_up_read(va_space);
 
-    remaining = copy_to_user((void *)params->tablePtr, uuids, sizeof(NvProcessorUuid) * max_processors_count);
+  remaining = copy_to_user((void *)params->tablePtr, uuids,
+                           sizeof(NvProcessorUuid) * max_processors_count);
 
-    uvm_kvfree(uuids);
+  uvm_kvfree(uuids);
 
-    if (remaining != 0)
-        return NV_ERR_INVALID_ADDRESS;
+  if (remaining != 0)
+    return NV_ERR_INVALID_ADDRESS;
 
-    return NV_OK;
+  return NV_OK;
 }
 
-NV_STATUS uvm_api_tools_get_processor_uuid_table(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS *params, struct file *filp)
-{
-    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params_v2 = (UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *) params;
+NV_STATUS uvm_api_tools_get_processor_uuid_table(
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS *params, struct file *filp) {
+  UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params_v2 =
+      (UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *)params;
 
-    BUILD_BUG_ON(!__same_type(params, params_v2));
+  BUILD_BUG_ON(!__same_type(params, params_v2));
 
-    return uvm_tools_get_processor_uuid_table_common(params_v2, uvm_va_space_get(filp), UVM_MAX_PROCESSORS_V1);
+  return uvm_tools_get_processor_uuid_table_common(
+      params_v2, uvm_va_space_get(filp), UVM_MAX_PROCESSORS_V1);
 }
 
-NV_STATUS uvm_api_tools_get_processor_uuid_table_v2(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params,
-                                                    struct file *filp)
-{
-    return uvm_tools_get_processor_uuid_table_common(params, uvm_va_space_get(filp), UVM_ID_MAX_PROCESSORS);
+NV_STATUS uvm_api_tools_get_processor_uuid_table_v2(
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params, struct file *filp) {
+  return uvm_tools_get_processor_uuid_table_common(
+      params, uvm_va_space_get(filp), UVM_ID_MAX_PROCESSORS);
 }
 
-void uvm_tools_flush_events(void)
-{
-    tools_schedule_completed_events();
+void uvm_tools_flush_events(void) {
+  tools_schedule_completed_events();
 
-    nv_kthread_q_flush(&g_tools_queue);
+  nv_kthread_q_flush(&g_tools_queue);
 }
 
-NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS *params, struct file *filp)
-{
-    uvm_tools_flush_events();
-    return NV_OK;
+NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS *params,
+                                     struct file *filp) {
+  uvm_tools_flush_events();
+  return NV_OK;
 }
 
-NV_STATUS uvm_test_tools_flush_replay_events(UVM_TEST_TOOLS_FLUSH_REPLAY_EVENTS_PARAMS *params, struct file *filp)
-{
-    NV_STATUS status = NV_OK;
-    uvm_gpu_t *gpu = NULL;
-    uvm_va_space_t *va_space = uvm_va_space_get(filp);
+NV_STATUS uvm_test_tools_flush_replay_events(
+    UVM_TEST_TOOLS_FLUSH_REPLAY_EVENTS_PARAMS *params, struct file *filp) {
+  NV_STATUS status = NV_OK;
+  uvm_gpu_t *gpu = NULL;
+  uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-    gpu = uvm_va_space_retain_gpu_by_uuid(va_space, &params->gpuUuid);
-    if (!gpu)
-        return NV_ERR_INVALID_DEVICE;
+  gpu = uvm_va_space_retain_gpu_by_uuid(va_space, &params->gpuUuid);
+  if (!gpu)
+    return NV_ERR_INVALID_DEVICE;
 
-    // Wait for register-based fault clears to queue the replay event
-    if (!gpu->parent->has_clear_faulted_channel_method) {
-        uvm_parent_gpu_non_replayable_faults_isr_lock(gpu->parent);
-        uvm_parent_gpu_non_replayable_faults_isr_unlock(gpu->parent);
-    }
+  // Wait for register-based fault clears to queue the replay event
+  if (!gpu->parent->has_clear_faulted_channel_method) {
+    uvm_parent_gpu_non_replayable_faults_isr_lock(gpu->parent);
+    uvm_parent_gpu_non_replayable_faults_isr_unlock(gpu->parent);
+  }
 
-    // Wait for pending fault replay methods to complete (replayable faults on
-    // all GPUs, and non-replayable faults on method-based GPUs).
-    status = uvm_channel_manager_wait(gpu->channel_manager);
+  // Wait for pending fault replay methods to complete (replayable faults on
+  // all GPUs, and non-replayable faults on method-based GPUs).
+  status = uvm_channel_manager_wait(gpu->channel_manager);
 
-    // Flush any pending events even if (status != NV_OK)
-    uvm_tools_flush_events();
-    uvm_gpu_release(gpu);
+  // Flush any pending events even if (status != NV_OK)
+  uvm_tools_flush_events();
+  uvm_gpu_release(gpu);
 
-    return status;
+  return status;
 }
 
-static const struct file_operations uvm_tools_fops =
-{
-    .open            = uvm_tools_open_entry,
-    .release         = uvm_tools_release_entry,
-    .unlocked_ioctl  = uvm_tools_unlocked_ioctl_entry,
+static const struct file_operations uvm_tools_fops = {
+    .open = uvm_tools_open_entry,
+    .release = uvm_tools_release_entry,
+    .unlocked_ioctl = uvm_tools_unlocked_ioctl_entry,
 #if NVCPU_IS_X86_64
-    .compat_ioctl    = uvm_tools_unlocked_ioctl_entry,
+    .compat_ioctl = uvm_tools_unlocked_ioctl_entry,
 #endif
-    .poll            = uvm_tools_poll_entry,
-    .owner           = THIS_MODULE,
+    .poll = uvm_tools_poll_entry,
+    .owner = THIS_MODULE,
 };
 
-static void _uvm_tools_destroy_cache_all(void)
-{
-    // The pointers are initialized to NULL,
-    // it's safe to call destroy on all of them.
-    kmem_cache_destroy_safe(&g_tools_event_tracker_cache);
-    kmem_cache_destroy_safe(&g_tools_block_migration_data_cache);
-    kmem_cache_destroy_safe(&g_tools_migration_data_cache);
-    kmem_cache_destroy_safe(&g_tools_replay_data_cache);
-    kmem_cache_destroy_safe(&g_tools_block_map_remote_data_cache);
-    kmem_cache_destroy_safe(&g_tools_map_remote_data_cache);
-}
-
-int uvm_tools_init(dev_t uvm_base_dev)
-{
-    dev_t uvm_tools_dev = MKDEV(MAJOR(uvm_base_dev), NVIDIA_UVM_TOOLS_MINOR_NUMBER);
-    int ret = -ENOMEM; // This will be updated later if allocations succeed
-
-    uvm_init_rwsem(&g_tools_va_space_list_lock, UVM_LOCK_ORDER_TOOLS_VA_SPACE_LIST);
-
-    g_tools_event_tracker_cache = NV_KMEM_CACHE_CREATE("uvm_tools_event_tracker_t",
-                                                        uvm_tools_event_tracker_t);
-    if (!g_tools_event_tracker_cache)
-        goto err_cache_destroy;
-
-    g_tools_block_migration_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_block_migration_data_t",
-                                                              block_migration_data_t);
-    if (!g_tools_block_migration_data_cache)
-        goto err_cache_destroy;
-
-    g_tools_migration_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_migration_data_t",
-                                                        migration_data_t);
-    if (!g_tools_migration_data_cache)
-        goto err_cache_destroy;
-
-    g_tools_replay_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_replay_data_t",
-                                                     replay_data_t);
-    if (!g_tools_replay_data_cache)
-        goto err_cache_destroy;
-
-    g_tools_block_map_remote_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_block_map_remote_data_t",
-                                                               block_map_remote_data_t);
-    if (!g_tools_block_map_remote_data_cache)
-        goto err_cache_destroy;
-
-    g_tools_map_remote_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_map_remote_data_t",
-                                                         map_remote_data_t);
-    if (!g_tools_map_remote_data_cache)
-        goto err_cache_destroy;
-
-    uvm_spin_lock_init(&g_tools_channel_list_lock, UVM_LOCK_ORDER_LEAF);
-
-    ret = nv_kthread_q_init(&g_tools_queue, "UVM Tools Event Queue");
-    if (ret < 0)
-        goto err_cache_destroy;
-
-    uvm_init_character_device(&g_uvm_tools_cdev, &uvm_tools_fops);
-    ret = cdev_add(&g_uvm_tools_cdev, uvm_tools_dev, 1);
-    if (ret != 0) {
-        UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n", MAJOR(uvm_tools_dev),
-                      MINOR(uvm_tools_dev), ret);
-        goto err_stop_thread;
-    }
-
-    return ret;
+static void _uvm_tools_destroy_cache_all(void) {
+  // The pointers are initialized to NULL,
+  // it's safe to call destroy on all of them.
+  kmem_cache_destroy_safe(&g_tools_event_tracker_cache);
+  kmem_cache_destroy_safe(&g_tools_block_migration_data_cache);
+  kmem_cache_destroy_safe(&g_tools_migration_data_cache);
+  kmem_cache_destroy_safe(&g_tools_replay_data_cache);
+  kmem_cache_destroy_safe(&g_tools_block_map_remote_data_cache);
+  kmem_cache_destroy_safe(&g_tools_map_remote_data_cache);
+}
+
+int uvm_tools_init(dev_t uvm_base_dev) {
+  dev_t uvm_tools_dev =
+      MKDEV(MAJOR(uvm_base_dev), NVIDIA_UVM_TOOLS_MINOR_NUMBER);
+  int ret = -ENOMEM; // This will be updated later if allocations succeed
+
+  uvm_init_rwsem(&g_tools_va_space_list_lock,
+                 UVM_LOCK_ORDER_TOOLS_VA_SPACE_LIST);
+
+  g_tools_event_tracker_cache = NV_KMEM_CACHE_CREATE(
+      "uvm_tools_event_tracker_t", uvm_tools_event_tracker_t);
+  if (!g_tools_event_tracker_cache)
+    goto err_cache_destroy;
+
+  g_tools_block_migration_data_cache = NV_KMEM_CACHE_CREATE(
+      "uvm_tools_block_migration_data_t", block_migration_data_t);
+  if (!g_tools_block_migration_data_cache)
+    goto err_cache_destroy;
+
+  g_tools_migration_data_cache =
+      NV_KMEM_CACHE_CREATE("uvm_tools_migration_data_t", migration_data_t);
+  if (!g_tools_migration_data_cache)
+    goto err_cache_destroy;
+
+  g_tools_replay_data_cache =
+      NV_KMEM_CACHE_CREATE("uvm_tools_replay_data_t", replay_data_t);
+  if (!g_tools_replay_data_cache)
+    goto err_cache_destroy;
+
+  g_tools_block_map_remote_data_cache = NV_KMEM_CACHE_CREATE(
+      "uvm_tools_block_map_remote_data_t", block_map_remote_data_t);
+  if (!g_tools_block_map_remote_data_cache)
+    goto err_cache_destroy;
+
+  g_tools_map_remote_data_cache =
+      NV_KMEM_CACHE_CREATE("uvm_tools_map_remote_data_t", map_remote_data_t);
+  if (!g_tools_map_remote_data_cache)
+    goto err_cache_destroy;
+
+  uvm_spin_lock_init(&g_tools_channel_list_lock, UVM_LOCK_ORDER_LEAF);
+
+  ret = nv_kthread_q_init(&g_tools_queue, "UVM Tools Event Queue");
+  if (ret < 0)
+    goto err_cache_destroy;
+
+  uvm_init_character_device(&g_uvm_tools_cdev, &uvm_tools_fops);
+  ret = cdev_add(&g_uvm_tools_cdev, uvm_tools_dev, 1);
+  if (ret != 0) {
+    UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n",
+                  MAJOR(uvm_tools_dev), MINOR(uvm_tools_dev), ret);
+    goto err_stop_thread;
+  }
+
+  return ret;
 
 err_stop_thread:
-    nv_kthread_q_stop(&g_tools_queue);
+  nv_kthread_q_stop(&g_tools_queue);
 
 err_cache_destroy:
-    _uvm_tools_destroy_cache_all();
-    return ret;
-}
-
-void uvm_tools_exit(void)
-{
-    unsigned i;
-    cdev_del(&g_uvm_tools_cdev);
-
-    nv_kthread_q_stop(&g_tools_queue);
+  _uvm_tools_destroy_cache_all();
+  return ret;
+}
+
+void uvm_tools_exit(void) {
+  unsigned i;
+  cdev_del(&g_uvm_tools_cdev);
+
+  nv_kthread_q_stop(&g_tools_queue);
+
+  for (i = 0; i < UvmEventNumTypesAll; ++i)
+    UVM_ASSERT(g_tools_enabled_event_count[i] == 0);
+
+  UVM_ASSERT(list_empty(&g_tools_va_space_list));
+
+  _uvm_tools_destroy_cache_all();
+}
+
+// added by kymartin for dumping GPU Page Tables.
+static void dump_gpu_tree(uvm_page_tree_t* tree, uvm_page_directory_t *dir) {
+
+  NvU32 depth; 
+  NvU32 entry_count;
+  NvU32 index;
+  uvm_page_directory_t* child;
+
+  if(dir == NULL)
+    return;
+
+  depth = dir->depth;
+  NvU64 phys_alloc_address = dir->phys_alloc.addr.address;
+  printk(KERN_INFO "Dumping GPU PageTable: PDE%d: 0x%llx Aperture: %d\n", depth, phys_alloc_address, dir->phys_alloc.addr.aperture);
+  
+  if(depth < 5) {
+    entry_count = tree->hal->entries_per_index(depth) << tree->hal->index_bits(depth, UVM_PAGE_SIZE_4K);
+    for(index = 0; index < entry_count; index++) {
+      child = dir->entries[index];
+      dump_gpu_tree(tree, child);
+    }
+  }
+}
+
+// added by kymartin for dumping GPU memory.
+NV_STATUS uvm_api_dump_gpu_memory(UVM_DUMP_GPU_MEMORY_PARAMS *params,
+                                  struct file *filep) {
+  NvU64 base_addr = params->base_addr;
+  NvU64 dump_size = params->dump_size;
+  NvU64 out_addr = params->out_addr;
+
+  NvU64 offset;
+
+  uvm_mem_t *cpu_mem = NULL;
+  uvm_mem_t *gpu_mem = NULL;
+  uvm_gpu_address_t cpu_addr;
+  uvm_gpu_address_t gpu_addr;
+  char gpu_uuid_buffer[UVM_UUID_STRING_LENGTH];
+
+  uvm_gpu_t *gpu;
+  uvm_parent_gpu_t *parent_gpu;
+  uvm_push_t push;
+
+  NV_STATUS status = NV_OK;
+
+  uvm_uuid_string(gpu_uuid_buffer, &params->gpu_uuid);
+  parent_gpu = uvm_parent_gpu_get_by_uuid(&params->gpu_uuid);
+  if (!parent_gpu) {
+    printk(KERN_USER
+           "uvm_api_dump_gpu_memory parent gpu not found with uuid: %s\n",
+           gpu_uuid_buffer);
+    return NV_ERR_INVALID_DEVICE;
+  }
+
+  if (test_bit(params->child_id, parent_gpu->valid_gpus)) {
+    gpu = parent_gpu->gpus[params->child_id];
+    uvm_uuid_string(gpu_uuid_buffer, &gpu->uuid);
+    printk(KERN_INFO "uvm_api_dump_gpu_memory child gpu %d uuid: %s\n",
+           params->child_id, gpu_uuid_buffer);
+  } else {
+    printk(KERN_ERR "uvm_api_dump_gpu_memory child gpu %d not found\n",
+           params->child_id);
+    return NV_ERR_INVALID_DEVICE;
+  }
+
+  // allocate a CPU memory buffer and map it for access.
+  status = uvm_mem_alloc_system_and_map_cpu_kernel(UVM_CHUNK_SIZE_MAX,
+                                                   current->mm, &cpu_mem);
+  if (status != NV_OK)
+    goto done;
+  status = uvm_mem_map_gpu_kernel(cpu_mem, gpu);
+  if (status != NV_OK)
+    goto done;
+
+  // allocate a small piece of GPU_MEMORY and map it for access;
+  status = uvm_mem_alloc_vidmem(UVM_CHUNK_SIZE_4K, gpu, &gpu_mem);
+  if (status != NV_OK)
+    goto done;
+  printk("GPU mem chunk size 0x%11x\n", gpu_mem->chunk_size);
+
+  cpu_addr = uvm_mem_gpu_address_virtual_kernel(cpu_mem, &gpu_mem);
+  gpu_addr = uvm_mem_gpu_address_physical(gpu_mem, gpu, 0, gpu_mem->chunk_size);
+  printk("GPU mem address 0x%11x\n", gpu_addr.address);
+
+  dump_gpu_tree(&gpu->address_space_tree, gpu->address_space_tree.root);
+  // dump GPU memory from the base_addr for the size of dump_size;
+  gpu_addr.address = base_addr;
+  printk(KERN_INFO "uvm_api_dump_gpu_memory instance phys start: 0x%11x, "
+                   "instance size: 0x%11x, gpu_addr.address: 0x%11x\n",
+         gpu->mem_info.phys_start, gpu->mem_info.size, gpu_addr.address);
+  offset = 0;
+  while (offset < dump_size) {
+    size_t cpy_size = min(UVM_CHUNK_SIZE_MAX, dump_size - offset);
+    status = uvm_push_begin(gpu->channel_manager, UVM_CHANNEL_TYPE_GPU_TO_CPU,
+                            &push, "dumping");
+    if (status != NV_OK)
+      goto done;
+    gpu->parent->ce_hal->memcopy(&push, cpu_addr, gpu_addr, cpy_size);
+    status = uvm_push_end_and_wait(&push);
+    if (status != NV_OK)
+      goto done;
 
-    for (i = 0; i < UvmEventNumTypesAll; ++i)
-        UVM_ASSERT(g_tools_enabled_event_count[i] == 0);
+    // copy stuff in the buffer into userspace.
+    copy_to_user((void *)out_addr, cpu_mem->kernel.cpu_addr, cpy_size);
+    gpu_addr.address += cpy_size;
+    out_addr += cpy_size;
+    offset += cpy_size;
+  }
 
-    UVM_ASSERT(list_empty(&g_tools_va_space_list));
+done:
+  if (cpu_mem)
+    uvm_mem_free(cpu_mem);
+  if (gpu_mem)
+    uvm_mem_free(gpu_mem);
 
-    _uvm_tools_destroy_cache_all();
+  return status;
 }
diff --git a/kernel-open/nvidia-uvm/uvm_tools.h b/kernel-open/nvidia-uvm/uvm_tools.h
index e8bfd67c..066989fa 100644
--- a/kernel-open/nvidia-uvm/uvm_tools.h
+++ b/kernel-open/nvidia-uvm/uvm_tools.h
@@ -25,67 +25,79 @@
 #define __UVM_TOOLS_H__
 
 #include "uvm_api.h"
-#include "uvm_types.h"
-#include "uvm_processors.h"
 #include "uvm_forward_decl.h"
-#include "uvm_test_ioctl.h"
 #include "uvm_hal_types.h"
+#include "uvm_processors.h"
+#include "uvm_test_ioctl.h"
+#include "uvm_types.h"
 #include "uvm_va_block_types.h"
 
-NV_STATUS uvm_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *params, struct file *filp);
-NV_STATUS uvm_test_inject_tools_event_v2(UVM_TEST_INJECT_TOOLS_EVENT_V2_PARAMS *params, struct file *filp);
-NV_STATUS uvm_test_increment_tools_counter(UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *params, struct file *filp);
-NV_STATUS uvm_test_tools_flush_replay_events(UVM_TEST_TOOLS_FLUSH_REPLAY_EVENTS_PARAMS *params, struct file *filp);
-
-NV_STATUS uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_write_process_memory(UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_get_processor_uuid_table(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS *params, struct file *filp);
-NV_STATUS uvm_api_tools_get_processor_uuid_table_v2(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params,
-                                                    struct file *filp);
-NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS *params, struct file *filp);
-
-static UvmEventFatalReason uvm_tools_status_to_fatal_fault_reason(NV_STATUS status)
-{
-    switch (status) {
-        case NV_OK:
-            return UvmEventFatalReasonInvalid;
-        case NV_ERR_NO_MEMORY:
-            return UvmEventFatalReasonOutOfMemory;
-        case NV_ERR_INVALID_ADDRESS:
-            return UvmEventFatalReasonInvalidAddress;
-        case NV_ERR_INVALID_ACCESS_TYPE:
-            return UvmEventFatalReasonInvalidPermissions;
-        case NV_ERR_INVALID_OPERATION:
-            return UvmEventFatalReasonInvalidOperation;
-        default:
-            return UvmEventFatalReasonInternalError;
-    }
+NV_STATUS
+uvm_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *params,
+                            struct file *filp);
+NV_STATUS
+uvm_test_inject_tools_event_v2(UVM_TEST_INJECT_TOOLS_EVENT_V2_PARAMS *params,
+                               struct file *filp);
+NV_STATUS uvm_test_increment_tools_counter(
+    UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *params, struct file *filp);
+NV_STATUS uvm_test_tools_flush_replay_events(
+    UVM_TEST_TOOLS_FLUSH_REPLAY_EVENTS_PARAMS *params, struct file *filp);
+
+NV_STATUS
+uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params,
+                                  struct file *filp);
+NV_STATUS uvm_api_tools_write_process_memory(
+    UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_get_processor_uuid_table(
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_get_processor_uuid_table_v2(
+    UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_V2_PARAMS *params, struct file *filp);
+NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS *params,
+                                     struct file *filp);
+
+// added by kymartin for dumping GPU memory
+NV_STATUS uvm_api_dump_gpu_memory(UVM_DUMP_GPU_MEMORY_PARAMS *params,
+                                  struct file *filep);
+
+static UvmEventFatalReason
+uvm_tools_status_to_fatal_fault_reason(NV_STATUS status) {
+  switch (status) {
+  case NV_OK:
+    return UvmEventFatalReasonInvalid;
+  case NV_ERR_NO_MEMORY:
+    return UvmEventFatalReasonOutOfMemory;
+  case NV_ERR_INVALID_ADDRESS:
+    return UvmEventFatalReasonInvalidAddress;
+  case NV_ERR_INVALID_ACCESS_TYPE:
+    return UvmEventFatalReasonInvalidPermissions;
+  case NV_ERR_INVALID_OPERATION:
+    return UvmEventFatalReasonInvalidOperation;
+  default:
+    return UvmEventFatalReasonInternalError;
+  }
 }
 
-void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t *va_space,
-                                      NvU64 address,
+void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t *va_space, NvU64 address,
                                       bool is_write,
                                       UvmEventFatalReason reason);
 
-void uvm_tools_record_gpu_fatal_fault(uvm_gpu_id_t gpu_id,
-                                      uvm_va_space_t *va_space,
-                                      const uvm_fault_buffer_entry_t *fault_entry,
-                                      UvmEventFatalReason reason);
+void uvm_tools_record_gpu_fatal_fault(
+    uvm_gpu_id_t gpu_id, uvm_va_space_t *va_space,
+    const uvm_fault_buffer_entry_t *fault_entry, UvmEventFatalReason reason);
 
-void uvm_tools_record_thrashing(uvm_va_space_t *va_space,
-                                NvU64 address,
+void uvm_tools_record_thrashing(uvm_va_space_t *va_space, NvU64 address,
                                 size_t region_size,
                                 const uvm_processor_mask_t *processors);
 
-void uvm_tools_record_throttling_start(uvm_va_space_t *va_space, NvU64 address, uvm_processor_id_t processor);
+void uvm_tools_record_throttling_start(uvm_va_space_t *va_space, NvU64 address,
+                                       uvm_processor_id_t processor);
 
-void uvm_tools_record_throttling_end(uvm_va_space_t *va_space, NvU64 address, uvm_processor_id_t processor);
+void uvm_tools_record_throttling_end(uvm_va_space_t *va_space, NvU64 address,
+                                     uvm_processor_id_t processor);
 
-void uvm_tools_record_map_remote(uvm_va_block_t *va_block,
-                                 uvm_push_t *push,
+void uvm_tools_record_map_remote(uvm_va_block_t *va_block, uvm_push_t *push,
                                  uvm_processor_id_t processor,
-                                 uvm_processor_id_t residency,
-                                 NvU64 address,
+                                 uvm_processor_id_t residency, NvU64 address,
                                  size_t region_size,
                                  UvmEventMapRemoteCause cause);
 
@@ -95,35 +107,33 @@ void uvm_tools_record_map_remote(uvm_va_block_t *va_block,
 // uvm_push_t object, and will be notified in burst when the last one finishes.
 // start parameter is only valid for managed migrations and ignored for pageable
 // migrations.
-void uvm_tools_record_migration_begin(uvm_va_space_t *va_space,
-                                      uvm_push_t *push,
-                                      uvm_processor_id_t dst_id,
-                                      int dst_nid,
-                                      uvm_processor_id_t src_id,
-                                      int src_nid,
-                                      NvU64 start,
-                                      uvm_make_resident_cause_t cause,
-                                      uvm_api_range_type_t type);
+void uvm_tools_record_migration_begin(
+    uvm_va_space_t *va_space, uvm_push_t *push, uvm_processor_id_t dst_id,
+    int dst_nid, uvm_processor_id_t src_id, int src_nid, NvU64 start,
+    uvm_make_resident_cause_t cause, uvm_api_range_type_t type);
 
 void uvm_tools_record_read_duplicate(uvm_va_block_t *va_block,
                                      uvm_processor_id_t dst,
                                      uvm_va_block_region_t region,
                                      const uvm_page_mask_t *page_mask);
 
-void uvm_tools_record_read_duplicate_invalidate(uvm_va_block_t *va_block,
-                                                uvm_processor_id_t dst,
-                                                uvm_va_block_region_t region,
-                                                const uvm_page_mask_t *page_mask);
+void uvm_tools_record_read_duplicate_invalidate(
+    uvm_va_block_t *va_block, uvm_processor_id_t dst,
+    uvm_va_block_region_t region, const uvm_page_mask_t *page_mask);
 
-void uvm_tools_broadcast_replay(uvm_gpu_t *gpu, uvm_push_t *push, NvU32 batch_id, uvm_fault_client_type_t client_type);
+void uvm_tools_broadcast_replay(uvm_gpu_t *gpu, uvm_push_t *push,
+                                NvU32 batch_id,
+                                uvm_fault_client_type_t client_type);
 
-void uvm_tools_broadcast_replay_sync(uvm_gpu_t *gpu, NvU32 batch_id, uvm_fault_client_type_t client_type);
+void uvm_tools_broadcast_replay_sync(uvm_gpu_t *gpu, NvU32 batch_id,
+                                     uvm_fault_client_type_t client_type);
 
-void uvm_tools_broadcast_access_counter(uvm_gpu_t *gpu, const uvm_access_counter_buffer_entry_t *buffer_entry);
+void uvm_tools_broadcast_access_counter(
+    uvm_gpu_t *gpu, const uvm_access_counter_buffer_entry_t *buffer_entry);
 
-void uvm_tools_record_access_counter(uvm_va_space_t *va_space,
-                                     uvm_gpu_id_t gpu_id,
-                                     const uvm_access_counter_buffer_entry_t *buffer_entry);
+void uvm_tools_record_access_counter(
+    uvm_va_space_t *va_space, uvm_gpu_id_t gpu_id,
+    const uvm_access_counter_buffer_entry_t *buffer_entry);
 
 void uvm_tools_test_hmm_split_invalidate(uvm_va_space_t *va_space);
 
